{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "\n",
    "from other_utils import to_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-right",
   "metadata": {},
   "source": [
    "# Test z-score normalization on-the-fly\n",
    "\n",
    "In this Jupyter Notebook, the on-the-fly z-score normalization is tested based on the preprocessed ERA5 to IFS downscaling data (under `/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_ifs/netcdf_data/all_files`). \n",
    "\n",
    "The check is performed on a smaller number of predictor/predictand variables. Also no shuffling is performed to ease the manual data reading from the corresponding netCDF-file. In particular, the first time step in the file `preproc_2016-01.nc` is checked.\n",
    "\n",
    "We start with some preparations on the data files to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "data_dir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_ifs/all_files\"\n",
    "\n",
    "predictors = [\"2t_in\", \"z_in\", \"sshf_in\"]\n",
    "predictands = [\"t2m_tar\", \"z_tar\"]\n",
    "\n",
    "# work on parameters\n",
    "nc_files_dir = glob.glob(os.path.join(data_dir, \"preproc*.nc\"))\n",
    "nc_files = sorted(nc_files_dir)\n",
    "\n",
    "all_vars = to_list(predictors) + to_list(predictands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nc_files)\n",
    "#for yr_mm in month_list:\n",
    "#    nc_files = nc_files + subset_files_on_date(nc_files_dir, yr_mm, date_alias=\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-crisis",
   "metadata": {},
   "source": [
    "Adapted function to create tf.Dataset-object from the netCDF-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(nc_files, predictors, predictands, norm_data):\n",
    "\n",
    "    def gen(nc_files_ds):\n",
    "\n",
    "        for file in nc_files_ds:\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[all_vars]\n",
    "            ntimes = len(ds[\"time\"])\n",
    "            for t in range(ntimes):\n",
    "                ds_t = ds.isel({\"time\": t})\n",
    "                in_data, tar_data = ds_t[predictors].to_array(dim=\"variables\").transpose(..., \"variables\"), \\\n",
    "                                    ds_t[predictands].to_array(dim=\"variables\").transpose(..., \"variables\")\n",
    "                yield tuple((in_data, tar_data))\n",
    "\n",
    "    s0 = next(iter(gen(nc_files)))\n",
    "    gen_mod = gen(nc_files)\n",
    "\n",
    "    tfds_dat = tf.data.Dataset.from_generator(lambda: gen_mod,\n",
    "                                              output_signature=(tf.TensorSpec(s0[0].shape, dtype=s0[0].dtype),\n",
    "                                                                tf.TensorSpec(s0[1].shape, dtype=s0[1].dtype)))\n",
    "\n",
    "    def normalize_batch(batch: tuple, norm_dict):\n",
    "\n",
    "        mu_in, std_in = tf.constant(norm_dict[\"mu_in\"], dtype=batch[0].dtype), \\\n",
    "                        tf.constant(norm_dict[\"std_in\"], dtype=batch[0].dtype)\n",
    "        mu_tar, std_tar = tf.constant(norm_dict[\"mu_tar\"], dtype=batch[1].dtype), \\\n",
    "                          tf.constant(norm_dict[\"std_tar\"], dtype=batch[1].dtype)\n",
    "\n",
    "        in_normed, tar_normed = tf.divide(tf.subtract(batch[0], mu_in), std_in), \\\n",
    "                                tf.divide(tf.subtract(batch[1], mu_tar), std_tar)\n",
    "\n",
    "        return in_normed, tar_normed\n",
    "\n",
    "    def parse_example(in_data, tar_data):\n",
    "        return normalize_batch((in_data, tar_data), norm_data)\n",
    "\n",
    "    tfds_dat = tfds_dat.batch(2).map(parse_example)\n",
    "    tfds_dat = tfds_dat.repeat(1).prefetch(1000)\n",
    "\n",
    "    return tfds_dat, {\"shape_in\": s0[0].shape, \"shape_tar\": s0[1].shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = {\"mu_in\": [275., 500., 6000.], \"mu_tar\": [275, 500.], \n",
    "             \"std_in\": [10, 300., 8000.], \"std_tar\": [10., 300.]}\n",
    "\n",
    "\n",
    "tfds_test, shp_dict = make_dataset(nc_files, predictors, predictands, norm_data)\n",
    "tfds_test = iter(tfds_test)\n",
    "\n",
    "batch1 = tfds_test.get_next()\n",
    "    \n",
    "in_data, tar_data = batch1[0].numpy()[0,...], batch1[1].numpy()[0,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-intent",
   "metadata": {},
   "source": [
    "Read the reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ref = xr.open_dataset(nc_files[0])\n",
    "\n",
    "ds_ref = ds_ref.isel(time=0)\n",
    "in_data_ref, tar_data_ref = ds_ref[predictors].to_array(\"variables\"), ds_ref[predictands].to_array(\"variables\")\n",
    "in_data_ref, tar_data_ref = in_data_ref.transpose(..., \"variables\"), tar_data_ref.transpose(..., \"variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-confusion",
   "metadata": {},
   "source": [
    "Convert the data from the dataset iterator into DataArrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = xr.DataArray(in_data, coords=in_data_ref.coords, dims=in_data_ref.dims)\n",
    "tar_data = xr.DataArray(tar_data, coords=tar_data_ref.coords, dims=tar_data_ref.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-emergency",
   "metadata": {},
   "source": [
    "Do the same with the data from the normalization dictionary to ease subsequent computation (-> z-score normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_in = xr.DataArray(norm_data[\"mu_in\"], coords=in_data[\"variables\"].coords, dims=in_data[\"variables\"].dims)\n",
    "mu_tar = xr.DataArray(norm_data[\"mu_tar\"], coords=tar_data[\"variables\"].coords, dims=tar_data[\"variables\"].dims)\n",
    "\n",
    "std_in = xr.DataArray(norm_data[\"std_in\"], coords=in_data[\"variables\"].coords, dims=in_data[\"variables\"].dims)\n",
    "std_tar = xr.DataArray(norm_data[\"std_tar\"], coords=tar_data[\"variables\"].coords, dims=tar_data[\"variables\"].dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-daniel",
   "metadata": {},
   "source": [
    "Finally, we perform a manual normalization of the data read from the netCDF-file and check the difference w.r.t. to the data from the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_in = (in_data_ref - mu_in)/std_in - in_data\n",
    "diff_tar = (tar_data_ref - mu_tar)/std_tar - tar_data\n",
    "\n",
    "assert np.amax(diff_in) < 1.e-06, \"Normalizing input data did not work properly.\"\n",
    "assert np.amax(diff_tar) < 1.e-06, \"Normalizing target data did not work properly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-footwear",
   "metadata": {},
   "source": [
    "Since no assert-error is thrown, the functionality of the normalization routine when setting up the dataset is verified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langguth1_downscaling_kernel_juwels",
   "language": "python",
   "name": "langguth1_downscaling_kernel_juwels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
