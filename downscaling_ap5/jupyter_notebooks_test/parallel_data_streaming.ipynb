{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af320e9-5d58-4458-9761-c15dc5a582db",
   "metadata": {},
   "source": [
    "# Improving data streaming from netCDF files with TensorFlow datasets\n",
    "\n",
    "In this Jupyter Notebook, approaches for handling data from multiple netCDF files with TF datasets will be tested. The aim is to come up with a performant approach that allows data reading from netCDF files rather than doing a conversion to TFRecords while keeping a memory-light data handling (to allow handling of datasets that do not fit into the memory of the computing node).\n",
    "\n",
    "The first approach samples data from multiple netCDF-files when creating individual batches. To speed up the operation, threading with `multiprocessing` is tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ceb668-5b0d-4319-82f1-71d16f1199e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import re\n",
    "#from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5ab17-35d3-4974-a0c5-e06bd751f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamMonthlyNetCDF():\n",
    "    def __init__(self, datadir, patt, workers=4, sample_dim: str = \"time\"):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        self.ds = xr.open_mfdataset(list(self.file_list), parallel=True)\n",
    "        self.sample_dim = sample_dim\n",
    "        self.times = self.ds[sample_dim].load()\n",
    "        self.nsamples = self.ds.dims[sample_dim]\n",
    "        self.file_handles = {}\n",
    "        self.time_dict_times = {}\n",
    "        for fnc in self.file_list:\n",
    "            self.file_handles[fnc] = xr.open_dataset(fnc)\n",
    "            self.time_dict_times[fnc] = self.file_handles[fnc][sample_dim].load()\n",
    "            # self.file_handles[fnc] = xr.open_dataset(fnc, decode_cf=False)\n",
    "            # self.file_handles.append(xr.open_dataset(fnc))\n",
    "        \n",
    "        print(f\"Number of used workers: {workers:d}\")\n",
    "        self.pool = multiprocessing.pool.ThreadPool(workers)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.index_to_sample(i)\n",
    "        return data\n",
    "    \n",
    "    def getitems(self, indices):\n",
    "        print(indices)\n",
    "        return np.array(self.pool.map(self.__getitem__ ,indices))\n",
    "    \n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "    \n",
    "    @data_dir.setter \n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise DirectoryNotFoundError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "            \n",
    "        self._data_dir = datadir\n",
    "        \n",
    "    @property \n",
    "    def file_list(self):\n",
    "        return self._file_list \n",
    "    \n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):        \n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\" \n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "            \n",
    "        self._file_list = sorted(files)        \n",
    "        \n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim \n",
    "    \n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "            \n",
    "        self._sample_dim = sample_dim \n",
    "        \n",
    "    def index_to_sample(self, index):\n",
    "        curr_time = pd.to_datetime(self.times[index].values)\n",
    "        \n",
    "        fname = [s for s in self.file_list if curr_time.strftime(\"%Y-%m\") in s]\n",
    "        if not fname:\n",
    "            raise FileNotFoundError(f\"Could not find a file matching requested date {date_ex}\")\n",
    "        elif len(fname) > 1:\n",
    "            raise ValueError(f\"Files found for requested date {date_ex} is not unique.\")\n",
    "        \n",
    "        ds = self.file_handles[fname[0]]  #\n",
    "        return ds.sel({self.sample_dim: curr_time}).to_array()\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343649e9-7d71-4596-b463-2be6437ea396",
   "metadata": {},
   "source": [
    "To speed up data reading, we stage the files on `CSCRATCH`, the high performance storage tier at JSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99e5d4-19a9-4632-a324-8bd3f35f9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! jutil env activate -p deepacf\n",
    "! datadir=\"${CSCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data\"\n",
    "! echo $datadir\n",
    "! datadir=\"${CSCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data\"; for yr in {2006..2018}; do for mm in {01..12}; do /opt/ddn/ime/bin/ime-ctl --prestage ${datadir}/${yr}/${yr}-${mm}/preproc_${yr}-${mm}.nc; done; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97aaf8-3d5b-4d93-af84-e8e6c0119d05",
   "metadata": {},
   "source": [
    "Let's check if the data is really staged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65875de8-be2f-4a8c-bf74-955e1d89d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! /opt/ddn/ime/bin/ime-ctl --frag-stat $CSCRATCH/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data/2015/2015-11/preproc_2015-11.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35d0a7-efad-4c7a-9f3a-31b1c7ce4e4e",
   "metadata": {},
   "source": [
    "Let's run a first test on the Tier-2 dataset of MAELSTROM's downscaling application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88b88c-c1bc-4e83-b570-93a89e485edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to netCDF-files\n",
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data/all_files/\"\n",
    "# batch size\n",
    "batch_size = 32\n",
    "# number of test sets\n",
    "test_steps = 500\n",
    "\n",
    "# get number of available (virtual) CPUs\n",
    "max_workers = multiprocessing.cpu_count()\n",
    "workers = min(batch_size, max_workers) \n",
    "\n",
    "workers_now = int(workers)\n",
    "print(f\"Number of available (virtual) CPUs: {max_workers:d}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26a729-af1c-4555-a746-e3aa573b8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an example monthly data stream\n",
    "all_data = StreamMonthlyNetCDF(datadir, \"preproc_*.nc\", workers=int(workers_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac1d2e-de1e-4829-8041-cd83756db8bb",
   "metadata": {},
   "source": [
    "Check the handled dataset (should have more than 100K samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf3a03-33c1-4714-a19f-76be971b65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = all_data.ds\n",
    "print(ds)\n",
    "# check timestamps and available number of samples\n",
    "print(f\"Available samples in dataset: {len(all_data):d}.\")\n",
    "print(all_data.times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd5ba0-c593-4d67-8ae6-aec1f8ae87df",
   "metadata": {},
   "source": [
    "Wrap everything into a numpy-function for TensorFlow and create a dataset. Note that the indices are shuffled while also ensuring that the buffer size is large enough to enable 'reasonable' sampling (20 K corresponds to 2.5 years of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635c2ca-884c-4841-ac7a-77cba36864bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fun2=lambda i: tf.numpy_function(all_data.getitems, [i] , tf.float64 )\n",
    "inp=tf.Variable(range(10))\n",
    "# some test\n",
    "data_test = tf_fun2(inp)\n",
    "\n",
    "# set-up TF dataset\n",
    "ds=tf.data.Dataset.range(len(all_data)).shuffle(buffer_size=20000).batch(int(workers_now*4)) \\\n",
    "                  .map(tf_fun2).unbatch().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9b5b2-7bc3-4835-8e1f-c82fca48c738",
   "metadata": {},
   "source": [
    "Run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9735ca-00e8-466e-a727-02ddc02d056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#from timeit import default_timer as timer\n",
    "batch_time = [timer()]\n",
    "# test with half of the workers\n",
    "for i, x in enumerate(ds):#tqdm(enumerate(ds)):\n",
    "    if i == 0:\n",
    "        print(tf.shape(x))\n",
    "    elif i > test_steps -1:\n",
    "        break\n",
    "    print(i)\n",
    "    batch_time.append(timer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5592e-5c9b-4585-a6bb-dceee77614b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time = np.asarray(batch_time)\n",
    "elapsed_times = batch_time[1:] - batch_time[0:-1]\n",
    "\n",
    "print(f\"Average time per batch: {np.mean(elapsed_times[3::]):.2f}s (+/- {np.std(elapsed_times[3::]):.3f}s). \\n\" +\n",
    "      f\"Total time: {np.sum(elapsed_times[3::]):.1f}s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aff514-7ff7-4783-8cf0-9dd50942bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34ed1b-1e90-47f5-bf7c-659e07a89611",
   "metadata": {},
   "source": [
    "## Preliminary result\n",
    "The experiments conducted above show that reading from multiple netCDF-files for sampling constitutes a severe bottleneck. Even with a higher number of threads, it takes 1.5s to 2.5s to sample a mini-batch with size 32. This is much slower than the forward and backward step take, e.g. for U-Net (about 0.1 to 0.2s).\n",
    "\n",
    "Without shuffling, the sampling is considerably quicker. This is obviously due to the fact that not several, but one netCDF-file is (most often) used for creating the mini-batch. \n",
    "\n",
    "The new idea is now to perform a manual shuffling before training. All the data will be read lazily and then shuffled to create netCDF-files from random time steps. By doing so, the netCDF-files can be consumed sequentially while also ensuring randomness in data sampling. For varying the ordering of the data during training, one might try to permute the file list order. However, it's not clear yet how this can be realized, maybe with the help of Keras Callbacks.\n",
    "\n",
    "### Prepare the shuffled dataset\n",
    "\n",
    "We start by first writing the data in a shuffled way to new netCDF files. Since coordinates in netCDF-files must be montonically ascending or descending, we need to introduce a sample index. <br>\n",
    "For later merging (i.e. opening with `xr.open_mfdataset`), the sample index must be unique and thus will be defined globally. Thus, it will run from 1 to `len(dataset)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e5f15-6b00-41f8-93be-c71fca396f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = all_data.ds \n",
    "times = all_data.times.copy(deep=True)\n",
    "ntimes = np.shape(times)[0]\n",
    "ds = ds.rename_dims({\"time\": \"sample_ind\"})\n",
    "ds[\"sample_ind\"] = range(np.shape(times)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccaab7-819e-45cf-bf01-c4b8469a8d15",
   "metadata": {},
   "source": [
    "(Double-)Check the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd883f-cd4f-4765-8bf1-e1e5fc581126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ntimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad03f2-b787-463f-9efd-c4db4dfdf962",
   "metadata": {},
   "source": [
    "Since any parallelized writing of netCDF-files proved to be terribly slow (e.g. with `xr.save_mfdataset` or using `multiprocessing`), we pursue a sequential netCDF-creation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade76ac5-7459-4227-9fd0-ddebc1ddbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2netcdf(id, indices):\n",
    "    print(f\"indices of process {id}: {indices}\", flush=True)\n",
    "    ds_subset = ds.isel({\"sample_ind\": indices}).load()\n",
    "    print(\"Data loaded sucsessfully!\", flush=True)\n",
    "    \n",
    "    nsamples_now = np.shape(ds_subset[\"sample_ind\"])[0]\n",
    "    ds_subset[\"sample_ind\"] = range(nsamples_now)\n",
    "    fname_now = os.path.join(datadir, \"test2\", f\"ds_resampled_{id:0d}_test.nc\")\n",
    "    \n",
    "    print(f\"Write data subset to file '{fname_now}'.\", flush=True)\n",
    "    ds_subset.to_netcdf(fname_now)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8d891-13dc-488f-8f06-e7099d2547c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datadir)\n",
    "#datadir = os.path.join(datadir, \"test2\")\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "print(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6265fa-b43f-4d69-847a-eee542750d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_file = int(8640)\n",
    "\n",
    "inds = np.arange(ntimes)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "# approach with multiprocessing -> slow\n",
    "# t0 = timer()\n",
    "# inds_list = [(i, inds[i*samples_per_file: (i+1)*samples_per_file]) for i in range(int(ntimes/samples_per_file))]\n",
    "# with multiprocessing.pool.ThreadPool(4) as Pool:\n",
    "#    for _ in Pool.starmap(sample2netcdf, inds_list):\n",
    "#        print(\"Done!\")\n",
    "        \n",
    "# print(f\"File creation took {timer()-t0:.2f}s.\")\n",
    "\n",
    "# approach with xr.save_mfdataset -> slow\n",
    "# fname_list, ds_list = [], []\n",
    "# for i in range(int(ntimes/samples_per_file)):    \n",
    "    # fname_list.append(os.path.join(datadir, \"test2\", f\"ds_resampled_{i:0d}.nc\"))\n",
    "    # ds_list.append(ds.isel({\"sample_ind\": inds_now}))\n",
    "# print(fname_list)\n",
    "# t0 = timer()\n",
    "# xr.save_mfdataset(ds_list, fname_list, mode=\"w\")\n",
    "# print(f\"Saving data took {timer()-t0:.1f}s.\")\n",
    "\n",
    "samples_per_file = int(8640)\n",
    "\n",
    "inds = np.arange(ntimes)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "t0 = timer()\n",
    "for i in range(int(ntimes/samples_per_file)):\n",
    "    \n",
    "    inds_now = inds[i*samples_per_file: (i+1)*samples_per_file]\n",
    "    print(f\"Load data to memory for {i+1:d}th subset...\")\n",
    "    ds_subset = ds.isel({\"sample_ind\": inds_now}).load()\n",
    "    print(\"Data loaded sucsessfully!\")\n",
    "    \n",
    "    nsamples_now = np.shape(ds_subset[\"sample_ind\"])[0]\n",
    "    #ds_subset[\"sample_ind\"] = range(i*nsamples_now, (i+1)*nsamples_now)\n",
    "    ds_subset[\"sample_ind\"] = range(nsamples_now)\n",
    "    fname_now = os.path.join(datadir, \"test2\", f\"ds_resampled_{i:0d}.nc\")\n",
    "    \n",
    "    print(f\"Write data subset to file '{fname_now}'.\")\n",
    "    ds_subset.to_netcdf(fname_now)\n",
    "    \n",
    "print(f\"File creation took {timer()-t0:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47175411-99c7-421b-8ff6-9b7007cca290",
   "metadata": {},
   "source": [
    "Next, we stage (again) the netCDF files to `CSCRATCH` for quicker data access: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49469ce-685e-4c27-adde-8f86b60dc552",
   "metadata": {},
   "outputs": [],
   "source": [
    "! datadir=\"${CSCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data/all_files/test\"; for i in {0..11}; do /opt/ddn/ime/bin/ime-ctl --prestage ${datadir}/ds_resampled_${i}.nc; done\n",
    "! datadir=\"${CSCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data/all_files/test\"; /opt/ddn/ime/bin/ime-ctl --frag-stat ${datadir}/ds_resampled_11.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceca138-c19c-4ced-beef-d3cb152183e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamMonthlyNetCDF():\n",
    "    def __init__(self, datadir, patt, workers=4, sample_dim: str = \"time\", samples_per_file: int = 8640):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        self.ds = xr.open_mfdataset(list(self.file_list), parallel=True)\n",
    "        self.sample_dim = sample_dim\n",
    "        self.times = self.ds[sample_dim].load()\n",
    "        self.nsamples = self.ds.dims[sample_dim]\n",
    "        self.samples_per_file = samples_per_file\n",
    "        self.ds_now = None\n",
    "        self.loaded_files = []\n",
    "        \n",
    "        print(f\"Number of used workers: {workers:d}\")\n",
    "        self.pool = multiprocessing.pool.ThreadPool(workers)\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.index_to_sample(i)\n",
    "        return data\n",
    "    \n",
    "    def getitems(self, indices):\n",
    "        inds_fname = list(set([int(i/self.samples_per_file) for i in indices]))\n",
    "        # before getting the data, check if we must load new files\n",
    "        if self.ds_now is None or not set(self.file_list[inds_fname]) == set(self.loaded_files):\n",
    "            print(f\"Load datafiles {*self.file_list[inds_fname],}\")\n",
    "            self.loaded_files = self.file_list[inds_fname]\n",
    "            self.ds_now = xr.open_mfdataset(list(self.loaded_files)).load()\n",
    "        return np.array(self.pool.map(self.__getitem__ , indices))\n",
    "    \n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "    \n",
    "    @data_dir.setter \n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise DirectoryNotFoundError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "            \n",
    "        self._data_dir = datadir\n",
    "        \n",
    "    @property \n",
    "    def file_list(self):\n",
    "        return self._file_list \n",
    "    \n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):        \n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\" \n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "            \n",
    "        self._file_list = np.asarray(sorted(files, key=lambda s: int(re.search(r'\\d+', os.path.basename(s)).group())))\n",
    "        \n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim \n",
    "    \n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "            \n",
    "        self._sample_dim = sample_dim \n",
    "        \n",
    "    def index_to_sample(self, index):    \n",
    "        try:\n",
    "            return self.ds_now.sel({self.sample_dim: index}).to_array()\n",
    "        except Exception as err:\n",
    "            # interestingly, this proves to work (racing condition?)\n",
    "            print(self.ds_now)\n",
    "            print(index)\n",
    "            print(index in self.ds_now[\"sample_ind\"])\n",
    "            print({self.sample_dim: index})\n",
    "            print(self.ds_now.sel({self.sample_dim: index}))\n",
    "            return self.ds_now.sel({self.sample_dim: index}).to_array()\n",
    "        #return ds.sel({self.sample_dim: curr_time}).to_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee54fdc-48f2-4f69-a229-7bfbb705e059",
   "metadata": {},
   "source": [
    "Next, we will instantiate the data object, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc187b3d-d1a6-450a-939d-efc2117e311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_new = StreamMonthlyNetCDF(os.path.join(datadir, \"test\"), \"ds_resampled_*.nc\", workers=int(workers_now), sample_dim = \"sample_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281741d4-6441-4541-9e53-a541c1a2249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data_new.ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1eecc-a07d-44ed-9642-e00e79b434b7",
   "metadata": {},
   "source": [
    "built a TF dataset (without shuffling to ensure sequential data loading), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da785cb-ce5c-4c20-a9d3-8e656aa30233",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fun2=lambda i: tf.numpy_function(all_data_new.getitems, [i] , tf.float64)\n",
    "\n",
    "# same experiment with all workers\n",
    "#ds=tf.data.Dataset.range(8578, 8643).batch(int(33)) \\\n",
    "ds=tf.data.Dataset.range(len(all_data)).batch(int(33)) \\\n",
    "                  .map(tf_fun2).unbatch().batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a393b6-87ef-4ace-b218-e06742cd4a0f",
   "metadata": {},
   "source": [
    "... and conduct the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef0c6f-087d-47d2-bb50-f4c9390d836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time = [timer()]\n",
    "test_steps = 500\n",
    "\n",
    "print(f\"Test for {test_steps}-times\")\n",
    "# test with half of the workers\n",
    "for i, x in enumerate(ds):#tqdm(enumerate(ds)):\n",
    "    if i == 0:\n",
    "        print(tf.shape(x))\n",
    "    elif i > test_steps -1:\n",
    "        break\n",
    "    batch_time.append(timer())\n",
    "    print(i)\n",
    "    \n",
    "batch_time = np.asarray(batch_time)\n",
    "elapsed_times = batch_time[1:] - batch_time[0:-1]\n",
    "\n",
    "print(f\"Average time per batch: {np.mean(elapsed_times):.2f} (+/- {np.std(elapsed_times):.3f}). \\n\" +\n",
    "      f\"Total time: {np.sum(elapsed_times):.1f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de903e9-b409-4afb-9efe-170d165407a4",
   "metadata": {},
   "source": [
    "It is seen that data sampling now is much quicker with an average creation time below 0.2s.\n",
    "Thus, this approach is a candidate for a real test when training the model. <br>\n",
    "However, open issues persist. These are:\n",
    "- [ ] Some missing data when the total number of samples is not a divider of samples_per_file\n",
    "- [ ] Potential racing condition when self.ds_now has to be updated (see the hacky try-except handling)\n",
    "- [ ] Fixed ordering of shuffled training samples (How to get variation into it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba9fb2-cc35-4d19-9ea3-506db018c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamMonthlyNetCDF():\n",
    "    def __init__(self, datadir, patt, workers=4, sample_dim: str = \"time\", samples_per_file: int = 8640):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        print(self.file_list)\n",
    "        self.ds = xr.open_mfdataset(list(self.file_list))#, parallel=True)\n",
    "        self.sample_dim = sample_dim\n",
    "        self.times = self.ds[sample_dim].load()\n",
    "        self.nsamples = self.ds.dims[sample_dim]\n",
    "        self.samples_per_file = samples_per_file\n",
    "        self.ds_now = None\n",
    "        self.istart = 0\n",
    "        self.loaded_files = []\n",
    "        \n",
    "        print(f\"Number of used workers: {workers:d}\")\n",
    "        self.pool = multiprocessing.pool.ThreadPool(workers)\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.index_to_sample(i)\n",
    "        return data\n",
    "    \n",
    "    def getitems(self, indices):\n",
    "        print(indices)\n",
    "        #inds_fname = list(set([int(i/self.samples_per_file) for i in indices]))\n",
    "        # before getting the data, check if we must load new files\n",
    "        #if self.ds_now is None or not set(self.file_list[inds_fname]) == set(self.loaded_files):\n",
    "        #    print(f\"Load datafiles {*self.file_list[inds_fname],}\")\n",
    "        #    self.loaded_files = self.file_list[inds_fname]\n",
    "        #    self.ds_now = xr.open_mfdataset(list(self.loaded_files)).load()\n",
    "        return np.array(self.pool.map(self.__getitem__ , indices))\n",
    "    \n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "    \n",
    "    @data_dir.setter \n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise DirectoryNotFoundError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "            \n",
    "        self._data_dir = datadir\n",
    "        \n",
    "    @property \n",
    "    def file_list(self):\n",
    "        return self._file_list \n",
    "    \n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):        \n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\" \n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "            \n",
    "        self._file_list = np.asarray(sorted(files, key=lambda s: int(re.search(r'\\d+', os.path.basename(s)).group())))\n",
    "        \n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim \n",
    "    \n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "            \n",
    "        self._sample_dim = sample_dim \n",
    "        \n",
    "    def read_netcdf(self, fname):\n",
    "        fname = tf.keras.backend.get_value(fname)\n",
    "        fname = str(fname).lstrip(\"b'\").rstrip(\"'\")\n",
    "        print(f\"Load data from {fname}...\")\n",
    "        ds_now = xr.open_dataset(str(fname), engine=\"netcdf4\")#.sel(**sel_dict)\n",
    "        self.ds_now = ds_now.astype(\"float32\", copy=False).load()\n",
    "        self.istart = self.ds_now[\"sample_ind\"][0].values\n",
    "        print(self.istart)\n",
    "\n",
    "        return True        \n",
    "        \n",
    "    def index_to_sample(self, index):  \n",
    "        index_loc = index + self.istart\n",
    "        try:\n",
    "            return self.ds_now.sel({self.sample_dim: index_loc}).astype(\"float32\", copy=False).to_array()\n",
    "        except Exception as err:\n",
    "            # interestingly, this proves to work (racing condition?)\n",
    "            print(f\"istart: {self.istart:d}\")\n",
    "            print(f\"dataset: {self.ds_now}\")\n",
    "            print(f\"index: {index}\")\n",
    "            print(f\"Bool: {index in self.ds_now['sample_ind']}\")\n",
    "            print({self.sample_dim: index})\n",
    "            print(self.ds_now.sel({self.sample_dim: index}))\n",
    "            return self.ds_now.sel({self.sample_dim: index}).to_array()\n",
    "        return ds.sel({self.sample_dim: curr_time}).to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f534a82-bc32-4988-a34f-73767b881e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_new = StreamMonthlyNetCDF(os.path.join(datadir, \"test\"), \"ds_resampled_*.nc\", workers=int(workers_now), sample_dim = \"sample_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab8fa9-1500-42e4-82c7-335deb645af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "tf_fun1 = lambda fname: tf.py_function(all_data_new.read_netcdf, [fname], tf.bool)\n",
    "tf_fun2 = lambda i: tf.numpy_function(all_data_new.getitems, [i] , (tf.float32, tf.float32))\n",
    "\n",
    "data_dir = all_data_new.data_dir\n",
    "#print(data_dir)\n",
    "\n",
    "\n",
    "def dataset_reader(fname):\n",
    "    fname = tf.keras.backend.get_value(fname)\n",
    "    print(str(fname).lstrip(\"b'\").strip(\"'\"))\n",
    "    ds = xr.open_dataset(str(fname).lstrip(\"b'\").strip(\"'\"), engine=\"netcdf4\").load()\n",
    "    da = np.array(ds.to_array().transpose(\"sample_ind\", \"variable\", ...))\n",
    "    \n",
    "    #dataset = tf.data.Dataset.from_tensor_slices(da)\n",
    "    return da\n",
    "\n",
    "tf_fun = lambda path: tf.py_function(dataset_reader, [path], Tout=tf.float64)\n",
    "data_iter = tf.data.Dataset.from_tensor_slices(all_data_new.file_list).map(tf_fun1)#.range(10).batch(5).map(tf_fun2)\n",
    "\n",
    "data_iter = data_iter.interleave(lambda x: tf.data.Dataset.range(10).batch(5).map(tf_fun2))\n",
    "#data_iter = tf.data.Dataset.from_tensor_slices(all_data_new.file_list).map(tf_fun)#.from_tensor_slices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63856398-f97e-444f-9114-7ed1c43b9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ele in enumerate(data_iter):\n",
    "    if i < 3:\n",
    "        print(ele)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8abee9-0537-476f-98e5-8a512bda7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.isfile(\"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data/all_files/test/ds_resampled_0.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffe15c-991e-463e-b7b2-d83330889737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.data.Dataset as Dataset\n",
    "\n",
    "filenames = [\"./test/file1.txt\", \"./test//file2.txt\",\n",
    "             \"./test/file3.txt\", \"./test/file4.txt\"]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "def parse_fn(filename):\n",
    "  return tf.data.Dataset.range(10)\n",
    "dataset = dataset.interleave(lambda x:\n",
    "    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
    "    cycle_length=4, block_length=16)\n",
    "\n",
    "\n",
    "#dataset = tf.data.Dataset.range(1, 6)\n",
    "#dataset = dataset.interleave(\n",
    "#    lambda x: tf.data.Dataset.from_tensors(x).repeat(6),\n",
    "#    cycle_length=2, block_length=4)\n",
    "#list(dataset.as_numpy_iterator())\n",
    "for ds in dataset:\n",
    "    for i in ds.as_numpy_iterator():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c24834-033f-430c-b432-f93f5199f9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langguth1_downscaling_kernel",
   "language": "python",
   "name": "langguth1_downscaling_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
