{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offensive-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-panel",
   "metadata": {},
   "source": [
    "# Build up efficient TF data pipelines from netCDF-files\n",
    "\n",
    "We test two different approaches to build up the dataset input streams. <br>\n",
    "The first one is based on `open_dataset` and requires a large buffer size to enable proper sampling (buffer size $\\mathcal{O}(10^4)$ so that at minimum 10 files are buffered). This is due to the fact that only the data files are randomized, not the data samples itself as in the next approach. <br>\n",
    "The second approach is based on `open_mfdatset` which makes the data sampling much easier since it allows randomization on an index-list for the time dimension to build up the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alone-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nc_dir_with_generator(dir_, patt, shuffle=True, seed=42):\n",
    "    \"\"\"\n",
    "    Opens datafiles via looping in the generator-method. This implies a larger buffer_size (> 12*744 where 12 corresponds to months per year\n",
    "    and 744 is the number of time steps in a monthly datafile) when shuffling since the buffer gets filled up with data from sequential (unordered) files.\n",
    "    :param dir_: The directory where the netCDF-files are located.\n",
    "    :param patt: Substring-pattern to identify the desired netCDF-files (\"{patt}*.nc\" is applied for searching)\n",
    "    :param shuffle: flag to enable shuffling\n",
    "    :param seed: seed for random shuffling\n",
    "    :return: tf.Dataset for data streain in neural networks\n",
    "    \"\"\"\n",
    "    \n",
    "    nc_files = glob.glob(os.path.join(dir_, f\"{patt}*.nc\"))\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(nc_files)\n",
    "    \n",
    "    def gen(nc_files, shuffle=True, seed=42):\n",
    "\n",
    "        for file in nc_files:\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ntimes = len(ds[\"time\"])\n",
    "            for t in range(ntimes):\n",
    "                ds_t = ds.isel({\"time\": t})\n",
    "                data_dict = {key: tf.convert_to_tensor(val) for key, val in ds_t.items()}\n",
    "                data_dict[\"time\"]= np.array([pd.to_datetime(ds_t[\"time\"].values).strftime(\"%Y-%m-%d %H:%M\")])\n",
    "                yield data_dict\n",
    "\n",
    "\n",
    "    sample = next(iter(gen(nc_files, shuffle, seed=seed)))\n",
    "    \n",
    "    gen_mod = gen(nc_files, shuffle, seed)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: gen_mod,\n",
    "        output_signature={\n",
    "            key: tf.TensorSpec(val.shape, dtype=val.dtype)\n",
    "            for key, val in sample.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "def load_mfnc_dir_with_generator(dir_: str, patt: str, shuffle: bool = True, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Opens netCDF-files using xarray's open_mfdataset-method. Shuffling of the data is achieved by shuffling over the time step-indices.\n",
    "    For efficiency, decoding of the time is disabled (implying shared time-units for all netCDF-data to avoid overwriting with open_mfdataset!!!) \n",
    "    since this information is not required for data streaming.\n",
    "    :param dir_: The directory where the netCDF-files are located.\n",
    "    :param patt: Substring-pattern to identify the desired netCDF-files (\"{patt}*.nc\" is applied for searching)\n",
    "    :param shuffle: flag to enable shuffling\n",
    "    :param seed: seed for random shuffling\n",
    "    :return: tf.Dataset for data streain in neural networks\n",
    "    \"\"\"    \n",
    "    ds_all = xr.open_mfdataset(os.path.join(dir_, f\"{patt}*.nc\"), cache=False, decode_cf=False)\n",
    "    ntimes = len(ds_all[\"time\"])\n",
    "    if shuffle: \n",
    "        random.seed(seed)\n",
    "        time_list = random.sample(range(ntimes), ntimes)\n",
    "    else:\n",
    "        time_list = range(ntimes)   \n",
    "\n",
    "\n",
    "    def gen(ds_all):\n",
    "        #ds_all = xr.open_mfdataset(os.path.join(dir_, f\"{patt}*.nc\"), cache=False, decode_cf=False)#, parallel=True)#, decode_times=False)       \n",
    "        for t in time_list:\n",
    "            # ds = xr.decode_cf(ds_all.isel({\"time\": t}))\n",
    "            ds = ds_all.isel({\"time\": t})\n",
    "            data_dict = {key: tf.convert_to_tensor(val) for key, val in ds.items()}\n",
    "            # data_dict[\"time\"]= np.array([pd.to_datetime(ds[\"time\"].values).strftime(\"%Y-%m-%d %H:%M\")])\n",
    "            yield data_dict        \n",
    "\n",
    "                    \n",
    "    sample = next(iter(gen(ds_all)))\n",
    "    \n",
    "    gen_mod = gen(ds_all)\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: gen_mod,\n",
    "        output_signature={\n",
    "            key: tf.TensorSpec(val.shape, dtype=val.dtype)\n",
    "            for key, val in sample.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "### highly in efficient in terms of memory -> not tested subsequently!!!\n",
    "def load_data(dir_, patt) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Obtain the data and meta information from the netcdf files, including the len of the samples, mim and max values\n",
    "    return: data as xarray's DataArray with dimensions [channels, time, lat, lon]\n",
    "    \"\"\"\n",
    "\n",
    "    def reshape_ds(ds):\n",
    "        da = ds.to_array(dim=\"variables\")\n",
    "        da = da.transpose(..., \"variables\")\n",
    "        return da\n",
    "    \n",
    "    ds = xr.open_mfdataset(os.path.join(dir_, f\"{patt}*.nc\"), cache=False, parallel=True)\n",
    "    da = reshape_ds(ds)\n",
    "    init_times = da[\"time\"]\n",
    "    \n",
    "    nvars = len(da[\"variables\"])\n",
    "\n",
    "    return da, init_times #da.chunk(chunks={\"time\": 744, \"variables\": nvars}), init_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-collins",
   "metadata": {},
   "source": [
    "After setting up the data directory, both strategies are benchmarked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funny-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_ifs/netcdf_data/all_files/\"\n",
    "pattern = \"preproc_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-oklahoma",
   "metadata": {},
   "source": [
    "We create the respective TF datasets, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alpine-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 13:30:41.079229: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-14 13:30:41.607481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30194 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5e:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "tfds_test = load_nc_dir_with_generator(datadir, pattern)\n",
    "tfds_test_mf = load_mfnc_dir_with_generator(datadir, pattern)\n",
    "tfds_test_mf2 = load_mfnc_dir_with_generator(datadir, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-peace",
   "metadata": {},
   "source": [
    "... configure them and... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "velvet-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 13:31:22.314399: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "sleep_sec = 0\n",
    "ap1 = iter(tfds_test.shuffle(buffer_size=20000).batch(32).prefetch(100).repeat(1))\n",
    "ap2 = iter(tfds_test_mf.batch(32).prefetch(100).repeat(1))\n",
    "ap3 = iter(tfds_test_mf2.shuffle(buffer_size=1000).batch(32).prefetch(100).repeat(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-native",
   "metadata": {},
   "source": [
    "then run both approaches. We start with approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fancy-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]2022-07-14 13:31:32.390863: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 1176 of 20000\n",
      "2022-07-14 13:31:42.393018: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 2364 of 20000\n",
      "2022-07-14 13:31:52.389588: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 3568 of 20000\n",
      "2022-07-14 13:32:02.390630: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 4759 of 20000\n",
      "2022-07-14 13:32:12.393867: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 5950 of 20000\n",
      "2022-07-14 13:32:22.391581: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 7137 of 20000\n",
      "2022-07-14 13:32:32.392505: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 8338 of 20000\n",
      "2022-07-14 13:32:42.394676: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 9564 of 20000\n",
      "2022-07-14 13:32:52.389783: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 10800 of 20000\n",
      "2022-07-14 13:33:02.401542: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 12002 of 20000\n",
      "2022-07-14 13:33:12.394499: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 13200 of 20000\n",
      "2022-07-14 13:33:22.396244: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 14405 of 20000\n",
      "2022-07-14 13:33:32.394863: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 15560 of 20000\n",
      "2022-07-14 13:33:42.395892: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 16762 of 20000\n",
      "2022-07-14 13:33:52.394951: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 17951 of 20000\n",
      "2022-07-14 13:34:02.403250: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 19141 of 20000\n",
      "2022-07-14 13:34:09.537888: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n",
      "100%|██████████| 1000/1000 [06:30<00:00,  2.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filling the buffer, retrieving each minibatch took: 0.2235s\n",
      "CPU times: user 5min 41s, sys: 1min, total: 6min 42s\n",
      "Wall time: 6min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sleep_sec = 0\n",
    "ntakes = 1000\n",
    "\n",
    "for i in tqdm(range(ntakes)):\n",
    "    if i == 1:\n",
    "        time_s = time.time()\n",
    "    \n",
    "    batch = ap1.get_next()\n",
    "    #print(batch[\"2t_in\"])\n",
    "    #print(\"***************\")\n",
    "   \n",
    "load_time_ap1 = (time.time() - time_s)/float(ntakes-1)\n",
    "print(\"After filling the buffer, retrieving each minibatch took: {0:5.04f}s\".format(load_time_ap1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-writing",
   "metadata": {},
   "source": [
    "Now, we continue with approach 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "useful-extra",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping for 0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [25:34<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 49s, sys: 2min 38s, total: 21min 27s\n",
      "Wall time: 25min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "time_s = time.time()\n",
    "\n",
    "for i in tqdm(range(ntakes)):\n",
    "    if i == 0:\n",
    "        print(f\"Sleeping for {sleep_sec}s...\")\n",
    "        time.sleep(sleep_sec)\n",
    "    \n",
    "    batch = ap2.get_next()\n",
    "    #print(batch[\"2t_in\"])\n",
    "    #print(\"***************\")\n",
    "    \n",
    "load_time_ap2 = (time.time() - time_s)/float(ntakes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trying-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]2022-07-14 14:03:37.568508: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 110 of 1000\n",
      "2022-07-14 14:03:47.533671: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 218 of 1000\n",
      "2022-07-14 14:03:57.521254: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 325 of 1000\n",
      "2022-07-14 14:04:07.606089: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 434 of 1000\n",
      "2022-07-14 14:04:17.535844: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 544 of 1000\n",
      "2022-07-14 14:04:27.577850: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 651 of 1000\n",
      "2022-07-14 14:04:37.580522: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 763 of 1000\n",
      "2022-07-14 14:04:47.548819: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 873 of 1000\n",
      "2022-07-14 14:04:57.547731: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 987 of 1000\n",
      "2022-07-14 14:04:58.724628: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n",
      "100%|██████████| 1000/1000 [28:15<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filling the buffer, retrieving each minibatch took: 1.6026s\n",
      "CPU times: user 21min 44s, sys: 3min 7s, total: 24min 51s\n",
      "Wall time: 28min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sleep_sec = 0\n",
    "\n",
    "for i in tqdm(range(ntakes)):\n",
    "    if i == 1:\n",
    "        time_s = time.time()\n",
    "    \n",
    "    batch = ap3.get_next()\n",
    "    #print(batch[\"2t_in\"])\n",
    "    #print(\"***************\")\n",
    "   \n",
    "load_time_ap3 = (time.time() - time_s)/float(ntakes-1)\n",
    "print(\"After filling the buffer, retrieving each minibatch took: {0:5.04f}s\".format(load_time_ap3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-anderson",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olympic-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filling the buffer, retrieving each minibatch with APPROACH 1 took: 0.2235s\n",
      "Retrieving each minibatch with APPROACH 2 took: 1.5344s\n",
      "After filling the buffer, retrieving each minibatch with APPROACH 3 took: 1.6026s\n"
     ]
    }
   ],
   "source": [
    "print(\"After filling the buffer, retrieving each minibatch with APPROACH 1 took: {0:5.04f}s\".format(load_time_ap1))\n",
    "print(\"Retrieving each minibatch with APPROACH 2 took: {0:5.04f}s\".format(load_time_ap2))\n",
    "print(\"After filling the buffer, retrieving each minibatch with APPROACH 3 took: {0:5.04f}s\".format(load_time_ap3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-picture",
   "metadata": {},
   "source": [
    "Thus, we see that the first approach outperforms the second approach (at least after the buffer has been filled once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-interest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langguth1_downscaling_kernel",
   "language": "python",
   "name": "langguth1_downscaling_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
