{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-panel",
   "metadata": {},
   "source": [
    "# Build up efficient TF data pipelines from netCDF-files\n",
    "\n",
    "We test two different approaches to build up the dataset input streams. <br>\n",
    "The first one is based on `open_dataset` and requires a large buffer size to enable proper sampling (buffer size $\\mathcal{O}(10^4)$ so that at minimum 10 files are buffered). This is due to the fact that only the data files are randomized, not the data samples itself as in the next approach. <br>\n",
    "The second approach is based on `open_mfdatset` which makes the data sampling much easier since it allows randomization on an index-list for the time dimension to build up the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nc_dir_with_generator(dir_, patt, shuffle=True, seed=42):\n",
    "    \"\"\"\n",
    "    Opens datafiles via looping in the generator-method. This implies a larger buffer_size (> 12*744 where 12 corresponds to months per year\n",
    "    and 744 is the number of time steps in a monthly datafile) when shuffling since the buffer gets filled up with data from sequential (unordered) files.\n",
    "    :param dir_: The directory where the netCDF-files are located.\n",
    "    :param patt: Substring-pattern to identify the desired netCDF-files (\"{patt}*.nc\" is applied for searching)\n",
    "    :param shuffle: flag to enable shuffling\n",
    "    :param seed: seed for random shuffling\n",
    "    :return: tf.Dataset for data streain in neural networks\n",
    "    \"\"\"\n",
    "    \n",
    "    nc_files = glob.glob(os.path.join(dir_, f\"{patt}*.nc\"))\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(nc_files)\n",
    "    \n",
    "    def gen(nc_files, shuffle=True, seed=42):\n",
    "\n",
    "        for file in nc_files:\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ntimes = len(ds[\"time\"])\n",
    "            for t in range(ntimes):\n",
    "                ds_t = ds.isel({\"time\": t})\n",
    "                data_dict = {key: tf.convert_to_tensor(val) for key, val in ds_t.items()}\n",
    "                data_dict[\"time\"]= np.array([pd.to_datetime(ds_t[\"time\"].values).strftime(\"%Y-%m-%d %H:%M\")])\n",
    "                yield data_dict\n",
    "\n",
    "\n",
    "    sample = next(iter(gen(nc_files, shuffle, seed=seed)))\n",
    "    \n",
    "    gen_mod = gen(nc_files, shuffle, seed)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: gen_mod,\n",
    "        output_signature={\n",
    "            key: tf.TensorSpec(val.shape, dtype=val.dtype)\n",
    "            for key, val in sample.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "def load_mfnc_dir_with_generator(dir_: str, patt: str, shuffle: bool = True, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Opens netCDF-files using xarray's open_mfdataset-method. Shuffling of the data is achieved by shuffling over the time step-indices.\n",
    "    For efficiency, decoding of the time is disabled (implying shared time-units for all netCDF-data to avoid overwriting with open_mfdataset!!!) \n",
    "    since this information is not required for data streaming.\n",
    "    :param dir_: The directory where the netCDF-files are located.\n",
    "    :param patt: Substring-pattern to identify the desired netCDF-files (\"{patt}*.nc\" is applied for searching)\n",
    "    :param shuffle: flag to enable shuffling\n",
    "    :param seed: seed for random shuffling\n",
    "    :return: tf.Dataset for data streain in neural networks\n",
    "    \"\"\"    \n",
    "    ds_all = xr.open_mfdataset(os.path.join(dir_, f\"{patt}*.nc\"), cache=False, decode_cf=False)\n",
    "    ntimes = len(ds_all[\"time\"])\n",
    "    if shuffle: \n",
    "        random.seed(seed)\n",
    "        time_list = random.sample(range(ntimes), ntimes)\n",
    "    else:\n",
    "        time_list = range(ntimes)   \n",
    "\n",
    "\n",
    "    def gen(ds_all):\n",
    "        #ds_all = xr.open_mfdataset(os.path.join(dir_, f\"{patt}*.nc\"), cache=False, decode_cf=False)#, parallel=True)#, decode_times=False)       \n",
    "        for t in time_list:\n",
    "            # ds = xr.decode_cf(ds_all.isel({\"time\": t}))\n",
    "            ds = ds_all.isel({\"time\": t})\n",
    "            data_dict = {key: tf.convert_to_tensor(val) for key, val in ds.items()}\n",
    "            # data_dict[\"time\"]= np.array([pd.to_datetime(ds[\"time\"].values).strftime(\"%Y-%m-%d %H:%M\")])\n",
    "            yield data_dict        \n",
    "\n",
    "                    \n",
    "    sample = next(iter(gen(ds_all)))\n",
    "    \n",
    "    gen_mod = gen(ds_all)\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: gen_mod,\n",
    "        output_signature={\n",
    "            key: tf.TensorSpec(val.shape, dtype=val.dtype)\n",
    "            for key, val in sample.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "### highly in efficient in terms of memory -> not tested subsequently!!!\n",
    "def load_data(dir_, patt) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Obtain the data and meta information from the netcdf files, including the len of the samples, mim and max values\n",
    "    return: data as xarray's DataArray with dimensions [channels, time, lat, lon]\n",
    "    \"\"\"\n",
    "\n",
    "    def reshape_ds(ds):\n",
    "        da = ds.to_array(dim=\"variables\")\n",
    "        da = da.transpose(..., \"variables\")\n",
    "        return da\n",
    "    \n",
    "    ds = xr.open_mfdataset(os.path.join(dir_, f\"{patt}*.nc\"), cache=False, parallel=True)\n",
    "    da = reshape_ds(ds)\n",
    "    init_times = da[\"time\"]\n",
    "    \n",
    "    nvars = len(da[\"variables\"])\n",
    "\n",
    "    return da, init_times #da.chunk(chunks={\"time\": 744, \"variables\": nvars}), init_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-collins",
   "metadata": {},
   "source": [
    "After setting up the data directory, both strategies are benchmarked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_ifs/all_files/\"\n",
    "pattern = \"preproc_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-oklahoma",
   "metadata": {},
   "source": [
    "We create the respective TF datasets, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_test = load_nc_dir_with_generator(datadir, pattern)\n",
    "tfds_test_mf = load_mfnc_dir_with_generator(datadir, pattern)\n",
    "tfds_test_mf2 = load_mfnc_dir_with_generator(datadir, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-peace",
   "metadata": {},
   "source": [
    "... configure them and... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_sec = 0\n",
    "ap1 = iter(tfds_test.shuffle(buffer_size=20000).batch(32).prefetch(100).repeat(1))\n",
    "ap2 = iter(tfds_test_mf.batch(32).prefetch(100).repeat(1))\n",
    "ap3 = iter(tfds_test_mf2.shuffle(buffer_size=1000).batch(32).prefetch(100).repeat(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-native",
   "metadata": {},
   "source": [
    "then run both approaches. We start with approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sleep_sec = 0\n",
    "ntakes = 1000\n",
    "\n",
    "for i in tqdm(range(ntakes)):\n",
    "    if i == 1:\n",
    "        time_s = time.time()\n",
    "    \n",
    "    batch = ap1.get_next()\n",
    "    #print(batch[\"2t_in\"])\n",
    "    #print(\"***************\")\n",
    "   \n",
    "load_time_ap1 = (time.time() - time_s)/float(ntakes-1)\n",
    "print(\"After filling the buffer, retrieving each minibatch took: {0:5.04f}s\".format(load_time_ap1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-writing",
   "metadata": {},
   "source": [
    "Now, we continue with approach 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "time_s = time.time()\n",
    "\n",
    "for i in tqdm(range(ntakes)):\n",
    "    if i == 0:\n",
    "        print(f\"Sleeping for {sleep_sec}s...\")\n",
    "        time.sleep(sleep_sec)\n",
    "    \n",
    "    batch = ap2.get_next()\n",
    "    #print(batch[\"2t_in\"])\n",
    "    #print(\"***************\")\n",
    "    \n",
    "load_time_ap2 = (time.time() - time_s)/float(ntakes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sleep_sec = 0\n",
    "\n",
    "for i in tqdm(range(ntakes)):\n",
    "    if i == 1:\n",
    "        time_s = time.time()\n",
    "    \n",
    "    batch = ap3.get_next()\n",
    "    #print(batch[\"2t_in\"])\n",
    "    #print(\"***************\")\n",
    "   \n",
    "load_time_ap3 = (time.time() - time_s)/float(ntakes-1)\n",
    "print(\"After filling the buffer, retrieving each minibatch took: {0:5.04f}s\".format(load_time_ap3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-anderson",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After filling the buffer, retrieving each minibatch with APPROACH 1 took: {0:5.04f}s\".format(load_time_ap1))\n",
    "print(\"Retrieving each minibatch with APPROACH 2 took: {0:5.04f}s\".format(load_time_ap2))\n",
    "print(\"After filling the buffer, retrieving each minibatch with APPROACH 3 took: {0:5.04f}s\".format(load_time_ap3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-picture",
   "metadata": {},
   "source": [
    "Thus, we see that the first approach outperforms the second approach (at least after the buffer has been filled once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xr.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550ace5-a6a8-4f86-9462-5c193a7f15f1",
   "metadata": {},
   "source": [
    "## New approach after discussion with Stefan Kesselheim\n",
    "\n",
    "A new approach which also allows for parallelization is tested by creating a dataset handler with a `get_item`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c88ba-6ecf-4471-9e78-f8e8c2742098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "from other_utils import to_list, subset_files_on_date, extract_date\n",
    "\n",
    "class MonthlyNetCDFDataset():\n",
    "    \n",
    "    def __init__(self, data_dir, month_list, predictors, predictands, fsuffix=\"preproc\"):\n",
    "        \"\"\"\n",
    "        Create netCDF dataset instance.\n",
    "        :param data_dir: directory where monthly netCDF-files are located\n",
    "        :param month_list: lisr if months (strings with format <YYYY>-<MM>)\n",
    "        :param predictors: list of predictor variables\n",
    "        :param predictands: list of predictands\n",
    "        :param fsuffix: suffix of netCDF-files \n",
    "        \"\"\"\n",
    "        # some preparations\n",
    "        nc_files_dir = glob.glob(os.path.join(data_dir, fsuffix+\"*.nc\"))\n",
    "        all_vars = to_list(predictors) + to_list(predictands)\n",
    "        \n",
    "        # filter net-CDF files based on months of interest\n",
    "        if not nc_files_dir:\n",
    "            raise FileNotFoundError(f\"Could not find any netCDF-files under '{data_dir}' with suffix {fsuffix}\")\n",
    "        self.files = []\n",
    "        for yr_mm in month_list:\n",
    "            self.files = self.files + subset_files_on_date(nc_files_dir, yr_mm, date_alias=\"%Y-%m\")\n",
    "                                    \n",
    "        self.fsuffix = fsuffix\n",
    "        self.date_of_files, self.times, self.nsamples = self.get_data_info()\n",
    "        self.file_handles = [xr.open_dataset(dfile) for dfile in self.files]\n",
    "        \n",
    "    def get_data_info(self):\n",
    "        # check if self.files is not empty\n",
    "        if not self.files:\n",
    "            raise FileNotFoundError(\"Could not find any datafiles under '{0}' containing data for the months: {1}\"\n",
    "                                    .format(data_dir, \", \".join(month_list)))\n",
    "            \n",
    "        # retrieve dates from filenames\n",
    "        date_of_files = [extract_date(os.path.basename(dfile)).strftime(\"%Y-%m\") for dfile in self.files]\n",
    "        # open (lazily) all netCDF-files to retrieve data timestamps\n",
    "        ds_all = xr.open_mfdataset(self.files)\n",
    "        \n",
    "        # convert timestamps to more convenient pandas datetime-objects and get number of samples\n",
    "        times = [pd.to_datetime(str(time.values)) for time in ds_all[\"time\"]]            \n",
    "        nsamples = len(ds_all[\"time\"])\n",
    "        \n",
    "        return date_of_files, times, nsamples\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        print(type(i))\n",
    "        file_index, infile_index = self.get_indices(i)\n",
    "        return self.file_handles[file_index].isel({\"time\": infile_index})\n",
    "    \n",
    "    def get_indices(self, i):\n",
    "        month_now = self.times[i].strftime(\"%Y-%m\")\n",
    "        file_index = self.date_of_files.index(month_now)\n",
    "        infile_index = int((self.times[i] - pd.to_datetime(month_now + \"-01 00:00\"))/pd.Timedelta(hours=1))\n",
    "        \n",
    "        return file_index, infile_index\n",
    "                                           \n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2fa5a-c034-45a5-9ec1-3b9c8aa790fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = [date.strftime(\"%Y-%m\") for date in pd.date_range(start=\"2017-01\", end=\"2017-12\", freq='MS')]\n",
    "\n",
    "ds = MonthlyNetCDFDataset(datadir, month_list, [\"t2m\", \"t_850\", \"slhf\"], [\"2t\", \"z\"], fsuffix=\"preproc_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd366e3-3785-4523-b20d-271849b47a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554faf4-0013-47da-bf96-6533cd41bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.range(ds.nsamples)\n",
    "\n",
    "#tfds = tf.data.Dataset.range(ds.nsamples).map(lambda x: ds.__getitem__(x))\n",
    "tfds = tf.data.Dataset.range(ds.nsamples).map(lambda x: ds.__getitem__(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72731570-9d1c-4661-9462-f6b8c0d509dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup_kernel_maelstrom",
   "language": "python",
   "name": "jup_kernel_maelstrom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
