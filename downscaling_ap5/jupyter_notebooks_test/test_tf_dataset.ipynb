{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e84ab10-1ed6-40ba-a43b-d6614bf1168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with importing packages at 2023-01-30 22:55:13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "sys.path.append(\"../models/\")\n",
    "sys.path.append(\"../handle_data/\")\n",
    "import glob\n",
    "import argparse\n",
    "from datetime import datetime as dt\n",
    "print(\"Start with importing packages at {0}\".format(dt.strftime(dt.now(), \"%Y-%m-%d %H:%M:%S\")))\n",
    "import gc\n",
    "import json as js\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from model_utils import ModelEngine, handle_opt_utils\n",
    "from handle_data_class import HandleDataClass, get_dataset_filename\n",
    "from all_normalizations import ZScore\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea90b04-e6ed-4355-9c48-6a4dff9c8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from operator import itemgetter\n",
    "import dask\n",
    "\n",
    "class StreamMonthlyNetCDF(object):\n",
    "    # TO-DO:\n",
    "    # - get samples_per_file from the data rather than predefining it (varying samples per file for monthly data files!)\n",
    "\n",
    "    def __init__(self, datadir, patt, workers=4, sample_dim: str = \"time\", selected_predictors: List = None,\n",
    "                 selected_predictands: List = None, var_tar2in: str = None, norm_dims: List = None, norm_obj = None):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        self.ds = xr.open_mfdataset(list(self.file_list), combine=\"nested\", concat_dim=sample_dim)  # , parallel=True)\n",
    "        # check if norm_dims or norm_obj should be used\n",
    "        assert norm_obj or norm_dims, f\"Neither norm_obj nor norm_dims has been provided.\"\n",
    "        if norm_obj and norm_dims:\n",
    "            print(\"WARNING: norm_obj and norm_dims have been passed. norm_dims will be ignored.\")\n",
    "            norm_dims = None\n",
    "        if norm_obj:\n",
    "            #assert isinstance(norm_obj, Normalize), \"norm_obj is not an instance of the Normalize-class.\"\n",
    "            self.data_norm = norm_obj\n",
    "        else:\n",
    "            self.data_norm = ZScore(norm_dims)      # TO-DO: Allow for arbitrary normalization\n",
    "        self.sample_dim = sample_dim\n",
    "        self.data_dim = self.get_data_dim()\n",
    "        self.norm_params = self.data_norm.get_required_stats(self.ds.to_array(dim=\"variables\"))\n",
    "        self.nsamples = self.ds.dims[sample_dim]\n",
    "        self.variables = list(self.ds.variables)\n",
    "        self.samples_per_file = 28175                # TO-DO avoid hard-coding\n",
    "        self.predictor_list = selected_predictors\n",
    "        self.predictand_list = selected_predictands\n",
    "        self.n_predictands = len(self.predictand_list)\n",
    "        self.var_tar2in = var_tar2in\n",
    "        self.data = None\n",
    "\n",
    "        print(f\"Number of used workers: {workers:d}\")\n",
    "        self.pool = multiprocessing.pool.ThreadPool(workers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.index_to_sample(i)\n",
    "        return data\n",
    "\n",
    "    def getitems(self, indices):\n",
    "        return np.array(self.pool.map(self.__getitem__, indices))\n",
    "\n",
    "    def get_data_dim(self):\n",
    "        \"\"\"\n",
    "        Retrieve the dimensionality of the data to be handled, i.e. without sample_dim which will be batched in a\n",
    "        data stream.\n",
    "        :return: tuple of data dimensions\n",
    "        \"\"\"\n",
    "        # get existing dimension names and remove sample_dim\n",
    "        dimnames = list(self.ds.coords)\n",
    "        dimnames.remove(self.sample_dim)\n",
    "\n",
    "        # get the dimensionality of the data of interest\n",
    "        all_dims = dict(self.ds.dims)\n",
    "        data_dim = itemgetter(*dimnames)(all_dims)\n",
    "\n",
    "        return data_dim\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    @data_dir.setter\n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise NotADirectoryError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "\n",
    "        self._data_dir = datadir\n",
    "\n",
    "    @property\n",
    "    def file_list(self):\n",
    "        return self._file_list\n",
    "\n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):\n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\"\n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "\n",
    "        self._file_list = np.asarray(sorted(files, key=lambda s: int(re.search(r'\\d+', os.path.basename(s)).group())))\n",
    "\n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim\n",
    "\n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "\n",
    "        self._sample_dim = sample_dim\n",
    "\n",
    "    @property\n",
    "    def predictor_list(self):\n",
    "        return self._predictor_list\n",
    "\n",
    "    @predictor_list.setter\n",
    "    def predictor_list(self, selected_predictors: List):\n",
    "        \"\"\"\n",
    "        Initalizes predictor list. In case that selected_predictors is set to None, all variables with suffix `_in` in their names are selected.\n",
    "        In case that a list of selected_predictors is parsed, their availability is checked.\n",
    "        :param selected_predictors: list of predictor variables or None\n",
    "        \"\"\"\n",
    "        self._predictor_list = self.check_and_choose_vars(selected_predictors, \"_in\")\n",
    "        \n",
    "    @property\n",
    "    def predictand_list(self):\n",
    "        return self._predictand_list\n",
    "    \n",
    "    @predictand_list.setter\n",
    "    def predictand_list(self, selected_predictands: List):\n",
    "        self._predictand_list = self.check_and_choose_vars(selected_predictands, \"_tar\")\n",
    "        \n",
    "    def check_and_choose_vars(self, var_list, suffix: str = \"*\"):\n",
    "        \"\"\"\n",
    "        Checks list of variables for availability or retrieves all variables named with a given suffix (for var_list = None)\n",
    "        :param var_list: list of predictor variables or None\n",
    "        :param suffix: optional suffix of variables to selected. Only effective if var_list is None.\n",
    "        :return selected_vars: list of selected variables\n",
    "        \"\"\"\n",
    "        if var_list is None:\n",
    "            selected_vars = [var for var in self.variables if var.endswith(suffix)]\n",
    "        else:\n",
    "            stat_list = [var in self.variables for var in var_list]         \n",
    "            if all(stat_list):\n",
    "                selected_vars = var_list\n",
    "            else:\n",
    "                miss_inds = [i for i, x in enumerate(stat_list) if x]\n",
    "                miss_vars = [var_list[i] for i in miss_inds]\n",
    "                raise ValueError(f\"Could not find the following variables in the dataset: {*miss_vars,}\")\n",
    "        \n",
    "        return selected_vars\n",
    "\n",
    "    def read_netcdf(self, fname):\n",
    "        fname = tf.keras.backend.get_value(fname)\n",
    "        fname = str(fname).lstrip(\"b'\").rstrip(\"'\")\n",
    "        print(f\"Load data from {fname}...\")\n",
    "        ds_now = xr.open_dataset(str(fname), engine=\"netcdf4\")\n",
    "        var_list = list(self.predictor_list) + list(self.predictand_list) \n",
    "        ds_now = ds_now[var_list]\n",
    "        da_now = ds_now.to_array(\"variables\").transpose(..., \"variables\").astype(\"float32\", copy=False)\n",
    "        # da_now = self.data_norm.normalize(da_now)\n",
    "        # da_now = xr.concat([da_now.sel({\"variables\": \"hsurf_tar\"}), da_now], dim=\"variables\")\n",
    "        \n",
    "        da_now = dask.compute(da_now)[0]\n",
    "        # da_now = self.data_norm.normalize(da_now)\n",
    "        # self.data = xr.concat([da_now.sel({\"variables\": \"hsurf_tar\"}), da_now], dim=\"variables\")\n",
    "        self.data = da_now\n",
    "        print(self.data)\n",
    "        \n",
    "        self.nsamples = len(ds_now[self.sample_dim])\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def index_to_sample(self, index):\n",
    "        data = self.data.isel({self.sample_dim: index})\n",
    "        data = self.data_norm.normalize(data)\n",
    "        data = xr.concat([data.sel({\"variables\": \"hsurf_tar\"}), data], dim=\"variables\")\n",
    "\n",
    "        return data\n",
    "        #return self.data.isel({self.sample_dim: index})        \n",
    "\n",
    "        \n",
    "    def normalize_batch(self, batch):\n",
    "        return self.data_norm.normalize(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4a2213-2e3d-4129-99a8-8657cc97bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_model = \"../HPC_batch_scripts/config_wgan.json\"\n",
    "js_ds = \"../HPC_batch_scripts/config_ds_tier2.json\"\n",
    "\n",
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files/tmp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c163c9d-35c2-4cff-b323-c7343f4c2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'norm_dims': ['time', 'rlat', 'rlon'], 'batch_size': 32, 'var_tar2in': 'hsurf_tar'}\n"
     ]
    }
   ],
   "source": [
    "model_instance = ModelEngine(\"wgan\")\n",
    "\n",
    "# read configuration files for model and dataset\n",
    "with open(js_ds, \"r\") as dsf:\n",
    "    ds_dict = js.load(dsf)\n",
    "\n",
    "print(ds_dict)\n",
    "with open(js_model, \"r\") as mdf:\n",
    "    hparams_dict = js.load(mdf)\n",
    "\n",
    "named_targets = hparams_dict.get(\"named_targets\", False)\n",
    "\n",
    "bs_train = ds_dict[\"batch_size\"] * hparams_dict[\"d_steps\"] + 1 if \"d_steps\" in hparams_dict else ds_dict[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c866bea-893f-4ff1-9acb-4d052b195715",
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers = multiprocessing.cpu_count()\n",
    "\n",
    "norm_dims = ds_dict[\"norm_dims\"]\n",
    "norm_obj = ZScore(norm_dims)\n",
    "norm_obj.read_norm_from_file(os.path.join(datadir, \"norm.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920b2931-47a5-481e-afad-fc0944377fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "#print(inspect.getmembers(norm_obj))\n",
    "mu, sigma = norm_obj.norm_stats[\"mu\"].copy(), norm_obj.norm_stats[\"sigma\"].copy()\n",
    "mu, sigma = mu.astype(\"float32\"), sigma.astype(\"float32\")\n",
    "norm_obj.norm_stats[\"mu\"], norm_obj.norm_stats[\"sigma\"] = mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9624b23-5877-4512-b6e9-628b5a7ae489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu and sigma are parsed for (de-)normalization.\n",
      "Number of used workers: 1\n"
     ]
    }
   ],
   "source": [
    "ds_obj = StreamMonthlyNetCDF(os.path.join(datadir), \"ds_resampled*.nc\", workers=1,\n",
    "                             var_tar2in=\"hsurf_tar\", norm_obj=norm_obj, \n",
    "                             selected_predictands=[\"t_2m_tar\", \"hsurf_tar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b116ad-2018-4afd-b08e-666b57e1f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_read_nc = lambda fname: tf.py_function(ds_obj.read_netcdf, [fname], tf.bool)\n",
    "tf_getdata = lambda i: tf.numpy_function(ds_obj.getitems, [i], tf.float32)\n",
    "#tf_norm = lambda arr: tf.numpy_function(ds_obj.normalize_batch, [arr], tf.float32)\n",
    "tf_split = lambda arr: (arr[..., 0:-ds_obj.n_predictands], arr[...,-ds_obj.n_predictands:])\n",
    "\n",
    "tfds = tf.data.Dataset.from_tensor_slices(ds_obj.file_list).map(tf_read_nc)\n",
    "#ds_obj.samples_per_file = 1500\n",
    "tfds = tfds.flat_map(lambda x: tf.data.Dataset.range(ds_obj.samples_per_file).shuffle(ds_obj.samples_per_file)\\\n",
    "                       .batch(bs_train, drop_remainder=True).map(tf_getdata))#.map(tf_norm))\n",
    "tfds = tfds.map(tf_split).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8e24b3-abda-4bfc-8457-2aed3ab17d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(tfds, iterations=250, test_case=\"default\"):\n",
    "    t0 = timer()\n",
    "    for i, x in enumerate(tfds):\n",
    "        data_in, data_tar = x\n",
    "        print(i)\n",
    "        if i > iterations:\n",
    "            break\n",
    "            \n",
    "    time_tot = timer()-t0\n",
    "    print(f\"Iteration for {test_case}-test took {time_tot:.2f}s.\")\n",
    "    \n",
    "    return time_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd951778-62ee-445a-9bf8-dcb5efd4058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_time = run_test(tfds, test_case=\"worker1_load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ecfea-a062-4e43-8c8b-1dbab81a3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel tfds\n",
    "%xdel ds_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4447638-6f52-4a82-abd5-63c692d09dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obj = StreamMonthlyNetCDF(os.path.join(datadir), \"ds_resampled*.nc\", workers=1,\n",
    "                             var_tar2in=\"hsurf_tar\", norm_obj=norm_obj)\n",
    "\n",
    "tf_read_nc = lambda fname: tf.py_function(ds_obj.read_netcdf, [fname], tf.bool)\n",
    "tf_getdata = lambda i: tf.numpy_function(ds_obj.getitems, [i], tf.float32)\n",
    "tf_split = lambda arr: (arr[..., 0:-2], arr[...,-2:])\n",
    "\n",
    "tfds = tf.data.Dataset.from_tensor_slices(ds_obj.file_list).map(tf_read_nc)\n",
    "#ds_obj.samples_per_file = 1500\n",
    "tfds = tfds.flat_map(lambda x: tf.data.Dataset.range(ds_obj.samples_per_file).shuffle(ds_obj.samples_per_file)\\\n",
    "                       .batch(bs_train, drop_remainder=True).map(tf_getdata, num_parallel_calls=tf.data.AUTOTUNE))\n",
    "tfds = tfds.map(tf_split).repeat()\n",
    "\n",
    "exp_time = run_test(tfds, test_case=\"worker1_parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abae8a-cccf-4614-ac07-079c479fd9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel tfds\n",
    "%xdel ds_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9467f-d516-493a-94fe-ee1c221c9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from operator import itemgetter\n",
    "import dask\n",
    "\n",
    "class StreamMonthlyNetCDF(object):\n",
    "    # TO-DO:\n",
    "    # - get samples_per_file from the data rather than predefining it (varying samples per file for monthly data files!)\n",
    "\n",
    "    def __init__(self, datadir, patt, workers=4, sample_dim: str = \"time\", selected_predictors: List = None,\n",
    "                 selected_predictands: List = None, var_tar2in: str = None, norm_dims: List = None, norm_obj = None):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        print(self.file_list)\n",
    "        self.ds = xr.open_mfdataset(list(self.file_list), combine=\"nested\", concat_dim=sample_dim)  # , parallel=True)\n",
    "        # check if norm_dims or norm_obj should be used\n",
    "        assert norm_obj or norm_dims, f\"Neither norm_obj nor norm_dims has been provided.\"\n",
    "        if norm_obj and norm_dims:\n",
    "            print(\"WARNING: norm_obj and norm_dims have been passed. norm_dims will be ignored.\")\n",
    "            norm_dims = None\n",
    "        if norm_obj:\n",
    "            #assert isinstance(norm_obj, Normalize), \"norm_obj is not an instance of the Normalize-class.\"\n",
    "            self.data_norm = norm_obj\n",
    "        else:\n",
    "            self.data_norm = ZScore(norm_dims)      # TO-DO: Allow for arbitrary normalization\n",
    "        self.sample_dim = sample_dim\n",
    "        self.data_dim = self.get_data_dim()\n",
    "        self.norm_params = self.data_norm.get_required_stats(self.ds.to_array(dim=\"variables\"))\n",
    "        self.nsamples = self.ds.dims[sample_dim]\n",
    "        self.variables = list(self.ds.variables)\n",
    "        self.samples_per_file = 28175                # TO-DO avoid hard-coding\n",
    "        self.predictor_list = selected_predictors\n",
    "        self.predictand_list = selected_predictands\n",
    "        self.n_predictands = len(self.predictand_list)\n",
    "        print(self.predictand_list)\n",
    "        print(self.predictor_list)   \n",
    "        self.var_tar2in = var_tar2in\n",
    "        self.data = None\n",
    "\n",
    "        print(f\"Number of used workers: {workers:d}\")\n",
    "        self.pool = multiprocessing.pool.ThreadPool(workers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.index_to_sample(i)\n",
    "        return data\n",
    "\n",
    "    def getitems(self, indices):\n",
    "        return np.array(self.pool.map(self.__getitem__, indices))\n",
    "\n",
    "    def get_data_dim(self):\n",
    "        \"\"\"\n",
    "        Retrieve the dimensionality of the data to be handled, i.e. without sample_dim which will be batched in a\n",
    "        data stream.\n",
    "        :return: tuple of data dimensions\n",
    "        \"\"\"\n",
    "        # get existing dimension names and remove sample_dim\n",
    "        dimnames = list(self.ds.coords)\n",
    "        dimnames.remove(self.sample_dim)\n",
    "\n",
    "        # get the dimensionality of the data of interest\n",
    "        all_dims = dict(self.ds.dims)\n",
    "        data_dim = itemgetter(*dimnames)(all_dims)\n",
    "\n",
    "        return data_dim\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    @data_dir.setter\n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise NotADirectoryError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "\n",
    "        self._data_dir = datadir\n",
    "\n",
    "    @property\n",
    "    def file_list(self):\n",
    "        return self._file_list\n",
    "\n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):\n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\"\n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "\n",
    "        self._file_list = np.asarray(sorted(files, key=lambda s: int(re.search(r'\\d+', os.path.basename(s)).group())))\n",
    "\n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim\n",
    "\n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "\n",
    "        self._sample_dim = sample_dim\n",
    "\n",
    "    @property\n",
    "    def predictor_list(self):\n",
    "        return self._predictor_list\n",
    "\n",
    "    @predictor_list.setter\n",
    "    def predictor_list(self, selected_predictors: List):\n",
    "        \"\"\"\n",
    "        Initalizes predictor list. In case that selected_predictors is set to None, all variables with suffix `_in` in their names are selected.\n",
    "        In case that a list of selected_predictors is parsed, their availability is checked.\n",
    "        :param selected_predictors: list of predictor variables or None\n",
    "        \"\"\"\n",
    "        self._predictor_list = self.check_and_choose_vars(selected_predictors, \"_in\")\n",
    "        \n",
    "    @property\n",
    "    def predictand_list(self):\n",
    "        return self._predictand_list\n",
    "    \n",
    "    @predictand_list.setter\n",
    "    def predictand_list(self, selected_predictands: List):\n",
    "        self._predictand_list = self.check_and_choose_vars(selected_predictands, \"_tar\")\n",
    "        \n",
    "    def check_and_choose_vars(self, var_list, suffix: str = \"*\"):\n",
    "        \"\"\"\n",
    "        Checks list of variables for availability or retrieves all variables named with a given suffix (for var_list = None)\n",
    "        :param var_list: list of predictor variables or None\n",
    "        :param suffix: optional suffix of variables to selected. Only effective if var_list is None.\n",
    "        :return selected_vars: list of selected variables\n",
    "        \"\"\"\n",
    "        if var_list is None:\n",
    "            selected_vars = [var for var in self.variables if var.endswith(suffix)]\n",
    "        else:\n",
    "            stat_list = [var in self.variables for var in var_list]         \n",
    "            if all(stat_list):\n",
    "                selected_vars = var_list\n",
    "            else:\n",
    "                miss_inds = [i for i, x in enumerate(stat_list) if x]\n",
    "                miss_vars = [var_list[i] for i in miss_inds]\n",
    "                raise ValueError(f\"Could not find the following variables in the dataset: {*miss_vars,}\")\n",
    "        \n",
    "        return selected_vars\n",
    "\n",
    "    def read_netcdf(self, fname):\n",
    "        fname = tf.keras.backend.get_value(fname)\n",
    "        fname = str(fname).lstrip(\"b'\").rstrip(\"'\")\n",
    "        print(f\"Load data from {fname}...\")\n",
    "        ds_now = xr.open_dataset(str(fname), engine=\"netcdf4\")\n",
    "        var_list = list(self.predictor_list) + list(self.predictand_list) \n",
    "        ds_now = ds_now[var_list]\n",
    "        da_now = ds_now.to_array(\"variables\").transpose(..., \"variables\").astype(\"float32\", copy=False)\n",
    "        # da_now = self.data_norm.normalize(da_now)\n",
    "        # da_now = xr.concat([da_now.sel({\"variables\": \"hsurf_tar\"}), da_now], dim=\"variables\")\n",
    "        \n",
    "        da_now = dask.compute(da_now)[0]\n",
    "        #da_now = self.data_norm.normalize(da_now)\n",
    "        self.data = xr.concat([da_now.sel({\"variables\": \"hsurf_tar\"}), da_now], dim=\"variables\")\n",
    "        #self.data = da_now\n",
    "        print(self.data)\n",
    "        \n",
    "        self.nsamples = len(ds_now[self.sample_dim])\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def index_to_sample(self, index):\n",
    "        #data = self.data.isel({self.sample_dim: index})\n",
    "        #data = self.data_norm.normalize(data)\n",
    "        #data = xr.concat([data.sel({\"variables\": \"hsurf_tar\"}), data], dim=\"variables\")\n",
    "        return self.data.isel({self.sample_dim: index})\n",
    "        \n",
    "        #return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c3d62-cc0e-4b80-bde7-44f14c81536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obj = StreamMonthlyNetCDF(os.path.join(datadir), \"ds_resampled*.nc\", workers=1,\n",
    "                             var_tar2in=\"hsurf_tar\", norm_obj=norm_obj)\n",
    "\n",
    "tf_read_nc = lambda fname: tf.py_function(ds_obj.read_netcdf, [fname], tf.bool)\n",
    "tf_getdata = lambda i: tf.numpy_function(ds_obj.getitems, [i], tf.float32)\n",
    "tf_split = lambda arr: (arr[..., 0:-2], arr[...,-2:])\n",
    "\n",
    "tfds = tf.data.Dataset.from_tensor_slices(ds_obj.file_list).map(tf_read_nc)\n",
    "tfds = tfds.flat_map(lambda x: tf.data.Dataset.range(ds_obj.samples_per_file).shuffle(ds_obj.samples_per_file)\\\n",
    "                       .batch(bs_train, drop_remainder=True).map(tf_getdata))\n",
    "tfds = tfds.map(tf_split).repeat()\n",
    "\n",
    "exp_time = run_test(tfds, test_case=f\"prenorm_noparallel\")\n",
    "\n",
    "%xdel tfds\n",
    "%xdel ds_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e541b2e-8fe9-45ee-8c32-789b1dc555f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obj = StreamMonthlyNetCDF(os.path.join(datadir), \"ds_resampled*.nc\", workers=1,\n",
    "                             var_tar2in=\"hsurf_tar\", norm_obj=norm_obj)\n",
    "\n",
    "tf_read_nc = lambda fname: tf.py_function(ds_obj.read_netcdf, [fname], tf.bool)\n",
    "tf_getdata = lambda i: tf.numpy_function(ds_obj.getitems, [i], tf.float32)\n",
    "tf_split = lambda arr: (arr[..., 0:-2], arr[...,-2:])\n",
    "\n",
    "tfds = tf.data.Dataset.from_tensor_slices(ds_obj.file_list).map(tf_read_nc)\n",
    "#ds_obj.samples_per_file = 1500\n",
    "tfds = tfds.flat_map(lambda x: tf.data.Dataset.range(ds_obj.samples_per_file).shuffle(ds_obj.samples_per_file)\\\n",
    "                       .batch(bs_train, drop_remainder=True).map(tf_getdata, num_parallel_calls=tf.data.AUTOTUNE))\n",
    "tfds = tfds.map(tf_split).repeat()\n",
    "\n",
    "exp_time = run_test(tfds, test_case=f\"prenorm_autotune\")\n",
    "\n",
    "%xdel tfds\n",
    "%xdel ds_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab266fa-d475-4392-825a-0e6e6a202a50",
   "metadata": {},
   "source": [
    "## Experiment with vectorized indexing\n",
    "\n",
    "Here, we check if vectorized indexing (cf. `getitems`-method) actually performs better than (potentially) parallelized mapping of the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddc0b51d-afae-419a-a1c3-7e95cb5fb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamMonthlyNetCDF(object):\n",
    "    # TO-DO:\n",
    "    # - get samples_per_file from the data rather than predefining it (varying samples per file for monthly data files!)\n",
    "\n",
    "    def __init__(self, datadir, patt, workers=4, sample_dim: str = \"time\", selected_predictors: List = None,\n",
    "                 selected_predictands: List = None, var_tar2in: str = None, norm_dims: List = None, norm_obj = None):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        print(self.file_list)\n",
    "        self.ds = xr.open_mfdataset(list(self.file_list), combine=\"nested\", concat_dim=sample_dim)  # , parallel=True)\n",
    "        self.sample_dim = sample_dim\n",
    "        self.data_dim = self.get_data_dim()\n",
    "        # check if norm_dims or norm_obj should be used\n",
    "        assert norm_obj or norm_dims, f\"Neither norm_obj nor norm_dims has been provided.\"\n",
    "        if norm_obj and norm_dims:\n",
    "            print(\"WARNING: norm_obj and norm_dims have been passed. norm_dims will be ignored.\")\n",
    "            norm_dims = None\n",
    "        if norm_obj:\n",
    "            #assert isinstance(norm_obj, Normalize), \"norm_obj is not an instance of the Normalize-class.\"\n",
    "            self.data_norm = norm_obj\n",
    "        else:\n",
    "            self.data_norm = ZScore(norm_dims)      # TO-DO: Allow for arbitrary normalization\n",
    "        self.sample_dim = sample_dim\n",
    "        self.data_dim = self.get_data_dim()\n",
    "        self.norm_params = self.data_norm.get_required_stats(self.ds.to_array(dim=\"variables\"))\n",
    "        self.nsamples = self.ds.dims[sample_dim]\n",
    "        self.variables = list(self.ds.variables)\n",
    "        self.samples_per_file = 28175                # TO-DO avoid hard-coding\n",
    "        self.predictor_list = selected_predictors\n",
    "        self.predictand_list = selected_predictands\n",
    "        self.n_predictands = len(self.predictand_list)\n",
    "        self.var_tar2in = var_tar2in\n",
    "        self.data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def getitems(self, indices):\n",
    "        return self.data.isel({self.sample_dim: indices}).transpose(..., \"variables\")\n",
    "\n",
    "    def get_data_dim(self):\n",
    "        \"\"\"\n",
    "        Retrieve the dimensionality of the data to be handled, i.e. without sample_dim which will be batched in a\n",
    "        data stream.\n",
    "        :return: tuple of data dimensions\n",
    "        \"\"\"\n",
    "        # get existing dimension names and remove sample_dim\n",
    "        dimnames = list(self.ds.coords)\n",
    "        dimnames.remove(self.sample_dim)\n",
    "\n",
    "        # get the dimensionality of the data of interest\n",
    "        all_dims = dict(self.ds.dims)\n",
    "        data_dim = itemgetter(*dimnames)(all_dims)\n",
    "\n",
    "        return data_dim\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    @data_dir.setter\n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise NotADirectoryError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "\n",
    "        self._data_dir = datadir\n",
    "\n",
    "    @property\n",
    "    def file_list(self):\n",
    "        return self._file_list\n",
    "\n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):\n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\"\n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "\n",
    "        self._file_list = np.asarray(sorted(files, key=lambda s: int(re.search(r'\\d+', os.path.basename(s)).group())))\n",
    "\n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim\n",
    "\n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "\n",
    "        self._sample_dim = sample_dim\n",
    "\n",
    "    @property\n",
    "    def predictor_list(self):\n",
    "        return self._predictor_list\n",
    "\n",
    "    @predictor_list.setter\n",
    "    def predictor_list(self, selected_predictors: List):\n",
    "        \"\"\"\n",
    "        Initalizes predictor list. In case that selected_predictors is set to None, all variables with suffix `_in` in their names are selected.\n",
    "        In case that a list of selected_predictors is parsed, their availability is checked.\n",
    "        :param selected_predictors: list of predictor variables or None\n",
    "        \"\"\"\n",
    "        self._predictor_list = self.check_and_choose_vars(selected_predictors, \"_in\")\n",
    "        \n",
    "    @property\n",
    "    def predictand_list(self):\n",
    "        return self._predictand_list\n",
    "    \n",
    "    @predictand_list.setter\n",
    "    def predictand_list(self, selected_predictands: List):\n",
    "        self._predictand_list = self.check_and_choose_vars(selected_predictands, \"_tar\")\n",
    "        \n",
    "    def check_and_choose_vars(self, var_list, suffix: str = \"*\"):\n",
    "        \"\"\"\n",
    "        Checks list of variables for availability or retrieves all variables named with a given suffix (for var_list = None)\n",
    "        :param var_list: list of predictor variables or None\n",
    "        :param suffix: optional suffix of variables to selected. Only effective if var_list is None.\n",
    "        :return selected_vars: list of selected variables\n",
    "        \"\"\"\n",
    "        if var_list is None:\n",
    "            selected_vars = [var for var in self.variables if var.endswith(suffix)]\n",
    "        else:\n",
    "            stat_list = [var in self.variables for var in var_list]         \n",
    "            if all(stat_list):\n",
    "                selected_vars = var_list\n",
    "            else:\n",
    "                miss_inds = [i for i, x in enumerate(stat_list) if x]\n",
    "                miss_vars = [var_list[i] for i in miss_inds]\n",
    "                raise ValueError(f\"Could not find the following variables in the dataset: {*miss_vars,}\")\n",
    "        \n",
    "        return selected_vars\n",
    "\n",
    "    def read_netcdf(self, fname):\n",
    "        fname = tf.keras.backend.get_value(fname)\n",
    "        fname = str(fname).lstrip(\"b'\").rstrip(\"'\")\n",
    "        print(f\"Load data from {fname}...\")\n",
    "        ds_now = xr.open_dataset(str(fname), engine=\"netcdf4\")\n",
    "        var_list = list(self.predictor_list) + list(self.predictand_list) \n",
    "        ds_now = ds_now[var_list]\n",
    "        da_now = ds_now.to_array(\"variables\").astype(\"float32\", copy=False)\n",
    "        self.data = xr.concat([da_now, da_now.sel({\"variables\": \"hsurf_tar\"})], dim=\"variables\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "faa92136-5741-4c37-a035-4f119945e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files/tmp/ds_resampled_0.nc'\n",
      " '/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files/tmp/ds_resampled_1.nc'\n",
      " '/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files/tmp/ds_resampled_2.nc']\n"
     ]
    }
   ],
   "source": [
    "ds_obj = StreamMonthlyNetCDF(os.path.join(datadir), \"ds_resampled*.nc\", workers=1,\n",
    "                             var_tar2in=\"hsurf_tar\", norm_obj=norm_obj, \n",
    "                             selected_predictands=[\"t_2m_tar\", \"hsurf_tar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c494812-514d-4f25-b458-bdcd66235b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_read_nc = lambda fname: tf.py_function(ds_obj.read_netcdf, [fname], tf.bool)\n",
    "tf_getdata = lambda i: tf.numpy_function(ds_obj.getitems, [i], tf.float32)\n",
    "tf_split = lambda arr: (arr[..., 0:-ds_obj.n_predictands], arr[...,-ds_obj.n_predictands:])\n",
    "\n",
    "tfds = tf.data.Dataset.from_tensor_slices(ds_obj.file_list).map(tf_read_nc)\n",
    "tfds = tfds.flat_map(lambda x: tf.data.Dataset.range(ds_obj.samples_per_file).shuffle(ds_obj.samples_per_file)\\\n",
    "                       .batch(bs_train, drop_remainder=True).map(tf_getdata))#.map(tf_norm))\n",
    "tfds = tfds.map(tf_split).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c5ac0b4-b534-4895-83a4-74117ac8b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from /p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files/tmp/ds_resampled_0.nc...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "Load data from /p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files/tmp/ds_resampled_1.nc...\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "Iteration for worker1_vectorized-test took 71.99s.\n"
     ]
    }
   ],
   "source": [
    "exp_time = run_test(tfds, test_case=\"worker1_vectorized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa32d3d-3306-4b9d-aa01-61c7c04fd6fa",
   "metadata": {},
   "source": [
    "Vectorized indexing yields a test time slightly above 70s which is 15-20s quicker than the prevouis experiments.\n",
    "\n",
    "## Data streaming with `xarray.open_mfdataset`\n",
    "\n",
    "Finally, we check an approach where no data has to be written on disk beforehand for streaming, but where streaming is realized by loading the data on-the-fly into memory.\n",
    "This is realized by creating sublists of all files to be processed which are then merged while loading with `xarray.open_mfdataset`. <br>\n",
    "Te advantage of this approach is that normalization can be performed exploiting the `preprocess`-argument of `xarray.open_mfdataset`. \n",
    "However, we must first revise the Normalization-class to enable processing on datasets as well as since currently only `xarray.DataArray`s are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62fabce-16f1-4a10-b5a3-8e9c2ab1b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abstract class to perform normalization on data\n",
    "\"\"\"\n",
    "\n",
    "__email__ = \"m.langguth@fz-juelich.de\"\n",
    "__author__ = \"Michael Langguth\"\n",
    "__date__ = \"2022-11-24\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, List\n",
    "import os\n",
    "import json as js\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "da_or_ds = Union[xr.DataArray, xr.Dataset]\n",
    "\n",
    "\n",
    "class Normalize(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for normalizing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, method: str, norm_dims: List):\n",
    "        self.method = method\n",
    "        self.norm_dims = norm_dims\n",
    "        self.norm_stats = None\n",
    "\n",
    "    def normalize(self, data: xr.DataArray, **stats):\n",
    "        \"\"\"\n",
    "        Normalize data.\n",
    "        :param data: The DataArray to be normalized.\n",
    "        :param **stats: Parameters to perform normalization. Must fit to normalization type!\n",
    "        :return: DataArray with normalized data.\n",
    "        \"\"\"\n",
    "        # sanity checks\n",
    "        #if not isinstance(data, xr.DataArray):\n",
    "        #    raise TypeError(f\"Passed data must be a xarray.DataArray, but is of type {str(type(data))}.\")\n",
    "\n",
    "        _ = self._check_norm_dims(data)\n",
    "        # do the computation\n",
    "        norm_stats = self.get_required_stats(data, **stats)\n",
    "        data_norm = self.normalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_norm\n",
    "\n",
    "    def denormalize(self, data: da_or_ds, **stats):\n",
    "        \"\"\"\n",
    "        Denormalize data.\n",
    "        :param data: The DataArray to be denormalized.\n",
    "        :param **stats: Parameters to perform denormalization. Must fit to normalization type!\n",
    "        :return: DataArray with denormalized data.\n",
    "        \"\"\"\n",
    "        # sanity checks\n",
    "        #if not isinstance(data, xr.DataArray):\n",
    "        #    raise TypeError(f\"Passed data must be a xarray.DataArray, but is of type {str(type(data))}.\")\n",
    "\n",
    "        _ = self._check_norm_dims(data)\n",
    "        # do the computation\n",
    "        norm_stats = self.get_required_stats(data, **stats)\n",
    "        data_denorm = self.denormalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_denorm\n",
    "\n",
    "    @property\n",
    "    def norm_dims(self):\n",
    "        return self._norm_dims\n",
    "\n",
    "    @norm_dims.setter\n",
    "    def norm_dims(self, norm_dims):\n",
    "        if norm_dims is None:\n",
    "            raise AttributeError(\"norm_dims must not be None. Please parse a list of dimensions\" +\n",
    "                                 \"over which normalization should be applied.\")\n",
    "\n",
    "        self._norm_dims = list(norm_dims)\n",
    "\n",
    "    def _check_norm_dims(self, data):\n",
    "        \"\"\"\n",
    "        Check if dimension for normalization reside in dimensions of data.\n",
    "        :param data: the data (xr.DataArray) to be normalized\n",
    "        :return True: in case of passed check, a ValueError is risen else\n",
    "        \"\"\"\n",
    "        data_dims = list(data.dims)\n",
    "        norm_dims_check = [norm_dim in data_dims for norm_dim in self.norm_dims]\n",
    "        if not all(norm_dims_check):\n",
    "            imiss = np.where(~np.array(norm_dims_check))[0]\n",
    "            miss_dims = list(np.array(self.norm_dims)[imiss])\n",
    "            raise ValueError(\"The following dimensions do not reside in the data: \" +\n",
    "                             f\"{', '.join(miss_dims)}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save_norm_to_file(self, js_file, missdir_ok: bool = True):\n",
    "        \"\"\"\n",
    "        Write normalization parameters to file.\n",
    "        :param js_file: Path to JSON-file to be created.\n",
    "        :param missdir_ok: If True, base-directory of JSON-file can be missing and will be created then.\n",
    "        :return: -\n",
    "        \"\"\"\n",
    "        if self.norm_stats is None:\n",
    "            raise AttributeError(\"norm_stats is still None. Please run (de-)normalization to get parameters.\")\n",
    "\n",
    "        if any([stat is None for stat in self.norm_stats.values()]):\n",
    "            raise AttributeError(\"Some parameters of norm_stats are None.\")\n",
    "\n",
    "        norm_serialized = {key: da.to_dict() for key, da in self.norm_stats.items()}\n",
    "\n",
    "        # serialization and (later) deserialization depends from data type.\n",
    "        # Thus, we have to save it to the dictionary\n",
    "        d0 = list(data_norm.norm_stats.values())[0]\n",
    "        if isinstance(d0, xr.DataArray):\n",
    "            norm_serialized[\"data_type\"]= \"data_array\"\n",
    "        elif isinstance(d0, xr.Dataset):\n",
    "            norm_serialized[\"data_type\"]= \"data_set\"\n",
    "        \n",
    "        if missdir_ok: os.makedirs(os.path.dirname(js_file), exist_ok=True)\n",
    "\n",
    "        with open(js_file, \"w\") as jsf:\n",
    "            js.dump(norm_serialized, jsf)\n",
    "\n",
    "    def read_norm_from_file(self, js_file):\n",
    "        \"\"\"\n",
    "        Read normalization parameters from file. Inverse function to write_norm_from_file.\n",
    "        :param js_file: Path to JSON-file to be read.\n",
    "        :return: Parameters set to self.norm_stats\n",
    "        \"\"\"\n",
    "        with open(js_file, \"r\") as jsf:\n",
    "            norm_data = js.load(jsf)\n",
    "\n",
    "        data_type = norm_data.pop('data_type', None)\n",
    "        \n",
    "        if data_type == \"data_array\":\n",
    "            xr_obj = xr.DataArray\n",
    "        elif data_type == \"data_set\":\n",
    "            xr_obj = xr.Dataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown data_type {data_type} in {js_file}. Only 'data_array' or 'data_set' are allowed.\")\n",
    "\n",
    "        norm_data.pop('data_type', None)\n",
    "            \n",
    "        norm_dict_restored = {key: xr_obj.from_dict(da_dict) for key, da_dict in norm_data.items()}\n",
    "\n",
    "        self.norm_stats = norm_dict_restored\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_required_stats(self, data, *stats):\n",
    "        \"\"\"\n",
    "        Function to retrieve either normalization parameters from data or from keyword arguments\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def normalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to normalize data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def denormalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to denormalize data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class ZScore(Normalize):\n",
    "    def __init__(self, norm_dims: List):\n",
    "        super().__init__(\"z_score\", norm_dims)\n",
    "        self.norm_stats = {\"mu\": None, \"sigma\": None}\n",
    "\n",
    "    def get_required_stats(self, data: xr.DataArray, **stats):\n",
    "        \"\"\"\n",
    "        Get required parameters for z-score normalization. They are either computed from the data\n",
    "        or can be parsed as keyword arguments.\n",
    "        :param data: the data to be (de-)normalized\n",
    "        :param stats: keyword arguments for mean (mu) and standard deviation (std) used for normalization\n",
    "        :return (mu, sigma): Parameters for normalization\n",
    "        \"\"\"\n",
    "        mu, std = stats.get(\"mu\", self.norm_stats[\"mu\"]), stats.get(\"sigma\", self.norm_stats[\"sigma\"])\n",
    "\n",
    "        if mu is None or std is None:\n",
    "            print(\"Retrieve mu and sigma from data...\")\n",
    "            mu, std = data.mean(self.norm_dims), data.std(self.norm_dims)\n",
    "            # the following ensure that both parameters are computed in one graph! \n",
    "            # This significantly reduces memory footprint as we don't end up having data duplicates \n",
    "            # in memory due to multiple graphs (and also seem to enfore usage of data chunks as well)\n",
    "            mu, std = dask.compute(mu, std)\n",
    "            self.norm_stats = {\"mu\": mu, \"sigma\": std}\n",
    "        # else:\n",
    "        #    print(\"Mu and sigma are parsed for (de-)normalization.\")\n",
    "        \n",
    "        return mu, std\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(data, mu, std):\n",
    "        \"\"\"\n",
    "        Perform z-score normalization on data\n",
    "        :param data: Data array of interest\n",
    "        :param mu: mean of data for normalization\n",
    "        :param std: standard deviation of data for normalization\n",
    "        :return data_norm: normalized data\n",
    "        \"\"\"\n",
    "        data_norm = (data - mu) / std\n",
    "\n",
    "        return data_norm\n",
    "\n",
    "    @staticmethod\n",
    "    def denormalize_data(data, mu, std):\n",
    "        \"\"\"\n",
    "        Perform z-score denormalization on data.\n",
    "        :param data: Data array of interest\n",
    "        :param mu: mean of data for denormalization\n",
    "        :param std: standard deviation of data for denormalization\n",
    "        :return data_norm: denormalized data\n",
    "        \"\"\"\n",
    "        data_denorm = data * std + mu\n",
    "\n",
    "        return data_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d82f92-0700-4c9f-9582-a894611a50b8",
   "metadata": {},
   "source": [
    "### New auxiliary function\n",
    "Besides, the following should be added to to be added to `other_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d40f93-41cf-464e-9d0a-700ed0e4a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_divisor(n1, div):\n",
    "    \n",
    "    def getDivisors(n, res=None) : \n",
    "        res = res or []\n",
    "        i = 1\n",
    "        while i <= n : \n",
    "            if (n % i==0) : \n",
    "                res.append(i), \n",
    "            i = i + 1\n",
    "        return res\n",
    "    \n",
    "    all_divs = getDivisors(n1)\n",
    "    \n",
    "    if div in all_divs:\n",
    "        return div\n",
    "    else:\n",
    "        i = np.argmin(np.abs(np.array(all_divs) - div))\n",
    "        print(a)\n",
    "        return all_divs[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b58f88-d002-447b-8d7d-05927e42b5d4",
   "metadata": {},
   "source": [
    "### Running the experiment\n",
    "\n",
    "The corresponding StreamMonthlyNetCDF-class is set-up in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f5db1a-6390-463d-a69e-d7715f7d91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = ZScore([\"time\", \"rlat\", \"rlon\"])\n",
    "data_norm.read_norm_from_file(\"./norm_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a99479-68ee-4be0-a72b-c70e8f39c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from other_utils import to_list\n",
    "from functools import partial\n",
    "\n",
    "class StreamMonthlyNetCDF(object):\n",
    "    # TO-DO:\n",
    "    # - get samples_per_file from the data rather than predefining it (varying samples per file for monthly data files!)\n",
    "\n",
    "    def __init__(self, datadir, patt, nfiles_merge: int, sample_dim: str = \"time\", selected_predictors: List = None,\n",
    "                 selected_predictands: List = None, var_tar2in: str = None, norm_dims: List = None, norm_obj = None):\n",
    "        self.data_dir = datadir\n",
    "        self.file_list = patt\n",
    "        self.nfiles = len(self.file_list)\n",
    "        self.file_list_random = random.sample(self.file_list, self.nfiles)\n",
    "        self.nfiles2merge = nfiles_merge\n",
    "        self.nfiles_merged = int(self.nfiles/self.nfiles2merge)\n",
    "        self.samples_merged = self.get_samples_per_merged_file()\n",
    "        self.predictor_list = selected_predictors\n",
    "        self.predictand_list = selected_predictands\n",
    "        self.n_predictands, self.n_predictors = len(self.predictand_list), len(self.predictor_list)\n",
    "        self.all_vars = self.predictor_list + self.predictand_list\n",
    "        self.ds_all = xr.open_mfdataset(list(self.file_list), decode_cf=False, data_vars=self.all_vars)  # , parallel=True)\n",
    "        self.var_tar2in = var_tar2in\n",
    "        if self.var_tar2in is not None:\n",
    "            self.n_predictors += len(to_list(self.var_tar2in))\n",
    "        self.sample_dim = sample_dim\n",
    "        self.nsamples = self.ds_all.dims[sample_dim]\n",
    "        self.data_dim = self.get_data_dim()\n",
    "        print(\"Start computing normalization parameters.\")\n",
    "        t0 = timer()\n",
    "        if norm_obj is None:                        # TO-DO: remove usgae of norm_obj\n",
    "            self.data_norm = ZScore(norm_dims)      # TO-DO: Allow for arbitrary normalization\n",
    "            self.norm_params = self.data_norm.get_required_stats(self.ds_all)\n",
    "        else:\n",
    "            self.data_norm = norm_obj\n",
    "        self.normalization_time = timer() - t0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "    def getitems(self, indices):\n",
    "        da_now = self.data.isel({self.sample_dim: indices}).to_array(\"variables\")\n",
    "        if self.var_tar2in is not None:\n",
    "            da_now = xr.concat([da_now, da_now.sel({\"variables\": self.var_tar2in})], dim=\"variables\")\n",
    "        return da_now.transpose(..., \"variables\")\n",
    "\n",
    "    def get_data_dim(self):\n",
    "        \"\"\"\n",
    "        Retrieve the dimensionality of the data to be handled, i.e. without sample_dim which will be batched in a\n",
    "        data stream.\n",
    "        :return: tuple of data dimensions\n",
    "        \"\"\"\n",
    "        # get existing dimension names and remove sample_dim\n",
    "        dimnames = list(self.ds_all.coords)\n",
    "        dimnames.remove(self.sample_dim)\n",
    "\n",
    "        # get the dimensionality of the data of interest\n",
    "        all_dims = dict(self.ds_all.dims)\n",
    "        data_dim = itemgetter(*dimnames)(all_dims)\n",
    "\n",
    "        return data_dim\n",
    "    \n",
    "    def get_samples_per_merged_file(self):\n",
    "        nsamples_merged = []\n",
    "        \n",
    "        for i in range(self.nfiles_merged):\n",
    "            file_list_now = self.file_list_random[i*self.nfiles2merge : (i+1)*self.nfiles2merge]\n",
    "            ds_now = xr.open_mfdataset(list(file_list_now), decode_cf=False)\n",
    "            nsamples_merged.append(ds_now.dims[\"time\"])                         # To-Do avoid hard-coding\n",
    "            \n",
    "        return max(nsamples_merged)\n",
    "        \n",
    "            \n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    @data_dir.setter\n",
    "    def data_dir(self, datadir):\n",
    "        if not os.path.isdir(datadir):\n",
    "            raise NotADirectoryError(f\"Parsed data directory '{datadir}' does not exist.\")\n",
    "\n",
    "        self._data_dir = datadir\n",
    "\n",
    "    @property\n",
    "    def file_list(self):\n",
    "        return self._file_list\n",
    "\n",
    "    @file_list.setter\n",
    "    def file_list(self, patt):\n",
    "        patt = patt if patt.endswith(\".nc\") else f\"{patt}.nc\"\n",
    "        files = glob.glob(os.path.join(self.data_dir, patt))\n",
    "\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Could not find any files with pattern '{patt}' under '{self.data_dir}'.\")\n",
    "\n",
    "        self._file_list = list(np.asarray(sorted(files, key=lambda s: int(re.search(r'\\d+', os.path.basename(s)).group()))))\n",
    "\n",
    "    @property\n",
    "    def nfiles2merge(self):\n",
    "        return self._nfiles2merge\n",
    "    \n",
    "    @nfiles2merge.setter\n",
    "    def nfiles2merge(self, n2merge):\n",
    "        n = find_closest_divisor(self.nfiles, n2merge)\n",
    "        if n != n2merge:\n",
    "            print(f\"{n2merge} is not a divisor of the total number of files. Value is changed to {n}\")\n",
    "        \n",
    "        self._nfiles2merge = n\n",
    "    \n",
    "    @property\n",
    "    def sample_dim(self):\n",
    "        return self._sample_dim\n",
    "\n",
    "    @sample_dim.setter\n",
    "    def sample_dim(self, sample_dim):\n",
    "        if not sample_dim in self.ds_all.dims:\n",
    "            raise KeyError(f\"Could not find dimension '{sample_dim}' in data.\")\n",
    "\n",
    "        self._sample_dim = sample_dim\n",
    "\n",
    "    @property\n",
    "    def predictor_list(self):\n",
    "        return self._predictor_list\n",
    "\n",
    "    @predictor_list.setter\n",
    "    def predictor_list(self, selected_predictors: List):\n",
    "        \"\"\"\n",
    "        Initalizes predictor list. In case that selected_predictors is set to None, all variables with suffix `_in` in their names are selected.\n",
    "        In case that a list of selected_predictors is parsed, their availability is checked.\n",
    "        :param selected_predictors: list of predictor variables or None\n",
    "        \"\"\"\n",
    "        self._predictor_list = self.check_and_choose_vars(selected_predictors, \"_in\")\n",
    "        \n",
    "    @property\n",
    "    def predictand_list(self):\n",
    "        return self._predictand_list\n",
    "    \n",
    "    @predictand_list.setter\n",
    "    def predictand_list(self, selected_predictands: List):\n",
    "        self._predictand_list = self.check_and_choose_vars(selected_predictands, \"_tar\")\n",
    "        \n",
    "    def check_and_choose_vars(self, var_list: List, suffix: str = \"*\"):\n",
    "        \"\"\"\n",
    "        Checks list of variables for availability or retrieves all variables named with a given suffix (for var_list = None)\n",
    "        :param var_list: list of predictor variables or None\n",
    "        :param suffix: optional suffix of variables to selected. Only effective if var_list is None.\n",
    "        :return selected_vars: list of selected variables\n",
    "        \"\"\"\n",
    "        ds_test = xr.open_dataset(self.file_list[0])\n",
    "        all_vars = list(ds_test.variables)\n",
    "        \n",
    "        if var_list is None:\n",
    "            selected_vars = [var for var in all_vars if var.endswith(suffix)]\n",
    "        else:\n",
    "            stat_list = [var in all_vars for var in var_list]         \n",
    "            if all(stat_list):\n",
    "                selected_vars = var_list\n",
    "            else:\n",
    "                miss_inds = [i for i, x in enumerate(stat_list) if x]\n",
    "                miss_vars = [var_list[i] for i in miss_inds]\n",
    "                raise ValueError(f\"Could not find the following variables in the dataset: {*miss_vars,}\")\n",
    "        \n",
    "        return selected_vars\n",
    "    \n",
    "    @staticmethod\n",
    "    def _preprocess_ds(ds, data_norm):\n",
    "        ds = data_norm.normalize(ds)\n",
    "        return ds.astype(\"float32\")\n",
    "\n",
    "    def read_netcdf(self, ind):\n",
    "        ind = tf.keras.backend.get_value(ind)\n",
    "        ind = int(str(ind).lstrip(\"b'\").rstrip(\"'\"))        \n",
    "        print(f\"Load data from {ind}th set of files...\")\n",
    "        file_list_now = self.file_list[ind*self.nfiles2merge:(ind+1)*self.nfiles2merge]\n",
    "        # read the normalized data into memory\n",
    "        ds_now = xr.open_mfdataset(list(file_list_now), decode_cf=False, data_vars=self.all_vars, \n",
    "                                   preprocess=partial(StreamMonthlyNetCDF._preprocess_ds, data_norm=ds_obj.data_norm), parallel=True).load()\n",
    "        print(\"Data loaded successfully from.\")\n",
    "        #da_now = ds_now.to_array(\"variables\").astype(\"float32\", copy=False)\n",
    "        nsamples = ds_now.dims[self.sample_dim]\n",
    "        if nsamples < self.samples_merged:\n",
    "            t0 = timer()\n",
    "            add_samples = self.samples_merged - nsamples\n",
    "            print(f\"Add {add_samples:d} samples to dataset.\")\n",
    "            add_inds = random.sample(range(nsamples), add_samples)\n",
    "            ds_add = ds_now.isel({self.sample_dim: add_inds})\n",
    "            ds_add[self.sample_dim] = ds_add[self.sample_dim] + 1.\n",
    "            ds_now = xr.concat([ds_now, ds_add], dim=self.sample_dim)\n",
    "            print(f\"Appending data took {timer()-t0:.2f}s.\")\n",
    "            \n",
    "        self.data = ds_now#xr.concat([da_now, da_now.sel({\"variables\": \"hsurf_tar\"})], dim=\"variables\")\n",
    "\n",
    "        return True\n",
    "\n",
    "datadir2 = \"/p/cscratch/fs/deepacf//maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files_copy/\"\n",
    "ds_obj = StreamMonthlyNetCDF(os.path.join(datadir2), \"downscaling_tier2_train*.nc\", 30, norm_dims = [\"time\", \"rlat\", \"rlon\"], norm_obj=data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4faab5a-069a-422f-b2d5-4ada06351d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38930806517601e-06\n",
      "{'mu': <xarray.Dataset>\n",
      "Dimensions:       ()\n",
      "Data variables:\n",
      "    rotated_pole  float64 1.0\n",
      "    2t_in         float64 281.5\n",
      "    sshf_in       float64 -4.993e+04\n",
      "    slhf_in       float64 -1.643e+05\n",
      "    blh_in        float64 512.4\n",
      "    10u_in        float64 0.7001\n",
      "    10v_in        float64 0.3209\n",
      "    z_in          float64 5.686e+03\n",
      "    t850_in       float64 277.5\n",
      "    t925_in       float64 281.4\n",
      "    hsurf_tar     float64 571.6\n",
      "    t_2m_tar      float64 282.0, 'sigma': <xarray.Dataset>\n",
      "Dimensions:       ()\n",
      "Data variables:\n",
      "    rotated_pole  float64 0.0\n",
      "    2t_in         float64 8.401\n",
      "    sshf_in       float64 1.934e+05\n",
      "    slhf_in       float64 2.493e+05\n",
      "    blh_in        float64 476.3\n",
      "    10u_in        float64 2.356\n",
      "    10v_in        float64 1.918\n",
      "    z_in          float64 4.485e+03\n",
      "    t850_in       float64 6.883\n",
      "    t925_in       float64 7.484\n",
      "    hsurf_tar     float64 497.5\n",
      "    t_2m_tar      float64 8.482}\n"
     ]
    }
   ],
   "source": [
    "data_norm = ds_obj.data_norm\n",
    "print(ds_obj.normalization_time)\n",
    "print(data_norm.norm_stats)\n",
    "#data_norm.save_norm_to_file(\"./norm_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92140d-2673-40f9-a027-05a201f3aced",
   "metadata": {},
   "source": [
    "Next, we run the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e486317-c38e-4d52-8a63-36627634a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "! jutil env activate -p deepacf\n",
    "#! export datadir1=\"${CSCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files_copy/\"; echo $datadir1; /opt/ddn/ime/bin/ime-ctl --prestage ${datadir1}/downscaling_tier2_train*.nc\n",
    "#! datadir1=\"${SCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files_copy/\"; ls ${datadir1}\n",
    "! jutil env activate -p deepacf; /opt/ddn/ime/bin/ime-ctl --frag-stat ${CSCRATCH}/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier2/monthly_files_copy/downscaling_tier2_train_9.nc\n",
    "! echo $HOSTNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e724ecb-f682-4df8-979e-88da47886ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_read_nc = lambda ind_set: tf.py_function(ds_obj.read_netcdf, [ind_set], tf.bool)\n",
    "tf_getdata = lambda i: tf.numpy_function(ds_obj.getitems, [i], tf.float32)\n",
    "tf_split = lambda arr: (arr[..., 0:-ds_obj.n_predictands], arr[...,-ds_obj.n_predictands:])\n",
    "\n",
    "tfds = tf.data.Dataset.range(int(ds_obj.nfiles_merged)).map(tf_read_nc)\n",
    "tfds = tfds.flat_map(lambda x: tf.data.Dataset.range(ds_obj.samples_merged).shuffle(ds_obj.samples_merged)\\\n",
    "                       .batch(bs_train, drop_remainder=True).map(tf_getdata))#.map(tf_norm))\n",
    "tfds = tfds.map(tf_split).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f9d3ac-0ba9-4a7a-8da9-90d79cda469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from 0th set of files...\n",
      "Data loaded successfully from.\n",
      "Add 121 samples to dataset.\n",
      "Appending data took 3.83s.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "Load data from 1th set of files...\n",
      "132\n",
      "Data loaded successfully from.\n",
      "Add 120 samples to dataset.\n",
      "Appending data took 3.84s.\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "Iteration for xarray_mfdataset-test took 78.10s.\n"
     ]
    }
   ],
   "source": [
    "exp_time = run_test(tfds, test_case=\"xarray_mfdataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5babf398-0af7-4265-bba8-2ae3c5243e8d",
   "metadata": {},
   "source": [
    "... and find that data streaming is reasonably quick. It takes only about 80s for 250 mini-batches. This is only a bit slower than the test with vectorized indexing, see above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81300768-9b22-4d93-89bb-b31e5741310d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
