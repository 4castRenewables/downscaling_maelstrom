{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-elevation",
   "metadata": {},
   "source": [
    "# Benchmarking the application \"Downscaling of 2m temperature from IFS HRES with a U-Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install climetlab==0.8.14\n",
    "!pip install climetlab-maelstrom-downscaling==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lucky-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.utils as ku\n",
    "sys.path += [\"../handle_data/\", \"../models\", \"../postprocess/\"]\n",
    "from handle_data_unet import *\n",
    "from unet_model import build_unet, get_lr_scheduler\n",
    "import xarray as xr\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "whole-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%get_data: Start reading the data from '/p/project/deepacf/maelstrom/data/downscaling_unet/'...\n",
      "%get_data: Dataset was retrieved succesfully.\n",
      "%get_data: Start reading the data from '/p/project/deepacf/maelstrom/data/downscaling_unet/'...\n",
      "%get_data: Dataset was retrieved succesfully.\n",
      "%get_data: Start reading the data from '/p/project/deepacf/maelstrom/data/downscaling_unet/'...\n",
      "%get_data: Dataset was retrieved succesfully.\n"
     ]
    }
   ],
   "source": [
    "datadir = \"/p/project/deepacf/maelstrom/data/downscaling_unet/\"\n",
    "\n",
    "data_obj = HandleUnetData(datadir, \"train\")\n",
    "data_obj.append_data(\"val\")\n",
    "data_obj.append_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "refined-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loading_times': {'train': 1.59577553300187, 'val': 0.2812669249251485, 'test': 0.27267765579745173}, 'normalizing_train': 0.8997006029821932, 'normalizing_val': 0.08563442481681705}\n",
      "{'train': 575681728, 'val': 73141456, 'test': 70782112}\n",
      "{'train': 1464, 'val': 186, 'test': 180}\n"
     ]
    }
   ],
   "source": [
    "# set daytime for which downsclaing model is trained (i.e. either 0 or 12)\n",
    "hour = 12    \n",
    "\n",
    "# preprocess data for training\n",
    "int_data, tart_data, opt_norm = data_obj.normalize(\"train\", daytime=12)\n",
    "inv_data, tarv_data = data_obj.normalize(\"val\", daytime=hour, opt_norm=opt_norm)\n",
    "\n",
    "print(data_obj.timing)\n",
    "print(data_obj.data_info[\"memory_datasets\"])\n",
    "print(data_obj.data_info[\"nsamples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thermal-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.utils as ku\n",
    "shape_in = (96, 128, 3)\n",
    "\n",
    "if \"login\" in data_obj.host:\n",
    "    unet_model = build_unet(shape_in, z_branch=True)\n",
    "    ku.plot_model(unet_model, to_file=os.path.join(os.getcwd(), \"unet_downscaling_model.png\"), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "popular-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class for creating timer callback\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "z_branch = True                    # flag if additionally training on surface elevation is performed\n",
    "\n",
    "lr_scheduler, time_tracker = get_lr_scheduler(), TimeHistory()\n",
    "# create callbacks\n",
    "callback_list = [lr_scheduler, time_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "otherwise-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "23/23 [==============================] - 3s 150ms/step - loss: 0.6055 - output_temp_loss: 0.3851 - output_z_loss: 0.2203 - val_loss: 8.1546 - val_output_temp_loss: 2.7239 - val_output_z_loss: 5.4307\n",
      "Epoch 2/70\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3125 - output_temp_loss: 0.2292 - output_z_loss: 0.0834 - val_loss: 2.4209 - val_output_temp_loss: 0.9422 - val_output_z_loss: 1.4787\n",
      "Epoch 3/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2685 - output_temp_loss: 0.2073 - output_z_loss: 0.0612 - val_loss: 0.9981 - val_output_temp_loss: 0.4703 - val_output_z_loss: 0.5279\n",
      "Epoch 4/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2432 - output_temp_loss: 0.1832 - output_z_loss: 0.0600 - val_loss: 0.6239 - val_output_temp_loss: 0.2989 - val_output_z_loss: 0.3250\n",
      "Epoch 5/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.2562 - output_temp_loss: 0.2026 - output_z_loss: 0.0536 - val_loss: 0.4892 - val_output_temp_loss: 0.2455 - val_output_z_loss: 0.2437\n",
      "Epoch 6/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2312 - output_temp_loss: 0.1849 - output_z_loss: 0.0462 - val_loss: 0.3909 - val_output_temp_loss: 0.1771 - val_output_z_loss: 0.2138\n",
      "Epoch 7/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2557 - output_temp_loss: 0.2131 - output_z_loss: 0.0426 - val_loss: 0.3041 - val_output_temp_loss: 0.1446 - val_output_z_loss: 0.1595\n",
      "Epoch 8/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2100 - output_temp_loss: 0.1705 - output_z_loss: 0.0395 - val_loss: 0.2702 - val_output_temp_loss: 0.1419 - val_output_z_loss: 0.1282\n",
      "Epoch 9/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2389 - output_temp_loss: 0.2042 - output_z_loss: 0.0347 - val_loss: 0.2452 - val_output_temp_loss: 0.1385 - val_output_z_loss: 0.1066\n",
      "Epoch 10/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2024 - output_temp_loss: 0.1695 - output_z_loss: 0.0329 - val_loss: 0.2325 - val_output_temp_loss: 0.1276 - val_output_z_loss: 0.1049\n",
      "Epoch 11/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2020 - output_temp_loss: 0.1724 - output_z_loss: 0.0296 - val_loss: 0.1848 - val_output_temp_loss: 0.1082 - val_output_z_loss: 0.0766\n",
      "Epoch 12/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.2046 - output_temp_loss: 0.1748 - output_z_loss: 0.0297 - val_loss: 0.1865 - val_output_temp_loss: 0.1084 - val_output_z_loss: 0.0781\n",
      "Epoch 13/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1909 - output_temp_loss: 0.1633 - output_z_loss: 0.0277 - val_loss: 0.1821 - val_output_temp_loss: 0.1173 - val_output_z_loss: 0.0648\n",
      "Epoch 14/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2171 - output_temp_loss: 0.1894 - output_z_loss: 0.0277 - val_loss: 0.1674 - val_output_temp_loss: 0.1059 - val_output_z_loss: 0.0615\n",
      "Epoch 15/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2024 - output_temp_loss: 0.1763 - output_z_loss: 0.0260 - val_loss: 0.1449 - val_output_temp_loss: 0.1032 - val_output_z_loss: 0.0417\n",
      "Epoch 16/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1926 - output_temp_loss: 0.1688 - output_z_loss: 0.0238 - val_loss: 0.1456 - val_output_temp_loss: 0.1049 - val_output_z_loss: 0.0407\n",
      "Epoch 17/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1930 - output_temp_loss: 0.1704 - output_z_loss: 0.0226 - val_loss: 0.1371 - val_output_temp_loss: 0.0998 - val_output_z_loss: 0.0373\n",
      "Epoch 18/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1843 - output_temp_loss: 0.1619 - output_z_loss: 0.0224 - val_loss: 0.1299 - val_output_temp_loss: 0.0985 - val_output_z_loss: 0.0314\n",
      "Epoch 19/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1755 - output_temp_loss: 0.1522 - output_z_loss: 0.0233 - val_loss: 0.1305 - val_output_temp_loss: 0.0949 - val_output_z_loss: 0.0356\n",
      "Epoch 20/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1654 - output_temp_loss: 0.1432 - output_z_loss: 0.0221 - val_loss: 0.1270 - val_output_temp_loss: 0.0968 - val_output_z_loss: 0.0302\n",
      "Epoch 21/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1978 - output_temp_loss: 0.1761 - output_z_loss: 0.0217 - val_loss: 0.1237 - val_output_temp_loss: 0.0947 - val_output_z_loss: 0.0290\n",
      "Epoch 22/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1972 - output_temp_loss: 0.1770 - output_z_loss: 0.0202 - val_loss: 0.1333 - val_output_temp_loss: 0.1060 - val_output_z_loss: 0.0273\n",
      "Epoch 23/70\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.1950 - output_temp_loss: 0.1734 - output_z_loss: 0.0216 - val_loss: 0.1177 - val_output_temp_loss: 0.0915 - val_output_z_loss: 0.0262\n",
      "Epoch 24/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.2108 - output_temp_loss: 0.1908 - output_z_loss: 0.0200 - val_loss: 0.1291 - val_output_temp_loss: 0.1031 - val_output_z_loss: 0.0260\n",
      "Epoch 25/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1707 - output_temp_loss: 0.1505 - output_z_loss: 0.0202 - val_loss: 0.1115 - val_output_temp_loss: 0.0901 - val_output_z_loss: 0.0214\n",
      "Epoch 26/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.2194 - output_temp_loss: 0.1990 - output_z_loss: 0.0204 - val_loss: 0.1116 - val_output_temp_loss: 0.0903 - val_output_z_loss: 0.0213\n",
      "Epoch 27/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1763 - output_temp_loss: 0.1570 - output_z_loss: 0.0192 - val_loss: 0.1096 - val_output_temp_loss: 0.0901 - val_output_z_loss: 0.0196\n",
      "Epoch 28/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1920 - output_temp_loss: 0.1737 - output_z_loss: 0.0183 - val_loss: 0.1120 - val_output_temp_loss: 0.0926 - val_output_z_loss: 0.0194\n",
      "Epoch 29/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1704 - output_temp_loss: 0.1520 - output_z_loss: 0.0184 - val_loss: 0.1072 - val_output_temp_loss: 0.0896 - val_output_z_loss: 0.0176\n",
      "Epoch 30/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2044 - output_temp_loss: 0.1858 - output_z_loss: 0.0186 - val_loss: 0.1099 - val_output_temp_loss: 0.0917 - val_output_z_loss: 0.0181\n",
      "Epoch 31/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1739 - output_temp_loss: 0.1559 - output_z_loss: 0.0180 - val_loss: 0.1090 - val_output_temp_loss: 0.0896 - val_output_z_loss: 0.0194\n",
      "Epoch 32/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1908 - output_temp_loss: 0.1722 - output_z_loss: 0.0185 - val_loss: 0.1045 - val_output_temp_loss: 0.0880 - val_output_z_loss: 0.0164\n",
      "Epoch 33/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1681 - output_temp_loss: 0.1502 - output_z_loss: 0.0179 - val_loss: 0.1083 - val_output_temp_loss: 0.0896 - val_output_z_loss: 0.0187\n",
      "Epoch 34/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1719 - output_temp_loss: 0.1534 - output_z_loss: 0.0185 - val_loss: 0.1050 - val_output_temp_loss: 0.0888 - val_output_z_loss: 0.0162\n",
      "Epoch 35/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1727 - output_temp_loss: 0.1551 - output_z_loss: 0.0177 - val_loss: 0.1073 - val_output_temp_loss: 0.0910 - val_output_z_loss: 0.0163\n",
      "Epoch 36/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1888 - output_temp_loss: 0.1700 - output_z_loss: 0.0187 - val_loss: 0.1048 - val_output_temp_loss: 0.0883 - val_output_z_loss: 0.0165\n",
      "Epoch 37/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1935 - output_temp_loss: 0.1752 - output_z_loss: 0.0183 - val_loss: 0.1045 - val_output_temp_loss: 0.0887 - val_output_z_loss: 0.0158\n",
      "Epoch 38/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1903 - output_temp_loss: 0.1734 - output_z_loss: 0.0169 - val_loss: 0.1028 - val_output_temp_loss: 0.0872 - val_output_z_loss: 0.0156\n",
      "Epoch 39/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1687 - output_temp_loss: 0.1517 - output_z_loss: 0.0170 - val_loss: 0.1033 - val_output_temp_loss: 0.0865 - val_output_z_loss: 0.0168\n",
      "Epoch 40/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1781 - output_temp_loss: 0.1606 - output_z_loss: 0.0175 - val_loss: 0.1041 - val_output_temp_loss: 0.0867 - val_output_z_loss: 0.0174\n",
      "Epoch 41/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1590 - output_temp_loss: 0.1423 - output_z_loss: 0.0167 - val_loss: 0.1003 - val_output_temp_loss: 0.0854 - val_output_z_loss: 0.0150\n",
      "Epoch 42/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1787 - output_temp_loss: 0.1618 - output_z_loss: 0.0168 - val_loss: 0.1024 - val_output_temp_loss: 0.0869 - val_output_z_loss: 0.0155\n",
      "Epoch 43/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2133 - output_temp_loss: 0.1954 - output_z_loss: 0.0180 - val_loss: 0.1025 - val_output_temp_loss: 0.0863 - val_output_z_loss: 0.0162\n",
      "Epoch 44/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1732 - output_temp_loss: 0.1559 - output_z_loss: 0.0173 - val_loss: 0.1007 - val_output_temp_loss: 0.0853 - val_output_z_loss: 0.0155\n",
      "Epoch 45/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1621 - output_temp_loss: 0.1454 - output_z_loss: 0.0167 - val_loss: 0.1007 - val_output_temp_loss: 0.0859 - val_output_z_loss: 0.0148\n",
      "Epoch 46/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1770 - output_temp_loss: 0.1606 - output_z_loss: 0.0163 - val_loss: 0.1001 - val_output_temp_loss: 0.0849 - val_output_z_loss: 0.0152\n",
      "Epoch 47/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1791 - output_temp_loss: 0.1628 - output_z_loss: 0.0164 - val_loss: 0.1023 - val_output_temp_loss: 0.0868 - val_output_z_loss: 0.0155\n",
      "Epoch 48/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1800 - output_temp_loss: 0.1630 - output_z_loss: 0.0169 - val_loss: 0.1099 - val_output_temp_loss: 0.0906 - val_output_z_loss: 0.0192\n",
      "Epoch 49/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1584 - output_temp_loss: 0.1403 - output_z_loss: 0.0181 - val_loss: 0.1029 - val_output_temp_loss: 0.0870 - val_output_z_loss: 0.0159\n",
      "Epoch 50/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1614 - output_temp_loss: 0.1451 - output_z_loss: 0.0163 - val_loss: 0.1027 - val_output_temp_loss: 0.0873 - val_output_z_loss: 0.0154\n",
      "Epoch 51/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1714 - output_temp_loss: 0.1546 - output_z_loss: 0.0167 - val_loss: 0.0995 - val_output_temp_loss: 0.0850 - val_output_z_loss: 0.0145\n",
      "Epoch 52/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1809 - output_temp_loss: 0.1652 - output_z_loss: 0.0158 - val_loss: 0.1028 - val_output_temp_loss: 0.0878 - val_output_z_loss: 0.0149\n",
      "Epoch 53/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1745 - output_temp_loss: 0.1587 - output_z_loss: 0.0158 - val_loss: 0.0994 - val_output_temp_loss: 0.0851 - val_output_z_loss: 0.0143\n",
      "Epoch 54/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1627 - output_temp_loss: 0.1461 - output_z_loss: 0.0166 - val_loss: 0.1002 - val_output_temp_loss: 0.0839 - val_output_z_loss: 0.0163\n",
      "Epoch 55/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1468 - output_temp_loss: 0.1309 - output_z_loss: 0.0159 - val_loss: 0.0982 - val_output_temp_loss: 0.0837 - val_output_z_loss: 0.0145\n",
      "Epoch 56/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1860 - output_temp_loss: 0.1695 - output_z_loss: 0.0165 - val_loss: 0.1059 - val_output_temp_loss: 0.0912 - val_output_z_loss: 0.0147\n",
      "Epoch 57/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1742 - output_temp_loss: 0.1580 - output_z_loss: 0.0162 - val_loss: 0.0979 - val_output_temp_loss: 0.0838 - val_output_z_loss: 0.0141\n",
      "Epoch 58/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1733 - output_temp_loss: 0.1578 - output_z_loss: 0.0155 - val_loss: 0.0999 - val_output_temp_loss: 0.0822 - val_output_z_loss: 0.0177\n",
      "Epoch 59/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2004 - output_temp_loss: 0.1825 - output_z_loss: 0.0179 - val_loss: 0.1000 - val_output_temp_loss: 0.0844 - val_output_z_loss: 0.0156\n",
      "Epoch 60/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1667 - output_temp_loss: 0.1508 - output_z_loss: 0.0159 - val_loss: 0.1055 - val_output_temp_loss: 0.0887 - val_output_z_loss: 0.0168\n",
      "Epoch 61/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2075 - output_temp_loss: 0.1912 - output_z_loss: 0.0163 - val_loss: 0.0970 - val_output_temp_loss: 0.0827 - val_output_z_loss: 0.0143\n",
      "Epoch 62/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1351 - output_temp_loss: 0.1190 - output_z_loss: 0.0162 - val_loss: 0.0953 - val_output_temp_loss: 0.0816 - val_output_z_loss: 0.0136\n",
      "Epoch 63/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1468 - output_temp_loss: 0.1304 - output_z_loss: 0.0164 - val_loss: 0.0967 - val_output_temp_loss: 0.0827 - val_output_z_loss: 0.0140\n",
      "Epoch 64/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1612 - output_temp_loss: 0.1455 - output_z_loss: 0.0157 - val_loss: 0.0972 - val_output_temp_loss: 0.0833 - val_output_z_loss: 0.0139\n",
      "Epoch 65/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1691 - output_temp_loss: 0.1529 - output_z_loss: 0.0162 - val_loss: 0.0997 - val_output_temp_loss: 0.0859 - val_output_z_loss: 0.0138\n",
      "Epoch 66/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1396 - output_temp_loss: 0.1244 - output_z_loss: 0.0153 - val_loss: 0.0947 - val_output_temp_loss: 0.0814 - val_output_z_loss: 0.0133\n",
      "Epoch 67/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1632 - output_temp_loss: 0.1477 - output_z_loss: 0.0155 - val_loss: 0.0976 - val_output_temp_loss: 0.0834 - val_output_z_loss: 0.0142\n",
      "Epoch 68/70\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1832 - output_temp_loss: 0.1680 - output_z_loss: 0.0151 - val_loss: 0.0964 - val_output_temp_loss: 0.0829 - val_output_z_loss: 0.0135\n",
      "Epoch 69/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1608 - output_temp_loss: 0.1452 - output_z_loss: 0.0156 - val_loss: 0.0976 - val_output_temp_loss: 0.0835 - val_output_z_loss: 0.0140\n",
      "Epoch 70/70\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1683 - output_temp_loss: 0.1531 - output_z_loss: 0.0152 - val_loss: 0.0998 - val_output_temp_loss: 0.0847 - val_output_z_loss: 0.0150\n"
     ]
    }
   ],
   "source": [
    "# build, compile and train the model\n",
    "nepochs = 70\n",
    "unet_model = build_unet(shape_in, z_branch=z_branch)\n",
    "if z_branch:\n",
    "    unet_model.compile(optimizer=Adam(learning_rate=5*10**(-4)),\n",
    "                   loss={\"output_temp\": \"mae\", \"output_z\": \"mae\"}, \n",
    "                   loss_weights={\"output_temp\": 1.0, \"output_z\": 1.0})\n",
    "    \n",
    "    history = unet_model.fit(x=int_data.values, y={\"output_temp\": tart_data.isel(variable=0).values,\n",
    "                                                   \"output_z\": tart_data.isel(variable=1).values},\n",
    "                             batch_size=32, epochs=nepochs, callbacks=callback_list, \n",
    "                             validation_data=(inv_data.values, {\"output_temp\": tarv_data.isel(variable=0).values,\n",
    "                                                                \"output_z\": tarv_data.isel(variable=1).values}))\n",
    "else:\n",
    "    unet_model.compile(optimizer=Adam(learning_rate=5*10**(-4)), loss=\"mae\")\n",
    "\n",
    "    history = unet_model.fit(x=int_data.values, y=tart_data.isel(variable=0).values, batch_size=32,\n",
    "                             epochs=nepochs, callbacks=callback_list,\n",
    "                             validation_data=(inv_data.values, tarv_data.isel(variable=0).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_times = time_tracker.epoch_times\n",
    "\n",
    "print(history.history[\"output_temp_loss\"][-1])\n",
    "print(history.history[\"val_output_temp_loss\"][-1])\n",
    "\n",
    "print(\"Total training time: {0:.2f}s\".format(np.sum(epoch_times)))\n",
    "print(\"Max. time per epoch: {0:.4f}s, min. time per epoch: {1:.4f}s\".format(np.amax(epoch_times), np.amin(epoch_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test data first\n",
    "inte_data, tarte_data = data_obj.preprocess_data(\"test\", daytime=hour, opt_norm=opt_norm)\n",
    "\n",
    "# generate the downscaled fields\n",
    "y_pred_test = unet_model.predict(inte_data.values, verbose=1)\n",
    "y_pred_val = unet_model.predict(inv_data.values, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_type = \"validation\"            # change here to switch between validation and testing data\n",
    "if comparison_type == \"validation\":\n",
    "  y_pred = y_pred_val\n",
    "  ds_ref = data_obj.data[\"val\"].sel(time=dt.time(hour))\n",
    "  var_ref = tarv_data.isel(variable=0)\n",
    "elif comparison_type == \"test\":\n",
    "  y_pred = y_pred_test\n",
    "  ds_ref = data_obj.data[\"test\"].sel(time=dt.time(hour))\n",
    "  var_ref = tarte_data.isel(variable=0)\n",
    "else:\n",
    "  ValueError(\"Unknown comparison_type '{0}' chosen.\".format(comparison_type))\n",
    "\n",
    "if np.ndim(y_pred) == 5:                # cropping necessary if z_branch is True (two output channels)\n",
    "  y_pred = y_pred[0]\n",
    "else:\n",
    "  pass\n",
    "\n",
    "print(np.abs(np.squeeze(y_pred) - var_ref).mean(dim=[\"lat\", \"lon\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some relevant information from the original dataset, ...\n",
    "coords = var_ref.squeeze().coords\n",
    "dims = var_ref.squeeze().dims\n",
    "\n",
    "# denomralize...\n",
    "y_pred_trans = np.squeeze(y_pred)*opt_norm[\"std_tar\"].squeeze().values + opt_norm[\"mu_tar\"].squeeze().values\n",
    "# and make xarray DataArray \n",
    "y_pred_trans = xr.DataArray(y_pred_trans, coords=coords, dims=dims, name=\"t2m_downscaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((y_pred_trans - ds_ref[\"t2m_tar\"])**2).mean(dim=[\"lat\", \"lon\"])\n",
    "\n",
    "print(\"MSE of downscaled 2m temperature: {0:.3f} K**2 (+/-{1:.3f} K**2)\".format(mse.mean().values, mse.std().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-victoria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "christian-grave",
   "metadata": {},
   "source": [
    "As we see, the model has learned to recover a lot of details resulting mainly from the topography. Especially over the Alpes, but also over the the German low mountain ranges, the differences have become smaller and less structured. It is also noted that the differences near the coast (e.g. at the Baltic Sea) have become smaller. <br>\n",
    "However, some systematic features are still visible, the differences can stilll be as large as 3 K and especially in the Alps, the differences are somehow 'blurry'. Thus, there is still room for further improvement. \n",
    "These improvements will not only pertain the model architecture, but will also target to engulf more meteorological variables. The latter will also enable the network to generalize with respect to daytime and season. Note, that this has not been done yet, since we trained the U-net with data between April and September at 12 UTC only.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.0",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
