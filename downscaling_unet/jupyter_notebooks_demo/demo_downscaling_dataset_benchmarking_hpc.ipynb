{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minor-dollar",
   "metadata": {},
   "source": [
    "# Benchmarking the application \"Downscaling of 2m temperature from IFS HRES with a U-Net\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absent-ukraine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 18:00:46.259675: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import time\n",
    "from downscaling_utils import *\n",
    "from unet_model import build_unet\n",
    "import xarray as xr\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-pasta",
   "metadata": {},
   "source": [
    "... we define to auxiliary functions that accomplish this job for us. Note that the function `get_ifs_data`automatically detects the running node to decide if data can be downloaded or if it must be available on scratch. If the data is unavailable in the file system, execution on the login node to acquire the data is **mandatory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-yeast",
   "metadata": {},
   "source": [
    "Now, let's get the data. For this, please **adapt the `datadir`-variable** if you don't have access to `/p/project/deepacf/maelstrom/data/downscaling_unet/` (check via terminal). In this case, also run on the **login node first** to download the data. If you have access (or if you already obtained the data), the data will be loaded from the filesystem automatically (also on the computing node). <br>\n",
    "For convenience, we will also take a brief look on the training data which comprises 1464 time steps over our target domain with 128x96 grid points in zonal and meridional direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respected-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%set_download_flag: Datafiles are already available under '/p/project/deepacf/maelstrom/data/downscaling_unet/'\n",
      "%get_data: Start reading the data from '/p/project/deepacf/maelstrom/data/downscaling_unet/'...\n",
      "5.0438979608006775\n"
     ]
    }
   ],
   "source": [
    "datadir = \"/p/project/deepacf/maelstrom/data/downscaling_unet/\"\n",
    "\n",
    "data_obj = DownscalingData(datadir)\n",
    "\n",
    "print(data_obj.loading_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-darwin",
   "metadata": {},
   "source": [
    "### Details on the data (additional information)\n",
    "\n",
    "- Input variables are denoted with the suffix `_in`, whereas the target, output variables carry `_tar` as suffix\n",
    "- The data is given at the analysis time of the IFS HRES model, that is at 00 UTC and 12 UTC. Besides, the data coverage is limited to the summer season defined as the period between April and September, inclusively\n",
    "- Spatially, the data is given on a limited domain over Central Europe with 128x96 grid points in zonal and meridional direction, respectively\n",
    "- The underlying spherical grid is the same for the input and output data as suggested by Sha et al., 2020 [1] for their U-net. However the information content differs: \n",
    "    - While the output data is directly obtained from the IFS HRES data (with only slight remapping onto a regular spherical grid), the input data has been preprocessed as follows:\n",
    "    - coarsening onto a grid whose spacing is eight times larger than the original data via conservative remapping using CDO [2]\n",
    "    - bilinear interpolation back onto the original grid \n",
    "- Note that the first step removes the spatial variability of the data on spatial scales smaller than the coarse-grained grid spacing. This step cannot be reverted via bilinear interpolation and therefore imitates effectively a model configuration of IFS HRES whose grid spacing is larger by a factor of eight than the original one\n",
    "\n",
    "The latter statement can be illustrated by plotting both meteorological data fields, the input and target 2m temperature.\n",
    "<br><br>\n",
    "[1] Sha, Yingkai, et al. \"Deep-learning-based gridded downscaling of surface meteorological variables in complex terrain. Part I: Daily maximum and minimum 2-m temperature.\" Journal of Applied Meteorology and Climatology 59.12 (2020): 2057-2073. <a href=\"https://doi.org/10.1175/JAMC-D-20-0057.1\">DOI</a> <br>\n",
    "[2] Schulzweida, Uwe. (2019, October 31). CDO User Guide (Version 1.9.8). <a href=\"http://doi.org/10.5281/zenodo.3539275\">DOI</a>\n",
    "\n",
    "### Plotting the data (additional information)\n",
    "\n",
    "First, we define some auxiliary functions. If you are not interested in the details of the plotting procedure, you may skip the following cells. However, don't forget to run it, so that the routines become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hostname (to suppress querying border- and coaslines on computing nodes without web access)\n",
    "host = os.getenv('HOSTNAME')\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# for querying dictionary\n",
    "def provide_default(dict_in, keyname, default=None, required=False):\n",
    "    \"\"\"\n",
    "    Returns values of key from input dictionary or alternatively its default\n",
    "\n",
    "    :param dict_in: input dictionary\n",
    "    :param keyname: name of key which should be added to dict_in if it is not already existing\n",
    "    :param default: default value of key (returned if keyname is not present in dict_in)\n",
    "    :param required: Forces existence of keyname in dict_in (otherwise, an error is returned)\n",
    "    :return: value of requested key or its default retrieved from dict_in\n",
    "    \"\"\"\n",
    "\n",
    "    if not required and default is None:\n",
    "        raise ValueError(\"Provide default when existence of key in dictionary is not required.\")\n",
    "        \n",
    "    if keyname not in dict_in.keys():\n",
    "        if required:\n",
    "            print(dict_in)\n",
    "            raise ValueError(\"Could not find '{0}' in input dictionary.\".format(keyname))\n",
    "        return default\n",
    "    else:\n",
    "        return dict_in[keyname]\n",
    "\n",
    "# auxiliary function for colormap\n",
    "def get_colormap_temp(levels = None):\n",
    "    \"\"\"\n",
    "    Get a nice colormap for plotting topographic height\n",
    "    :param levels: level boundaries\n",
    "    :return cmap: colormap-object\n",
    "    :return norm: normalization object corresponding to colormap and levels\n",
    "    \"\"\"\n",
    "    bounds = np.asarray(levels)\n",
    "        \n",
    "    nbounds = len(bounds)\n",
    "    col_obj = mpl.cm.PuOr_r(np.linspace(0., 1., nbounds))\n",
    "    \n",
    "    # create colormap and corresponding norm\n",
    "    cmap = mpl.colors.ListedColormap(col_obj, name=\"temp\" + \"_map\")\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)    \n",
    "    \n",
    "    return cmap, norm, bounds\n",
    "\n",
    "# for making plot nice\n",
    "def decorate_plot(ax_plot, plot_xlabel=True, plot_ylabel=True):\n",
    "        \n",
    "    fs = 16\n",
    "    if \"login\" in host:\n",
    "        # add nice coast- and borderlines\n",
    "        ax_plot.coastlines(linewidth=0.75)\n",
    "        ax_plot.coastlines(linewidth=0.75)\n",
    "        ax_plot.add_feature(cartopy.feature.BORDERS)\n",
    "        \n",
    "    # adjust extent and ticks as well as axis-label\n",
    "    ax_plot.set_xticks(np.arange(0., 360. + 0.1, 5.))  # ,crs=projection_crs)\n",
    "    ax_plot.set_yticks(np.arange(-90., 90. + 0.1, 5.))  # ,crs=projection_crs)\n",
    "\n",
    "    ax_plot.set_extent([3.5, 17., 44.5, 55.])#, crs=prj_crs)\n",
    "    ax_plot.minorticks_on()\n",
    "    ax_plot.tick_params(axis=\"both\", which=\"both\", direction=\"out\", labelsize=12)\n",
    "\n",
    "    # some labels\n",
    "    if plot_xlabel:\n",
    "        ax_plot.set_xlabel(\"Longitude [°E]\", fontsize=fs)\n",
    "    if plot_ylabel:\n",
    "        ax_plot.set_ylabel(\"Latitude[°N]\", fontsize=fs)\n",
    "    \n",
    "    return ax_plot\n",
    "\n",
    "# for creating plot\n",
    "def create_plots(data1, data2, opt_plot={}):\n",
    "    \n",
    "    # get coordinate data \n",
    "    try:\n",
    "        time, lat, lon = data1[\"time\"].values, data1[\"lat\"].values, data1[\"lon\"].values\n",
    "        time_stamp = (pd.to_datetime(time)).strftime(\"%Y-%m-%d %H:00 UTC\")\n",
    "    except Exception as err:\n",
    "        print(\"Failed to retrieve coordinates from data1\")\n",
    "        raise err\n",
    "    # construct array for edges of grid points\n",
    "    dy, dx = np.round((lat[1] - lat[0]), 2), np.round((lon[1] - lon[0]), 2)\n",
    "    lat_e, lon_e = np.arange(lat[0]-dy/2, lat[-1]+dy, dy), np.arange(lon[0]-dx/2, lon[-1]+dx, dx)\n",
    "    \n",
    "    title1, title2 = provide_default(opt_plot, \"title1\", \"input T2m\"), provide_default(opt_plot, \"title2\", \"target T2m\")\n",
    "    title1, title2 = \"{0}, {1}\".format(title1, time_stamp), \"{0}, {1}\".format(title2, time_stamp)\n",
    "    levels = provide_default(opt_plot, \"levels\", np.arange(-5., 25., 1.))\n",
    "    \n",
    "    # get colormap\n",
    "    cmap_temp, norm_temp, lvl = get_colormap_temp(levels)\n",
    "    # create plot objects\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8), sharex=True, sharey=True, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    \n",
    "    # perform plotting\n",
    "    temp1 = ax1.pcolormesh(lon_e, lat_e, np.squeeze(data1.values-273.15), cmap=cmap_temp, norm=norm_temp)\n",
    "    temp2 = ax2.pcolormesh(lon_e, lat_e, np.squeeze(data2.values-273.15), cmap=cmap_temp, norm=norm_temp)\n",
    "\n",
    "    ax1, ax2 = decorate_plot(ax1), decorate_plot(ax2, plot_ylabel=False) \n",
    "\n",
    "    ax1.set_title(title1, size=14)\n",
    "    ax2.set_title(title2, size=14)\n",
    "\n",
    "    # add colorbar\n",
    "    cax = fig.add_axes([0.92, 0.3, 0.02, 0.4])\n",
    "    cbar = fig.colorbar(temp2, cax=cax, orientation=\"vertical\", ticks=lvl[1::2])\n",
    "    cbar.ax.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-mouse",
   "metadata": {},
   "source": [
    "The following cell produces a plot of the input 2m temperature which was coarsened during preprocessing (left) and the target temperature field. You may play around with `tind` to select different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even time indices yield 00 UTC, odd yield 12 UTC\n",
    "tind = 1\n",
    "\n",
    "create_plots(ds_train[\"t2m_in\"].isel(time=tind), ds_train[\"t2m_tar\"].isel(time=tind))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-current",
   "metadata": {},
   "source": [
    "Note that the input data shows up wit a very smooth temperature field, whereas the target 2m temperature is variable in the presence of complex topography (Alps and Low-mountain range), but also near the coastal areas (e.g. Netherlands)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-insulation",
   "metadata": {},
   "source": [
    "## Build and train a U-net for downscaling (required)\n",
    "\n",
    "<b> Note: </b> From this step on, everything should be executed on the <b>computing node</b>. <br>\n",
    "\n",
    "Before building the specific U-net, we need to define some auxiliary functions that wrap the building blocks of the model architecture. Again, you may skip digging into the details, but you must, of course, run the cell to enable building the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow and required stuff from Keras API\n",
    "import tensorflow as tf\n",
    "\n",
    "# all the layers used for U-net\n",
    "from tensorflow.keras.layers import (Activation, BatchNormalization, Concatenate, Conv2D,\n",
    "                                     Conv2DTranspose, Input, MaxPool2D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(inputs, num_filters: int, kernel: tuple = (3,3), padding: str = \"same\",\n",
    "               activation: str = \"relu\", kernel_init: str = \"he_normal\", l_batch_normalization: bool = True):\n",
    "    \"\"\"\n",
    "    A convolutional layer with optional batch normalization\n",
    "    :param inputs: the input data with dimensions nx, ny and nc\n",
    "    :param num_filters: number of filters (output channel dimension)\n",
    "    :param kernel: tuple indictating kernel size\n",
    "    :param padding: technique for padding (e.g. \"same\" or \"valid\")\n",
    "    :param activation: activation fuction for neurons (e.g. \"relu\")\n",
    "    :param kernel_init: initialization technique (e.g. \"he_normal\" or \"glorot_uniform\")\n",
    "    \"\"\"\n",
    "    \n",
    "    x = Conv2D(num_filters, kernel, padding=padding, kernel_initializer=kernel_init)(inputs)\n",
    "    if l_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv_block_n(inputs, num_filters, n=2, kernel=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                     kernel_init=\"he_normal\", l_batch_normalization=True):\n",
    "    \"\"\"\n",
    "    Sequential application of two convolutional layers (using conv_block).\n",
    "    \"\"\"\n",
    "    \n",
    "    x = conv_block(inputs, num_filters, kernel, padding, activation,\n",
    "                   kernel_init, l_batch_normalization)\n",
    "    for i in np.arange(n-1):\n",
    "        x = conv_block(x, num_filters, kernel, padding, activation,\n",
    "                       kernel_init, l_batch_normalization)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters, kernel_maxpool: tuple=(2,2), l_large: bool=True):\n",
    "    \"\"\"\n",
    "    One complete encoder-block used in U-net\n",
    "    \"\"\"\n",
    "    if l_large:\n",
    "        x = conv_block_n(inputs, num_filters, n=2)\n",
    "    else:\n",
    "        x = conv_block(inputs, num_filters)\n",
    "        \n",
    "    p = MaxPool2D(kernel_maxpool)(x)\n",
    "    \n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters, kernel: tuple=(3,3), strides_up: int=2, padding: str= \"same\", \n",
    "                  activation=\"relu\", kernel_init=\"he_normal\", l_batch_normalization: bool=True):\n",
    "    \"\"\"\n",
    "    One complete decoder block used in U-net (reverting the encoder)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = Conv2DTranspose(num_filters, (strides_up, strides_up), strides=strides_up, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block_n(x, num_filters, 2, kernel, padding, activation, kernel_init, l_batch_normalization)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-desire",
   "metadata": {},
   "source": [
    "The following cell creates the function to build up the U-net following the architecture used in the study by Sha et al. 2020 [1]. <br>\n",
    "Note, that an additional output branch for the surface elevation can be activated.\n",
    "This output branch helps the network to extract meaningful features of the topography and is therefore considered to enhance the generalization capacities as argued in [1]. Likewise, it would allow an transfer learning approach to regions where now high-resolved 2m temperature data is available for training.<br>\n",
    "<b> Note:</b> Here, we apply two consecutive convolutional layers per encoding block, whereas Sha et al. only use one layer (except for the top-level encoding block). Although this has only small effects on training accuracy, it  tends to yield slight improvements. <br><br>\n",
    "[1]: Sha, Yingkai, et al. \"Deep-learning-based gridded downscaling of surface meteorological variables in complex terrain. Part I: Daily maximum and minimum 2-m temperature.\" Journal of Applied Meteorology and Climatology 59.12 (2020): 2057-2073. (<a href=\"https://doi.org/10.1175/JAMC-D-20-0057.1\">DOI</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape, channels_start=56, z_branch=False):\n",
    "    \n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    \"\"\" encoder \"\"\"\n",
    "    s1, e1 = encoder_block(inputs, channels_start, l_large=True)\n",
    "    s2, e2 = encoder_block(e1, channels_start*2, l_large=False)\n",
    "    s3, e3 = encoder_block(e2, channels_start*4, l_large=False)\n",
    "    \n",
    "    \"\"\" bridge encoder <-> decoder \"\"\"\n",
    "    b1 = conv_block(e3, channels_start*8)\n",
    "    \n",
    "    \"\"\" decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s3, channels_start*4)\n",
    "    d2 = decoder_block(d1, s2, channels_start*2)\n",
    "    d3 = decoder_block(d2, s1, channels_start)\n",
    "    \n",
    "    output_temp = Conv2D(1, (1,1), kernel_initializer=\"he_normal\", name=\"output_temp\")(d3)\n",
    "    if z_branch:\n",
    "        output_z = Conv2D(1, (1, 1), kernel_initializer=\"he_normal\", name=\"output_z\")(d3)\n",
    "\n",
    "        model = Model(inputs, [output_temp, output_z], name=\"t2m_downscaling_unet_with_z\")\n",
    "    else:    \n",
    "        model = Model(inputs, output_temp, name=\"t2m_downscaling_unet\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-depression",
   "metadata": {},
   "source": [
    "Let's build and visualize the model architecture. For conveninece, we predescribe the input size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.utils as ku\n",
    "shape_in = (96, 128, 3)\n",
    "\n",
    "if \"login\" in host:\n",
    "    unet_model = build_unet(shape_in, z_branch=True)\n",
    "    ku.plot_model(unet_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-terrorism",
   "metadata": {},
   "source": [
    "Next, we need to preprocess the data which we obtained from the database. <br>\n",
    "Here, we make use of the z-score normalization to project the data onto a value range which eases backpropagation. Besides, we limit the data to a particluar datetime. This is reasonable in the scope of this approach, since no further information on the atmospheric state is offered to the model, e.g. atmospheric stratification in the planetary boundary layer (PBL). <br>\n",
    "By slicing the data for 12 UTC only, it is ensured that the PBL is usually well mixed during summertimes. However, you may also try training with data from 00 UTC (at night). <br><br>\n",
    "Again we start by defining some further helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_unet(dataset, daytime=12, opt_norm={}):\n",
    "    \"\"\"\n",
    "    Preprocess the data for feeding into the U-net, i.e. conversion to data arrays incl. z-score normalization\n",
    "    :param dataset: the dataset obtained from the database\n",
    "    :param daytime: daytime in UTC for temporal slicing\n",
    "    :param opt_norm: dictionary holding data for z-score normalization of data (\"mu_in\", \"std_in\", \"mu_tar\", \"std_tar\")\n",
    "    :return: normalized data ready to be fed to U-net model\n",
    "    \"\"\"\n",
    "    norm_dims_t = [\"time\"]                   # normalization of 2m temperature for each grid point\n",
    "    norm_dims_z = [\"time\", \"lat\", \"lon\"]     # 'global' normalization of surface elevation\n",
    "    \n",
    "    # slice the dataset\n",
    "    dsf = dataset.sel(time=dt.time(daytime))\n",
    "    \n",
    "    # retrieve and normalize input and target data\n",
    "    if not opt_norm:\n",
    "        t2m_in, t2m_in_mu, t2m_in_std  = z_norm_data(dsf[\"t2m_in\"], dims=norm_dims_t, return_stat=True)\n",
    "        t2m_tar, t2m_tar_mu, t2m_tar_std = z_norm_data(dsf[\"t2m_tar\"], dims=norm_dims_t, return_stat=True)\n",
    "    else: \n",
    "        t2m_in = z_norm_data(dsf[\"t2m_in\"], mu=opt_norm[\"mu_in\"], std=opt_norm[\"std_in\"])\n",
    "        t2m_tar = z_norm_data(dsf[\"t2m_tar\"], mu=opt_norm[\"mu_tar\"], std=opt_norm[\"std_tar\"])\n",
    "        \n",
    "    z_in, z_tar = z_norm_data(dsf[\"z_in\"], dims=norm_dims_z), z_norm_data(dsf[\"z_tar\"], dims=norm_dims_z)\n",
    "\n",
    "    in_data = xr.concat([t2m_in, z_in, z_tar], dim=\"variable\")\n",
    "    tar_data = xr.concat([t2m_tar, z_tar], dim=\"variable\")\n",
    "\n",
    "    # re-order data\n",
    "    in_data = in_data.transpose(\"time\",...,\"variable\")\n",
    "    tar_data = tar_data.transpose(\"time\",...,\"variable\")\n",
    "    if not opt_norm:\n",
    "        opt_norm = {\"mu_in\": t2m_in_mu, \"std_in\": t2m_in_std,\n",
    "                    \"mu_tar\": t2m_tar_mu, \"std_tar\": t2m_tar_std}\n",
    "        return in_data, tar_data, opt_norm\n",
    "    else:\n",
    "        return in_data, tar_data\n",
    "    \n",
    "\n",
    "def z_norm_data(data, mu=None, std=None, dims=None, return_stat=False):\n",
    "    \"\"\"\n",
    "    Perform z-score normalization on the data\n",
    "    :param data: the data-array\n",
    "    :param mu: the mean used for normalization (set to False if calculation from data is desired)\n",
    "    :param std: the standard deviation used for normalization (set to False if calculation from data is desired)\n",
    "    :param dims: list of dimension over which statistical quantities for normalization are calculated\n",
    "    :param return_stat: flag if normalization statistics are returned\n",
    "    :return: the normalized data\n",
    "    \"\"\"\n",
    "    if mu is None and std is None:\n",
    "        if not dims:\n",
    "            dims = list(data.dims)\n",
    "        mu = data.mean(dim=dims)\n",
    "        std = data.std(dim=dims)\n",
    "        \n",
    "    data_out = (data-mu)/std\n",
    "    \n",
    "    if return_stat:\n",
    "        return data_out, mu, std\n",
    "    else:\n",
    "        return data_out    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-disorder",
   "metadata": {},
   "source": [
    "Now, let's preprocess the training data. Change hour to 0 if you would like to train a U-net for nightly conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = 12\n",
    "\n",
    "int_data, tart_data, opt_norm = preprocess_data_for_unet(ds_train, daytime=hour)\n",
    "inv_data, tarv_data = preprocess_data_for_unet(ds_val, daytime=hour, opt_norm=opt_norm)\n",
    "\n",
    "# infer the shape of the data\n",
    "shape_in_data = int_data.shape\n",
    "assert shape_in_data[1:] == shape_in, \"Shape of preprocessed data ({0}) differs from expected shape ({1}).\"\\\n",
    "                                      .format(shape_in_data, shape_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-curtis",
   "metadata": {},
   "source": [
    "Having preprocessed the training data, we are ready to compile and train the U-net. <br>\n",
    "The U-net produces good results when it is trained for >50 epochs. Thereby, the learning rate gets decayed exponentially after 5 epochs using a learning rate scheduler that is passed to the model via Keras callback-functionality.\n",
    "\n",
    "### Start training (required)\n",
    "We are now ready to start the training. The optimization makes use of the Adam-optimizer and learning rate decay is deployed between the epoch 5 and 30. In total, we train for 70 epochs with a batch-size of 32 training samples.\n",
    "By default, training on the z_branch is enabled to make the model learn abstractions of the underlying topography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "z_branch = True                    # flag if additionally training on surface elevation is performed\n",
    "\n",
    "# define a earning-rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "  if epoch < 5:\n",
    "    return lr\n",
    "  elif epoch >= 5 and epoch < 30:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "  elif epoch >= 30:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# build, compile and train the model\n",
    "unet_model = build_unet(shape_in, z_branch=z_branch)\n",
    "if z_branch:\n",
    "    unet_model.compile(optimizer=Adam(learning_rate=5*10**(-4)),\n",
    "                   loss={\"output_temp\": \"mae\", \"output_z\": \"mae\"}, \n",
    "                   loss_weights={\"output_temp\": 1.0, \"output_z\": 1.0})\n",
    "    \n",
    "    history = unet_model.fit(x=int_data.values, y={\"output_temp\": tart_data.isel(variable=0).values,\n",
    "                                                   \"output_z\": tart_data.isel(variable=1).values},\n",
    "                             batch_size=32, epochs=70, callbacks=[callback], \n",
    "                             validation_data=(inv_data.values, {\"output_temp\": tarv_data.isel(variable=0).values,\n",
    "                                                                \"output_z\": tarv_data.isel(variable=1).values}))\n",
    "else:\n",
    "    unet_model.compile(optimizer=Adam(learning_rate=5*10**(-4)), loss=\"mae\")\n",
    "\n",
    "    history = unet_model.fit(x=int_data.values, y=tart_data.isel(variable=0).values, batch_size=32,\n",
    "                             epochs=70, callbacks=[callback],\n",
    "                             validation_data=(inv_data.values, tarv_data.isel(variable=0).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-clear",
   "metadata": {},
   "source": [
    "## Taking a look at the results (required)\n",
    "\n",
    "<b>Note:</b> This step is recommended since we check the success of the training here. The training is considered to be successful when the averaged MSE on the testing dataset is about 0.40 K$^2$<br> \n",
    "\n",
    "Having trained our U-net model, let's have a look at the results. <br>\n",
    "In the following, we will produce downscaled 2m temperature fields for the validation or testing datasets. \n",
    "Both datasets have not been part (explictly) of the optimization and can therefore be used for verifying the success of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test data first\n",
    "inte_data, tarte_data = preprocess_data_for_unet(ds_test, daytime=hour, opt_norm=opt_norm)\n",
    "\n",
    "# generate the downscaled fields\n",
    "y_pred_val = unet_model.predict(inv_data.values, verbose=1)\n",
    "y_pred_test = unet_model.predict(inte_data.values, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_type = \"test\"            # change here to switch between validation and testing data\n",
    "if comparison_type == \"validation\":\n",
    "  y_pred = y_pred_val\n",
    "  ds_ref = ds_val.sel(time=dt.time(hour))\n",
    "  var_ref = tarv_data.isel(variable=0)\n",
    "elif comparison_type == \"test\":\n",
    "  y_pred = y_pred_test\n",
    "  ds_ref = ds_test.sel(time=dt.time(hour))\n",
    "  var_ref = tarte_data.isel(variable=0)\n",
    "else:\n",
    "  ValueError(\"Unknown comparison_type '{0}' chosen.\".format(comparison_type))\n",
    "\n",
    "if np.ndim(y_pred) == 5:                # cropping necessary if z_branch is True (two output channels)\n",
    "  y_pred = y_pred[0]\n",
    "else:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-bidding",
   "metadata": {},
   "source": [
    "The data is still normalized and thus must be denormalized first. <br> \n",
    "For this purpose, we can exploit the `opt_norm`-dictionary created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some relevant information from the original dataset, ...\n",
    "coords = var_ref.squeeze().coords\n",
    "dims = var_ref.squeeze().dims\n",
    "\n",
    "# denomralize...\n",
    "y_pred_trans = np.squeeze(y_pred)*opt_norm[\"std_tar\"].squeeze().values + opt_norm[\"mu_tar\"].squeeze().values\n",
    "# and make xarray DataArray \n",
    "y_pred_trans = xr.DataArray(y_pred_trans, coords=coords, dims=dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-mouth",
   "metadata": {},
   "source": [
    "First let's a perform a simple verification by calculating the MSE (the MSE should be about/below 0.40 K$^2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((y_pred_trans - ds_ref[\"t2m_tar\"])**2).mean(dim=[\"lat\", \"lon\"])\n",
    "\n",
    "print(\"MSE of downscaled 2m temperature: {0:.3f} K**2 (+/-{1:.3f} K**2)\".format(mse.mean().values, mse.std().values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-electronics",
   "metadata": {},
   "source": [
    "To create the plots, we can use the `create_plots`-function defined above. The optional dictionary `opt_plot` helps to customize their appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a time index\n",
    "tind = 0\n",
    "\n",
    "# plot the full 2m temperature\n",
    "create_plots(y_pred_trans.isel(time=tind), ds_ref[\"t2m_in\"].isel(time=tind),\n",
    "             opt_plot={\"title1\": \"downscaled T2m\", \"title2\": \"input T2m\"})\n",
    "create_plots(y_pred_trans.isel(time=tind), ds_ref[\"t2m_tar\"].isel(time=tind),\n",
    "             opt_plot={\"title1\": \"downscaled T2m\"})\n",
    "# plot differences\n",
    "diff_in_tar = ds_ref[\"t2m_in\"].isel(time=tind)-ds_ref[\"t2m_tar\"].isel(time=tind) + 273.15\n",
    "diff_down_tar = y_pred_trans.isel(time=tind)-ds_ref[\"t2m_tar\"].isel(time=tind) + 273.15\n",
    "create_plots(diff_in_tar, diff_down_tar,\n",
    "             opt_plot={\"title1\": \"diff. input-target\", \"title2\": \"diff. downscaled-target\",\n",
    "                       \"levels\": np.arange(-3., 3.1, .2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-belize",
   "metadata": {},
   "source": [
    "As we see, the model has learned to recover a lot of details resulting mainly from the topography. Especially over the Alpes, but also over the the German low mountain ranges, the differences have become smaller and less structured. It is also noted that the differences near the coast (e.g. at the Baltic Sea) have become smaller. <br>\n",
    "However, some systematic features are still visible, the differences can stilll be as large as 3 K and especially in the Alps, the differences are somehow 'blurry'. Thus, there is still room for further improvement. \n",
    "These improvements will not only pertain the model architecture, but will also target to engulf more meteorological variables. The latter will also enable the network to generalize with respect to daytime and season. Note, that this has not been done yet, since we trained the U-net with data between April and September at 12 UTC only.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup_kernel_maelstrom",
   "language": "python",
   "name": "jup_kernel_maelstrom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
