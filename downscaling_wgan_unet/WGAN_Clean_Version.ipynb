{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9c93bb-d57c-4edf-950d-2d1e31eb6141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: climetlab in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (0.10.9)\n",
      "Requirement already satisfied: climetlab_maelstrom_downscaling in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (0.1.1)\n",
      "Requirement already satisfied: cdsapi in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.5.1)\n",
      "Requirement already satisfied: tqdm in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (4.56.0)\n",
      "Requirement already satisfied: dask in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (2021.6.0)\n",
      "Requirement already satisfied: eccodes>=1.3.0 in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (1.4.1)\n",
      "Requirement already satisfied: entrypoints in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (0.3)\n",
      "Requirement already satisfied: magics>=1.5.6 in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (1.5.7)\n",
      "Requirement already satisfied: filelock in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (3.0.12)\n",
      "Requirement already satisfied: ecmwf-api-client>=1.6.1 in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (1.6.3)\n",
      "Requirement already satisfied: markdown in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (3.3.4)\n",
      "Requirement already satisfied: cfgrib>=0.9.10 in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.9.10.1)\n",
      "Requirement already satisfied: pyodc in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (1.1.2)\n",
      "Requirement already satisfied: branca==0.3.1 in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.3.1)\n",
      "Requirement already satisfied: toolz in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/toolz-0.10.0-py3.8.egg (from climetlab) (0.10.0)\n",
      "Requirement already satisfied: ecmwf-opendata in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.1.1)\n",
      "Requirement already satisfied: pyyaml in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages (from climetlab) (5.3.1)\n",
      "Requirement already satisfied: multiurl>=0.0.15 in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.0.15)\n",
      "Requirement already satisfied: pdbufr in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.9.0)\n",
      "Collecting xarray>=0.19.0\n",
      "  Using cached xarray-2022.3.0-py3-none-any.whl (870 kB)\n",
      "Requirement already satisfied: numpy in /p/software/hdfml/stages/2020/software/TensorFlow/2.5.0-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (1.19.4)\n",
      "Requirement already satisfied: pandas in /p/software/hdfml/stages/2020/software/SciPy-Stack/2021-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages/pandas-1.1.0-py3.8-linux-x86_64.egg (from climetlab) (1.1.0)\n",
      "Requirement already satisfied: requests in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/requests-2.24.0-py3.8.egg (from climetlab) (2.24.0)\n",
      "Requirement already satisfied: netcdf4 in /p/software/hdfml/stages/2020/software/netcdf4-python/1.5.4-GCCcore-10.3.0-serial-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (1.5.4)\n",
      "Requirement already satisfied: termcolor in /p/software/hdfml/stages/2020/software/TensorFlow/2.5.0-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from climetlab) (1.1.0)\n",
      "Requirement already satisfied: ecmwflibs in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from climetlab) (0.4.8)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from dask->climetlab) (1.6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from dask->climetlab) (1.1.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from dask->climetlab) (0.6.2)\n",
      "Requirement already satisfied: findlibs in /p/home/jusers/gong1/hdfml/.local/lib/python3.8/site-packages (from eccodes>=1.3.0->climetlab) (0.0.2)\n",
      "Requirement already satisfied: cffi in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/cffi-1.14.1-py3.8-linux-x86_64.egg (from eccodes>=1.3.0->climetlab) (1.14.1)\n",
      "Requirement already satisfied: attrs in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/attrs-19.3.0-py3.8.egg (from eccodes>=1.3.0->climetlab) (19.3.0)\n",
      "Requirement already satisfied: click in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/click-7.1.2-py3.8.egg (from cfgrib>=0.9.10->climetlab) (7.1.2)\n",
      "Requirement already satisfied: six in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/six-1.15.0-py3.8.egg (from branca==0.3.1->climetlab) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/Jinja2-2.11.2-py3.8.egg (from branca==0.3.1->climetlab) (2.11.2)\n",
      "Requirement already satisfied: python-dateutil in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/python_dateutil-2.8.1-py3.8.egg (from multiurl>=0.0.15->climetlab) (2.8.1)\n",
      "Requirement already satisfied: pytz in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/pytz-2020.1-py3.8.egg (from multiurl>=0.0.15->climetlab) (2020.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/packaging-20.4-py3.8.egg (from xarray>=0.19.0->climetlab) (20.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/chardet-3.0.4-py3.8.egg (from requests->climetlab) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/idna-2.10-py3.8.egg (from requests->climetlab) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/urllib3-1.25.10-py3.8.egg (from requests->climetlab) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/certifi-2020.6.20-py3.8.egg (from requests->climetlab) (2020.6.20)\n",
      "Requirement already satisfied: cftime in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from netcdf4->climetlab) (1.2.1)\n",
      "Requirement already satisfied: locket in /p/software/hdfml/stages/2020/software/Jupyter/2021.3.2-gcccoremkl-10.3.0-2021.2.0-Python-3.8.5/lib/python3.8/site-packages (from partd>=0.3.10->dask->climetlab) (0.2.0)\n",
      "Requirement already satisfied: pycparser in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages (from cffi->eccodes>=1.3.0->climetlab) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/MarkupSafe-1.1.1-py3.8-linux-x86_64.egg (from jinja2->branca==0.3.1->climetlab) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /p/software/hdfml/stages/2020/software/Python/3.8.5-GCCcore-10.3.0/lib/python3.8/site-packages/pyparsing-2.4.7-py3.8.egg (from packaging>=20.0->xarray>=0.19.0->climetlab) (2.4.7)\n",
      "Installing collected packages: xarray\n",
      "Successfully installed xarray-2022.3.0\n"
     ]
    }
   ],
   "source": [
    "import climetlab as cml\n",
    "import os\n",
    "!pip install climetlab climetlab_maelstrom_downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4bc12c4-abf4-42fe-931c-281c2dcdae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_4741/3820622598.py:6: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Input, Concatenate,Conv3D,LeakyReLU, Dense, Conv2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import sigmoid, linear\n",
    "from tensorflow.keras.preprocessing.image import NumpyArrayIterator\n",
    "import  tensorflow as tf\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca6c339-ebfd-410c-921b-637574c8fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By downloading data from this dataset, you agree to the terms and conditions defined at https://git.ecmwf.int/projects/MLFET/repos/maelstrom-downscaling-ap5/browse/climetlab-maelstrom-downscaling-ap5/LICENSEIf you do not agree with such terms, do not download the data. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmlds_train = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"training\")\n",
    "cmlds_val = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"validation\")\n",
    "cmlds_test = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd7cfc1-5adb-4127-98f5-b48cc40841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary functions used for parsing the hyerparameters from hparams_dict\n",
    "def reduce_dict(dict_in: dict, dict_ref: dict):\n",
    "    \"\"\"\n",
    "    Reduces input dictionary to keys from reference dictionary. If the input dictionary lacks some keys, these are \n",
    "    copied over from the reference dictionary, i.e. the reference dictionary provides the defaults\n",
    "    :param dict_in: input dictionary\n",
    "    :param dict_ref: reference dictionary\n",
    "    :return: reduced form of input dictionary (with keys complemented from dict_ref if necessary)\n",
    "    \"\"\"\n",
    "    method = reduce_dict.__name__\n",
    "\n",
    "    # sanity checks\n",
    "    assert isinstance(dict_in, dict), \"%{0}: dict_in must be a dictionary, but is of type {1}\"\\\n",
    "                                      .format(method, type(dict_in))\n",
    "    assert isinstance(dict_ref, dict), \"%{0}: dict_ref must be a dictionary, but is of type {1}\"\\\n",
    "                                       .format(method, type(dict_ref)) \n",
    "\n",
    "    dict_merged = {**dict_ref, **dict_in}\n",
    "    dict_reduced = {key: dict_merged[key] for key in dict_ref}\n",
    "\n",
    "    return dict_reduced\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e40ccc-1dc6-4a6c-8fa6-d77de3df2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your hparameters in a dictionary\n",
    "hparams_dict = {\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 0.001,\n",
    "    \"max_epochs\": 5,\n",
    "    \"context_frames\": 7,\n",
    "    \"sequence_length\": 15,\n",
    "    \"ngf\": 16,\n",
    "    \"gan\": False, #enable gan\n",
    "    \"enable_wgan\":True, #enable wgan \n",
    "    \"enalbe_cgan\":False #enable the conditional information \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fbfa586-03d6-4f13-874f-698658616a6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2858739789.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4741/2858739789.py\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    def parse_hparams(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class WGANModel(object):\n",
    "\n",
    "    def __init__(self, mode: str = \"train\", hparams_dict: dict = None,input_shape:list=None, target_shape:list=None): \n",
    "        \"\"\"\n",
    "         This is a class for building convLSTM GAN architecture by using updated hparameters\n",
    "             mode                  : string, either \"train\" or \"val\" \n",
    "             hparams_dict          : dictionary, contains the hyperparameters names and default values\n",
    "             input_shape           : tf.Tensor shape equal to the input shape\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.hparams_dict = hparams_dict\n",
    "        self.hparams = self.parse_hparams()\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        self.learning_rate = self.hparams.lr\n",
    "        self.max_epochs = self.hparams.max_epochs\n",
    "        self.sequence_length = self.hparams.sequence_length\n",
    "        self.context_frames = self.hparams.context_frames\n",
    "        self.loss_fun = self.hparams.loss_fun\n",
    "        self.enable_gan = self.hparams.enable_gan\n",
    "        self.enable_wgan = self.hparams.enable_wgan\n",
    "        self.enable_cgan = self.hparams.enable_cgan\n",
    "        self.ngf = self.hparams.ngf\n",
    "        self.gan = self.hparams.gan\n",
    "        self.input_shape = input_shape\n",
    "        self.target_shape = target_shape\n",
    "        \n",
    "        \n",
    "    def hparams_check(self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def parse_hparams(self): \n",
    "        self.hparams_dict = dotdict(self.hparams_dict)\n",
    "        return self.hparams_dict\n",
    "\n",
    "    def generator(self,channels_start=56, z_branch=False):\n",
    "        \"\"\"\n",
    "        Function to build up the generator architecture, here we take UNET as generator\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        \"\"\" encoder \"\"\"\n",
    "        s1, e1 = WGANModel.encoder_block(inputs, channels_start, l_large=True)\n",
    "        s2, e2 = WGANModel.encoder_block(e1, channels_start*2, l_large=False)\n",
    "        s3, e3 = WGANModel.encoder_block(e2, channels_start*4, l_large=False)\n",
    "\n",
    "        \"\"\" bridge encoder <-> decoder \"\"\"\n",
    "        b1 = conv_block(e3, channels_start*8)\n",
    "\n",
    "        \"\"\" decoder \"\"\"\n",
    "        d1 = WGANModel.decoder_block(b1, s3, channels_start*4)\n",
    "        d2 = WGANModel.decoder_block(d1, s2, channels_start*2)\n",
    "        d3 = WGANModel.decoder_block(d2, s1, channels_start)\n",
    "\n",
    "        output_temp = Conv2D(1, (1,1), kernel_initializer=\"he_normal\", name=\"output_temp\")(d3)\n",
    "        if z_branch:\n",
    "            output_z = Conv2D(1, (1, 1), kernel_initializer=\"he_normal\", name=\"output_z\")(d3)\n",
    "\n",
    "            model = Model(inputs, [output_temp, output_z], name=\"t2m_downscaling_unet_with_z\")\n",
    "        else:    \n",
    "            model = Model(inputs, output_temp, name=\"t2m_downscaling_unet\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(inputs: tf.Tensor = None, num_filters: int = None, kernel: tuple = (3,3), padding: str=\"same\",\n",
    "                  activation: str = \"relu\", kernel_init: str = \"he_normal\", l_batch_normalization: bool=False): \n",
    "\n",
    "        \"\"\"\n",
    "        A convolutional layer with optional batch normalization\n",
    "        :param inputs: the input data with dimensions nx, ny and nc\n",
    "        :param num_filters: number of filters (output channel dimension)\n",
    "        :param kernel: tuple indictating kernel size\n",
    "        :param padding: technique for padding (e.g. \"same\" or \"valid\")\n",
    "        :param activation: activation fuction for neurons (e.g. \"relu\")\n",
    "        :param kernel_init: initialization technique (e.g. \"he_normal\" or \"glorot_uniform\")\n",
    "        \"\"\"\n",
    "        x = Conv2D(num_filters, kernel, padding=padding, kernel_initializer=kernel_init)(inputs)\n",
    "        if l_batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_block_n(inputs, num_filters, n=2, kernel=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                         kernel_init=\"he_normal\", l_batch_normalization=False):\n",
    "        \"\"\"\n",
    "        Sequential application of two convolutional layers (using conv_block).\n",
    "        \"\"\"\n",
    "\n",
    "        x = WGANModel.conv_block(inputs, num_filters, kernel, padding, activation,\n",
    "                       kernel_init, l_batch_normalization)\n",
    "        for i in np.arange(n-1):\n",
    "            x = WGANModel.conv_block(x, num_filters, kernel, padding, activation,\n",
    "                           kernel_init, l_batch_normalization)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def encoder_block(inputs, num_filters, kernel_maxpool: tuple=(2,2), l_large: bool=False):\n",
    "        \"\"\"\n",
    "        One complete encoder-block used in U-net\n",
    "        \"\"\"\n",
    "        if l_large:\n",
    "            x = WGANModel.conv_block_n(inputs, num_filters, n=2)\n",
    "        else:\n",
    "            x = WGANModel.conv_block(inputs, num_filters)\n",
    "\n",
    "        p = MaxPool2D(kernel_maxpool)(x)\n",
    "\n",
    "        return x, p\n",
    "    \n",
    "    @staticmethod\n",
    "    def decoder_block(inputs, skip_features, num_filters, kernel: tuple=(3,3), strides_up: int=2, padding: str= \"same\",\n",
    "                      activation: str=\"relu\", kernel_init: str=\"he_normal\", l_batch_normalization: bool=False):\n",
    "        \"\"\"\n",
    "        One complete decoder block used in U-net (reverting the encoder)\n",
    "        \"\"\"\n",
    "\n",
    "        x = Conv2DTranspose(num_filters, (strides_up, strides_up), strides=strides_up, padding=\"same\")(inputs)\n",
    "        x = Concatenate()([x, skip_features])\n",
    "        x = WGANModel.conv_block_n(x, num_filters, 2, kernel, padding, activation, kernel_init, l_batch_normalization)\n",
    "        return x\n",
    "    \n",
    "    def discriminator(self):\n",
    "        \"\"\"\n",
    "        Discriminator: this discriminaotr so far perfoms best on the precipitation dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        x = Input(self.target_shape)\n",
    "        conv1 = Conv3D(filters=4, kernel_size=2, strides=(1, 1, 1), padding='same')(x)\n",
    "        conv1 = Activation(\"relu\")(conv1)\n",
    "        conv2 = tf.reshape(conv1, [-1,1])\n",
    "        fc2 = LeakyReLU(0.2)(conv2)\n",
    "        out_logit = Dense(1)(fc2)\n",
    "        out = tf.nn.sigmoid(out_logit) \n",
    "        D = Model(x, [out,out_logit])\n",
    "        return D\n",
    "\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        self.d_optim = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        self.g_optim = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        \n",
    "    def get_gen_loss(self):\n",
    "        \"\"\"\n",
    "        Define generator loss\n",
    "\n",
    "        Return:  the loss of generator given inputs\n",
    "        \"\"\"\n",
    "        real_labels = tf.ones_like(self.D_fake)\n",
    "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake_logits, labels=real_labels))            \n",
    "        \n",
    "        return self.G_loss\n",
    "    \n",
    "\n",
    "    def get_disc_loss(self):\n",
    "        \"\"\"\n",
    "        Return the loss of discriminator given inputs\n",
    "        \"\"\"\n",
    "        real_labels = tf.ones_like(self.D_real)\n",
    "        gen_labels = tf.zeros_like(self.D_fake)\n",
    "        self.D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_real_logits,\n",
    "                                                                                  labels=real_labels))\n",
    "        self.D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake_logits,\n",
    "                                                                                  labels=gen_labels))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "\n",
    "        return self.D_loss\n",
    " \n",
    "\n",
    "    def get_recon_loss(self, target, gen_images):\n",
    "        recon_loss = tf.reduce_mean(tf.square(target - gen_images))\n",
    "        return recon_loss\n",
    "\n",
    "    @tf.function     \n",
    "    def train_step(self, inputs, target, i):\n",
    "        \"\"\"\n",
    "        Training models\n",
    "        \"\"\"\n",
    "        self.G = self.generator()\n",
    "        self.D = self.discriminator()\n",
    "        self.define_optimizers()\n",
    "        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
    "\n",
    "            gen_images = self.G(inputs)\n",
    "            self.D_real, self.D_real_logits = self.D(target)\n",
    "            self.D_fake, self.D_fake_logits = self.D(gen_images[:,self.context_frames-1:,:,:,0]) \n",
    "\n",
    "            g_loss = self.get_gen_loss()\n",
    "            d_loss = self.get_disc_loss()\n",
    "            recon_loss = self.get_recon_loss(target[:, :, :, :, 0], gen_images[:,self.context_frames-1:,:,:,0])\n",
    "            d_gradients = d_tape.gradient(d_loss, self.D.trainable_variables)\n",
    "\n",
    "            if not self.gan:\n",
    "                if i == 0:\n",
    "                    print(\"You are only training generator\")\n",
    "                # if the discriminator is not used for training, only train generator part\n",
    "                g_gradients = g_tape.gradient(recon_loss, self.G.trainable_variables)\n",
    "                self.g_optim.apply_gradients(zip(g_gradients, self.G.trainable_variables))\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    print(\"You are training both generator and discriminator\")\n",
    "                g_gradients = g_tape.gradient(g_loss + recon_loss, self.G.trainable_variables)\n",
    "                self.d_optim.apply_gradients(zip(d_gradients, self.D.trainable_variables))\n",
    "                self.g_optim.apply_gradients(zip(g_gradients, self.G.trainable_variables))\n",
    "\n",
    "        return g_loss, d_loss, recon_loss\n",
    "\n",
    "\n",
    "    def calculate_samples(self,predictors,train_ratio=0.5):\n",
    "        \"\"\"\n",
    "        calculate the number of training and validatioin samples \n",
    "        \"\"\"\n",
    "        self.Itrain = range(int(predictors.shape[0]*train_ratio))\n",
    "        self.Ieval = range(int(predictors.shape[0]*train_ratio), predictors.shape[0])\n",
    "        self.train_samples = int(predictors.shape[0]*train_ratio)\n",
    "        self.val_samples = predictors.shape[0] - int(predictors.shape[0]*train_ratio)\n",
    "        print(\"Total samples: {}, trainng samples: {}\".format(self.train_samples + self.val_samples,self.train_samples))\n",
    "\n",
    "\n",
    "    def make_data_generator(self, predictors, target):\n",
    "        self.calculate_samples(predictors)\n",
    "        train_predictors, train_target = predictors[Itrain, ...], target[Itrain, ...]\n",
    "        val_predictors, val_target = predictors[Ieval, ...], target[Ieval, ...]\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_predictors,train_target))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_predictors,val_target))\n",
    "        train_dataset = train_dataset.shuffle(100).repeat(self.max_epochs).batch(self.batch_size)\n",
    "        val_dataset = val_dataset.batch(self.batch_size)\n",
    "        self.train_iterator = iter(train_dataset)\n",
    "        self.val_iterator = iter(val_dataset) \n",
    "\n",
    "    def train(self,log_freq=5):\n",
    "\n",
    "        iterations_epoch = self.train_samples // self.batch_size\n",
    "        iteration = self.max_epochs * iterations_epoch\n",
    "\n",
    "        for step in range(iteration):\n",
    "            (x,y) = next(self.train_iterator)\n",
    "            \n",
    "            train_start_time = time.time()\n",
    "            g_loss, d_loss, recon_loss = self.train_step(x, y, step)\n",
    "            train_step_time = time.time() - train_start_time\n",
    "\n",
    "            if step % log_freq == 0:\n",
    "                template = '[{}/{}] D_loss={:.5f} G_loss={:.5f}, g_recon_loss={:.5f} training time per step: {:.5f}/s'\n",
    "                print(template.format(step, iteration, d_loss,g_loss,recon_loss, train_step_time))\n",
    "\n",
    "\n",
    "    def prediction(self):\n",
    "        iterations = self.val_samples // self.batch_size\n",
    "        (x_val,y_val) = next(self.val_iterator)\n",
    "        is_first = True\n",
    "        for i in range(iterations):\n",
    "            output = modelCase.G(x_val)\n",
    "            if is_first:\n",
    "                outputs = output\n",
    "                is_first = False\n",
    "            else:\n",
    "                outputs = np.concatenate((outputs,output), axis=0)\n",
    "        print(\"Inference is done\")\n",
    "        return outputs\n",
    "\n",
    "modelCase = WGANModel(mode=\"train\", hparams_dict=hparams_dict,\n",
    "                                         input_shape=[7,128,128,7],target_shape=[8,128,128,1])\n",
    "modelCase.make_data_generator(predictors, target_scaled)\n",
    "modelCase.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b68c6-8792-4877-bf99-e2587bc995d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
