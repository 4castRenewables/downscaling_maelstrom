{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9c93bb-d57c-4edf-950d-2d1e31eb6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import climetlab as cml\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "#!pip install climetlab climetlab_maelstrom_downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bc12c4-abf4-42fe-931c-281c2dcdae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 19:20:22.656132: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_28977/2554164913.py:9: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Input, Concatenate,Conv3D,LeakyReLU, Dense, Conv2D,Activation, BatchNormalization,\n",
    "                                     Conv2DTranspose, Input, MaxPool2D,Concatenate,Reshape)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import sigmoid, linear\n",
    "from tensorflow.keras.preprocessing.image import NumpyArrayIterator\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "np_config.enable_numpy_behavior()\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from typing import Protocol\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca6c339-ebfd-410c-921b-637574c8fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By downloading data from this dataset, you agree to the terms and conditions defined at https://git.ecmwf.int/projects/MLFET/repos/maelstrom-downscaling-ap5/browse/climetlab-maelstrom-downscaling-ap5/LICENSEIf you do not agree with such terms, do not download the data. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10003/152472913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcmlds_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maelstrom-downscaling\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcmlds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maelstrom-downscaling\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ds_train.nc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mds_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ds_val.nc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ds_test.nc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "cmlds_train = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"training\")\n",
    "cmlds_val = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"validation\")\n",
    "cmlds_test = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"testing\")\n",
    "ds_train.to_netcdf(\"ds_train.nc\")\n",
    "ds_val.to_netcdf(\"ds_val.nc\")\n",
    "ds_test.to_netcdf(\"ds_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a871dd91-d841-4dc2-864d-5831b594285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201604', '201605', '201606', '201607', '201608', '201609', '201704', '201705', '201706', '201707', '201708', '201709', '201804', '201805', '201806', '201807', '201808', '201809', '201904', '201905', '201906', '201907', '201908', '201909', '202004', '202005', '202006', '202007', '202008', '202009']\n"
     ]
    }
   ],
   "source": [
    "print(cmlds_train.all_datelist)\n",
    "ds_train = cmlds_train.to_xarray()\n",
    "ds_val = cmlds_val.to_xarray()\n",
    "ds_test = cmlds_test.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76df2f8a-21b6-42ac-bc23-9002a202cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ds_train = xr.open_dataset(\"ds_train.nc\")\n",
    "ds_val = xr.open_dataset(\"ds_val.nc\")\n",
    "ds_test = xr.open_dataset(\"ds_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a622c41b-eace-4e83-a212-62fb83856cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [pd.to_datetime(i).month for i in ds_train[\"time\"].values]\n",
    "times_val = [pd.to_datetime(i).month for i in ds_val[\"time\"].values]\n",
    "times_test = [pd.to_datetime(i).month for i in ds_test[\"time\"].values]\n",
    "times_val = np.array(times_val).reshape((len(times_val), 1))\n",
    "times_test= np.array(times_test).reshape((len(times_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ad963e-d544-439c-8c59-46c088f5a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One_Hot_Encode Categorical Data\n",
    "#get the month values\n",
    "#Bing: you can also embed hour, year by replacing. '.month' to 'hour', 'year'\n",
    "times = [pd.to_datetime(i).month for i in ds_train[\"time\"].values]\n",
    "n_classes = len(list(set(times)))\n",
    "times = np.array(times).reshape((len(times), 1))\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le.fit(times)\n",
    "embed_train = le.transform(times).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478c36cc-3a18-4b9f-b037-4aeedb1bdd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe31f005-030e-4cc8-bd33-6a709a03abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_val = le.transform(times_val).toarray()\n",
    "embed_test = le.transform(times_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d57b61-ac2b-415d-8eba-5a2446589cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(list(set(times)))\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efeb81bf-d51f-404d-8df6-0489b92ebf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def reshape_ds(ds):\n",
    "    ds = ds.to_array(dim = \"variables\").squeeze()\n",
    "    ds = np.squeeze(ds.values)\n",
    "    ds = np.transpose(ds, (1, 2, 3, 0))\n",
    "    return ds\n",
    "ds_test = reshape_ds(ds_test)\n",
    "ds_val = reshape_ds(ds_val)\n",
    "ds_train = reshape_ds(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ffe543-0863-4401-9866-28d697ac85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = ds_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc3b212-f8f2-470c-9f7c-754898e2ead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f100a1-cd63-4b5f-b863-e9f8275f4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 96, 128, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3be03dc-69ce-4300-ab20-e164f404eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = ds_train.shape[1]\n",
    "H = ds_train.shape[2]\n",
    "C = ds_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "210ffdd1-142f-415d-8a18-35dc8776c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [ds_train.shape[1],ds_train.shape[2], 2]\n",
    "target_shape = [ds_train.shape[1],ds_train.shape[2],1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a3be8b-e439-4809-81e2-7e5eff8cfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 128, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dd7cfc1-5adb-4127-98f5-b48cc40841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary functions used for parsing the hyerparameters from hparams_dict\n",
    "def reduce_dict(dict_in: dict, dict_ref: dict):\n",
    "    \"\"\"\n",
    "    Reduces input dictionary to keys from reference dictionary. If the input dictionary lacks some keys, these are \n",
    "    copied over from the reference dictionary, i.e. the reference dictionary provides the defaults\n",
    "    :param dict_in: input dictionary\n",
    "    :param dict_ref: reference dictionary\n",
    "    :return: reduced form of input dictionary (with keys complemented from dict_ref if necessary)\n",
    "    \"\"\"\n",
    "    method = reduce_dict.__name__\n",
    "\n",
    "    # sanity checks\n",
    "    assert isinstance(dict_in, dict), \"%{0}: dict_in must be a dictionary, but is of type {1}\"\\\n",
    "                                      .format(method, type(dict_in))\n",
    "    assert isinstance(dict_ref, dict), \"%{0}: dict_ref must be a dictionary, but is of type {1}\"\\\n",
    "                                       .format(method, type(dict_ref)) \n",
    "\n",
    "    dict_merged = {**dict_ref, **dict_in}\n",
    "    dict_reduced = {key: dict_merged[key] for key in dict_ref}\n",
    "\n",
    "    return dict_reduced\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "532f4b61-89cd-4576-861b-a35c903963e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(inputs: tf.Tensor = None, num_filters: int = None, kernel: tuple = (3,3), padding: str=\"same\",\n",
    "              activation: str = \"relu\", kernel_init: str = \"he_normal\", l_batch_normalization: bool=False): \n",
    "\n",
    "    \"\"\"\n",
    "    A convolutional layer with optional batch normalization\n",
    "    :param inputs: the input data with dimensions nx, ny and nc\n",
    "    :param num_filters: number of filters (output channel dimension)\n",
    "    :param kernel: tuple indictating kernel size\n",
    "    :param padding: technique for padding (e.g. \"same\" or \"valid\")\n",
    "    :param activation: activation fuction for neurons (e.g. \"relu\")\n",
    "    :param kernel_init: initialization technique (e.g. \"he_normal\" or \"glorot_uniform\")\n",
    "    \"\"\"\n",
    "    x = Conv2D(num_filters, kernel, padding=padding, kernel_initializer=kernel_init)(inputs)\n",
    "    if l_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def conv_block_n(inputs, num_filters, n=2, kernel=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                     kernel_init=\"he_normal\", l_batch_normalization=False):\n",
    "    \"\"\"\n",
    "    Sequential application of two convolutional layers (using conv_block).\n",
    "    \"\"\"\n",
    "\n",
    "    x = conv_block(inputs, num_filters, kernel, padding, activation,\n",
    "                   kernel_init, l_batch_normalization)\n",
    "    \n",
    "    for i in np.arange(n-1):\n",
    "        x = conv_block(x, num_filters, kernel, padding, activation,\n",
    "                       kernel_init, l_batch_normalization)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def encoder_block(inputs, num_filters:int=None, kernel_maxpool: tuple=(2,2), l_large: bool=False):\n",
    "    \"\"\"\n",
    "    One complete encoder-block used in U-net\n",
    "    \"\"\"\n",
    "    if l_large:\n",
    "        x = conv_block_n(inputs, num_filters, n=2)\n",
    "    else:\n",
    "        x = conv_block(inputs, num_filters)\n",
    "\n",
    "    p = MaxPool2D(kernel_maxpool)(x)\n",
    "\n",
    "    return x, p\n",
    "\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters, kernel: tuple=(3,3), strides_up: int=2, padding: str= \"same\",\n",
    "                  activation: str=\"relu\", kernel_init: str=\"he_normal\", l_batch_normalization: bool=False):\n",
    "    \"\"\"\n",
    "    One complete decoder block used in U-net (reverting the encoder)\n",
    "    \"\"\"\n",
    "\n",
    "    x = Conv2DTranspose(num_filters, (strides_up, strides_up), strides=strides_up, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block_n(x, num_filters, 2, kernel, padding, activation, kernel_init, l_batch_normalization)\n",
    "    return x\n",
    "\n",
    "def generator(input_shape, embed_shape, channels_start=56, z_branch=False):\n",
    "    \"\"\"\n",
    "    Function to build up the generator architecture, here we take UNET as generator\n",
    "    \"\"\"\n",
    "\n",
    "   # embedding input\n",
    "    in_label = Input(shape=(embed_shape))\n",
    "\n",
    "    # linear multiplication\n",
    "    n_nodes = input_shape[0] * input_shape[1]* input_shape[2]\n",
    "    li = Dense(n_nodes)(in_label)\n",
    "    # reshape to additional channel\n",
    "    li = Reshape((input_shape[0], input_shape[1],input_shape[2]))(li)\n",
    "\n",
    "    #image generator \n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    #merge image gen and label input\n",
    "    merge = Concatenate()([inputs, li])\n",
    "\n",
    "    \"\"\" encoder \"\"\"\n",
    "    s1, e1 = encoder_block(merge, channels_start, l_large=True)\n",
    "    s2, e2 = encoder_block(e1, channels_start*2, l_large=False)\n",
    "    s3, e3 = encoder_block(e2, channels_start*4, l_large=False)\n",
    "\n",
    "    \"\"\" bridge encoder <-> decoder \"\"\"\n",
    "    b1 = conv_block(e3, channels_start*8)\n",
    "\n",
    "    \"\"\" decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s3, channels_start*4)\n",
    "    d2 = decoder_block(d1, s2, channels_start*2)\n",
    "    d3 = decoder_block(d2, s1, channels_start)\n",
    "\n",
    "    output_temp = Conv2D(1, (1,1), kernel_initializer=\"he_normal\", name=\"output_temp\")(d3)\n",
    "    if z_branch:\n",
    "        output_z = Conv2D(1, (1, 1), kernel_initializer=\"he_normal\", name=\"output_z\")(d3)\n",
    "\n",
    "        model = Model([inputs,in_label], [output_temp, output_z], name=\"t2m_downscaling_unet_with_z\")\n",
    "    else:    \n",
    "        model = Model([inputs, in_label], output_temp, name=\"t2m_downscaling_unet\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c563d867-2965-4d24-947d-b78f1b784e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(target_shape):\n",
    "    \"\"\"\n",
    "    Discriminator: this discriminator so far perfoms best on the precipitation dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x = Input(target_shape)\n",
    "    conv1 = Conv2D(filters=4, kernel_size=2, strides=(1, 1), padding='same')(x)\n",
    "   \n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv2 = tf.reshape(conv1, [-1,1])\n",
    "    fc2 = LeakyReLU(0.2)(conv2)\n",
    "    out_logit = Dense(1)(fc2)\n",
    "    out = tf.nn.sigmoid(out_logit) \n",
    "    D = Model(x, out)\n",
    "    return D\n",
    "\n",
    "def ciritic(target_shape):\n",
    "    print(\"You are trainaing Wasserstain GAN\")\n",
    "    x = Input(target_shape)\n",
    "    conv1 = Conv3D(filters=4, kernel_size=2, strides=(1, 1,1), padding='same')(x)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv2 = tf.reshape(conv1, [-1,1])\n",
    "    fc2 = LeakyReLU(0.2)(conv2)\n",
    "    out = Dense(1, activiation=\"linear\")(fc2)\n",
    "    D = Model(x, out)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adac712e-4538-405b-bc46-e69cca5509e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This chunk defines the losses\n",
    "from typing import Any, Callable\n",
    "### Vanilla GAN loss        \n",
    "def gan_gen_loss(D_fake):\n",
    "    \"\"\"\n",
    "    Define generator loss\n",
    "\n",
    "    Return:  the loss of generator given inputs\n",
    "    \"\"\"\n",
    "    real_labels = tf.ones_like(D_fake)\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=real_labels))            \n",
    "    return G_loss\n",
    "\n",
    "def gan_disc_loss(D_real, D_fake):\n",
    "    \"\"\"\n",
    "    Return the loss of discriminator (Vanilla GAN)\n",
    "    \"\"\"\n",
    "    real_labels = tf.ones_like(D_real)\n",
    "    gen_labels = tf.zeros_like(D_fake)\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real,\n",
    "                                                                          labels=real_labels))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake,\n",
    "                                                                          labels=gen_labels))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    return D_loss\n",
    "\n",
    "def get_gan_loss(D_real, D_fake):\n",
    "    D_loss = gan_disc_loss(D_real, D_fake)\n",
    "    G_loss = gan_gen_loss(D_fake)\n",
    "    return D_loss, G_loss\n",
    "\n",
    "####WGAN loss\n",
    "def wgan_gen_loss(D_fake):\n",
    "    G_loss = -tf.reduce_mean(D_fake)\n",
    "    return G_loss\n",
    "\n",
    "def wgan_critic_loss(D_real,D_fake):\n",
    "    \"\"\"\n",
    "    Return the loss of critic (WGAN)\n",
    "    \"\"\"\n",
    "    D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\n",
    "    return D_loss\n",
    "\n",
    "def get_wgan_losses(D_real, D_fake):\n",
    "    G_loss =  wgan_gen_loss(D_fake)\n",
    "    D_loss = wgan_critic_loss(D_real,D_fake)\n",
    "    return D_loss, G_loss\n",
    "\n",
    "## Reconstruction loss for generator (UNet)\n",
    "def get_recon_loss(target, gen_images):\n",
    "    \"\"\"\n",
    "    Get reconstruction loss \n",
    "    \"\"\"\n",
    "    recon_loss = tf.reduce_mean(tf.square(target - gen_images))\n",
    "    return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e4483-f530-43da-b0ac-1cd5d4281de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22e40ccc-1dc6-4a6c-8fa6-d77de3df2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your hparameters in a dictionary\n",
    "hparams_dict = {\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 0.001,\n",
    "    \"max_epochs\": 5,\n",
    "    \"context_frames\": 7,\n",
    "    \"sequence_length\": 15,\n",
    "    \"ngf\": 16,\n",
    "    \"enable_gan\": True, #enable gan\n",
    "    \"enable_wgan\":True, #enable wgan \n",
    "    \"enable_embed\":False,  #enable the conditional information \n",
    "    \"d_steps\": 5, # The original paper recommends training, the discriminator for `x` more steps (typically 5) as compared t # one step of the generator.\n",
    "    \"alpha\":0.00005, #WGAN hparams\n",
    "    \"clip_const\":0.01, # default WGAN hparams\n",
    "    \"weight_recon\":0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fbfa586-03d6-4f13-874f-698658616a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANModel(object):\n",
    "\n",
    "    def __init__(self, mode: str = \"train\", hparams_dict: dict = None, \n",
    "                 target_shape:list=None, input_shape:list=None,embed_shape:list=None,\n",
    "                 discriminator: Callable=None, generator:Callable=None): \n",
    "        \"\"\"\n",
    "         This is a class for building convLSTM GAN architecture by using updated hparameters\n",
    "             mode                  : string, either \"train\" or \"val\" \n",
    "             hparams_dict          : dictionary, contains the hyperparameters names and default values\n",
    "             input_shape           : tf.Tensor shape equal to the input shape\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        self.target_shape = target_shape\n",
    "        self.embed_shape = embed_shape\n",
    "        #obtain the hyperparmeters\n",
    "        self.hparams_dict = hparams_dict\n",
    "        self.hparams = self.parse_hparams()\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        self.learning_rate = self.hparams.lr\n",
    "        self.max_epochs = self.hparams.max_epochs\n",
    "        self.sequence_length = self.hparams.sequence_length\n",
    "        self.context_frames = self.hparams.context_frames\n",
    "        self.loss_fun = self.hparams.loss_fun\n",
    "        self.enable_gan = self.hparams.enable_gan\n",
    "        self.enable_wgan = self.hparams.enable_wgan\n",
    "        self.enable_embed = self.hparams.enable_embed\n",
    "        self.d_steps = self.hparams.d_steps\n",
    "        self.ngf = self.hparams.ngf #latent dim\n",
    "        self.weight_recon = self.hparams.weight_recon\n",
    "        self.clip_const = self.hparams.clip_const\n",
    "        #Class attributes\n",
    "        self.recon_loss = None\n",
    "        self.G_loss = None\n",
    "        self.D_loss = None\n",
    "\n",
    "        \n",
    "\n",
    "    def hparams_check(self):\n",
    "        if not self.enable_gan:\n",
    "            if self.enable_wgan:\n",
    "                raise(\"You must set enable_gan to 'True' in hparams_dict congifuration\")\n",
    "\n",
    "                \n",
    "    def parse_hparams(self): \n",
    "        self.hparams_dict = dotdict(self.hparams_dict)\n",
    "        return self.hparams_dict\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        self.d_optim = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        self.g_optim = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    def get_losses(self, D_real, D_fake, target, gen_image):\n",
    "        \"\"\"\n",
    "        Get the losses based on the adopted model (GAN, WGAN, or UNet)\n",
    "        \"\"\"\n",
    "        # Reconstruction loss\n",
    "        recon_loss = get_recon_loss(target,gen_image)\n",
    "\n",
    "        if self.enable_wgan:\n",
    "            G_loss, D_loss = get_wgan_losses(D_real, D_fake)\n",
    "        else:\n",
    "            #vanilla GAN\n",
    "             G_loss, D_loss = get_gan_losses(D_real, D_fake)\n",
    "\n",
    "        return G_loss, D_loss, recon_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs, target, embed, i):\n",
    "        \"\"\"\n",
    "        Training model per step\n",
    "        inputs: inputs tensorflow\n",
    "        target: target tensor\n",
    "        embed: one hot embeding tensor\n",
    "        i: the training step\n",
    "        d_steps: the original paper recommends training, the discriminator for `x` more steps (typically 5) as compared t # one step of the generator.\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        \n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator.\n",
    "        \"\"\"\n",
    "\n",
    "        self.G = self.generator(self.input_shape, self.embed_shape)\n",
    "        self.D = self.discriminator(self.target_shape, self.embed_shape)\n",
    "        self.define_optimizers()\n",
    "\n",
    "        #Train discriminator/critic\n",
    "        for step in range(self.d_steps):\n",
    "            if self.enable_gan:\n",
    "                with tf.GradientTape() as d_tape:\n",
    "                    # Generate fake images given input images\n",
    "                    gen_images = self.G([inputs, embed])\n",
    "                    # Get the logits for the real images\n",
    "                    D_real = self.D(target)\n",
    "                    # Get the logits for the fake images\n",
    "                    D_fake = self.D(gen_images)\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    if self.enable_wgan:\n",
    "                        if i == 0 and step == 0 :\n",
    "                            print(\"You are training both generator and discriminator (WGAN)\")\n",
    "                            print(\"You are training {} times more discriminator/critic for one time generator\".format(self.d_steps))   \n",
    "                        D_loss = wgan_critic_loss(D_real,D_fake)\n",
    "                    else:\n",
    "                        if i == 0 and step == 0:\n",
    "                            print(\"You are training both generator and discriminator (GAN)\")\n",
    "                            print(\"You are training {} times more discriminator/critic for one time generator\".format(self.d_steps))   \n",
    "                        D_loss = gan_disc_loss(D_real, D_fake)\n",
    "\n",
    "                    d_gradients = d_tape.gradient(D_loss, self.D.trainable_variables)\n",
    "                    #define the training stratigy (ratio of number iteration of training on discriminator)\n",
    "                    self.d_optim.apply_gradients(zip(d_gradients, self.D.trainable_variables))\n",
    "            \n",
    "\n",
    "\n",
    "        # Train generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # Generate fake images given input images\n",
    "            gen_images = self.G([inputs,embed])\n",
    "            D_fake = self.D(gen_images)\n",
    "            G_loss = gan_gen_loss(D_fake)\n",
    "            recon_loss = get_recon_loss(target, gen_images)\n",
    "            total_gen_loss = (1-self.weight_recon) * G_loss + self.weight_recon*(recon_loss)\n",
    "            g_gradients = g_tape.gradient(total_gen_loss, self.G.trainable_variables)\n",
    "            self.g_optim.apply_gradients(zip(g_gradients, self.G.trainable_variables))\n",
    "            \n",
    "            for w in self.D.trainable_variables:\n",
    "                w.assign(tf.clip_by_value(w, -self.clip_const, self.clip_const))\n",
    "                \n",
    "            \n",
    "        return G_loss, D_loss, recon_loss\n",
    "\n",
    "\n",
    "    def make_data_generator(self, ds_train,ds_val,ds_test,embed_train,embed_val,embed_test):\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((ds_train[...,:2],ds_train[...,2:3],embed_train))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((ds_val[...,:2],ds_val[...,2:3],embed_val))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((ds_test[...,:2],ds_test[...,2:3],embed_test))\n",
    "        train_dataset = train_dataset.shuffle(100).repeat(self.max_epochs).batch(self.batch_size)\n",
    "        val_dataset = val_dataset.batch(self.batch_size)\n",
    "        test_dataset = test_dataset.batch(self.batch_size)\n",
    "        self.train_iterator = iter(train_dataset)\n",
    "        self.val_iterator = iter(val_dataset) \n",
    "        self.test_iterator = iter(test_dataset) \n",
    "        \n",
    "    def minic_train(self,n_samples,log_freq=10):\n",
    "        \n",
    "        iterations_epoch = n_samples // self.batch_size\n",
    "        iteration = self.max_epochs * iterations_epoch\n",
    "\n",
    "        for step in range(iteration):\n",
    "            x,y  = next(self.train_iterator)\n",
    "            train_start_time = time.time()\n",
    "            time.sleep(0.1)\n",
    "            train_step_time = time.time() - train_start_time\n",
    "\n",
    "            if step % log_freq == 0:\n",
    "                template = 'training time per step: {:.5f}/s'\n",
    "                print(template.format(train_step_time))\n",
    "                print(x.shape)\n",
    "\n",
    "    def train(self,  n_samples,log_freq=5):\n",
    "    \n",
    "        iterations_epoch = n_samples // self.batch_size\n",
    "        iteration = self.max_epochs * iterations_epoch\n",
    "\n",
    "        for step in range(iteration):\n",
    "            x,y,embed = next(self.train_iterator)\n",
    "\n",
    "            train_start_time = time.time()\n",
    "\n",
    "            g_loss, d_loss, recon_loss = self.train_step(x, y,embed, step)\n",
    "            train_step_time = time.time() - train_start_time\n",
    "\n",
    "            if step % log_freq == 0:\n",
    "                template = '[{}/{}] D_loss={:.5f} G_loss={:.5f}, g_recon_loss={:.5f} training time per step: {:.5f}/s'\n",
    "                print(template.format(step, iteration, d_loss, g_loss,recon_loss, train_step_time))\n",
    "\n",
    "                \n",
    "    # def prediction(self):\n",
    "    #     iterations = self.val_samples // self.batch_size\n",
    "    #     (x_val,y_val) = next(self.val_iterator)\n",
    "    #     is_first = True\n",
    "    #     for i in range(iterations):\n",
    "    #         output = modelCase.G(x_val)\n",
    "    #         if is_first:\n",
    "    #             outputs = output\n",
    "    #             is_first = False\n",
    "    #         else:\n",
    "    #             outputs = np.concatenate((outputs,output), axis=0)\n",
    "    #     print(\"Inference is done\")\n",
    "    #     return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21beac37-707e-45a4-b41e-ec4e049ababd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 128, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14da4019-178e-4f1d-94d7-f4b4b3595eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 128, 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e3200d8-8dc6-450c-9d06-b4bf4f9aa3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_shape = [n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ac1fa-bc54-4b69-a1e8-56d6b2cf5458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are training both generator and discriminator (WGAN)\n",
      "You are training 5 times more discriminator/critic for one time generator\n",
      "[0/1830] D_loss=-0.00940 G_loss=0.57294, g_recon_loss=7059137.88875 training time per step: 1.62504/s\n",
      "[5/1830] D_loss=-0.01025 G_loss=0.57625, g_recon_loss=106479004.04901 training time per step: 1.39342/s\n",
      "[10/1830] D_loss=0.01424 G_loss=0.43599, g_recon_loss=31139880.89970 training time per step: 1.65144/s\n",
      "[15/1830] D_loss=-0.37400 G_loss=0.50093, g_recon_loss=50224483.21153 training time per step: 1.37706/s\n",
      "[20/1830] D_loss=-0.05907 G_loss=0.37845, g_recon_loss=67725740.73684 training time per step: 1.45122/s\n",
      "[25/1830] D_loss=0.01984 G_loss=0.58815, g_recon_loss=61789893.88656 training time per step: 1.40940/s\n",
      "[30/1830] D_loss=0.25357 G_loss=0.64181, g_recon_loss=67402192.50581 training time per step: 1.62414/s\n",
      "[35/1830] D_loss=0.14030 G_loss=0.59502, g_recon_loss=41187681.54477 training time per step: 1.62659/s\n",
      "[40/1830] D_loss=-0.05203 G_loss=0.56442, g_recon_loss=77577040.18574 training time per step: 1.46015/s\n",
      "[45/1830] D_loss=-0.01555 G_loss=0.38447, g_recon_loss=29650709.42082 training time per step: 1.53592/s\n",
      "[50/1830] D_loss=-0.22374 G_loss=0.52971, g_recon_loss=72198292.03259 training time per step: 1.47092/s\n",
      "[55/1830] D_loss=-0.10459 G_loss=0.39273, g_recon_loss=64724977.01686 training time per step: 1.42355/s\n",
      "[60/1830] D_loss=-0.21214 G_loss=0.36482, g_recon_loss=83461987.17314 training time per step: 1.59885/s\n",
      "[65/1830] D_loss=-0.05561 G_loss=0.37756, g_recon_loss=32317916.00237 training time per step: 1.45592/s\n",
      "[70/1830] D_loss=-0.01804 G_loss=0.42378, g_recon_loss=19677848.02383 training time per step: 1.50320/s\n",
      "[75/1830] D_loss=-0.00733 G_loss=0.39369, g_recon_loss=40469536.19687 training time per step: 1.53337/s\n",
      "[80/1830] D_loss=-0.03993 G_loss=0.63916, g_recon_loss=25476475.10880 training time per step: 1.51145/s\n",
      "[85/1830] D_loss=0.14305 G_loss=0.40572, g_recon_loss=45927234.76397 training time per step: 1.63941/s\n",
      "[90/1830] D_loss=0.00124 G_loss=0.43028, g_recon_loss=15753699.59606 training time per step: 1.44263/s\n",
      "[95/1830] D_loss=-0.01140 G_loss=0.58091, g_recon_loss=44831666.88489 training time per step: 1.51538/s\n",
      "[100/1830] D_loss=-0.00988 G_loss=0.39119, g_recon_loss=64645536.07207 training time per step: 1.64771/s\n",
      "[105/1830] D_loss=-0.16928 G_loss=0.37667, g_recon_loss=59974692.03277 training time per step: 1.46519/s\n",
      "[110/1830] D_loss=-0.09071 G_loss=0.39126, g_recon_loss=101474136.94184 training time per step: 1.41986/s\n",
      "[115/1830] D_loss=-0.01206 G_loss=0.58339, g_recon_loss=35699364.59544 training time per step: 1.55279/s\n",
      "[120/1830] D_loss=0.03099 G_loss=0.37536, g_recon_loss=34899632.93934 training time per step: 1.57966/s\n",
      "[125/1830] D_loss=0.20804 G_loss=0.42524, g_recon_loss=53567150.61230 training time per step: 1.60403/s\n",
      "[130/1830] D_loss=0.07534 G_loss=0.56149, g_recon_loss=39855995.50950 training time per step: 1.66264/s\n",
      "[135/1830] D_loss=-0.03765 G_loss=0.54220, g_recon_loss=25690258.79374 training time per step: 1.66806/s\n",
      "[140/1830] D_loss=0.02727 G_loss=0.54556, g_recon_loss=13212266.92391 training time per step: 1.44276/s\n",
      "[145/1830] D_loss=0.04386 G_loss=0.58862, g_recon_loss=69989931.28554 training time per step: 1.49917/s\n",
      "[150/1830] D_loss=-0.18241 G_loss=0.36110, g_recon_loss=59079181.49159 training time per step: 1.48489/s\n",
      "[155/1830] D_loss=0.01733 G_loss=0.38243, g_recon_loss=10305506.75084 training time per step: 1.45025/s\n",
      "[160/1830] D_loss=-0.02316 G_loss=0.51072, g_recon_loss=18696057.01075 training time per step: 1.54838/s\n",
      "[165/1830] D_loss=0.05886 G_loss=0.60306, g_recon_loss=33758993.21011 training time per step: 1.54612/s\n",
      "[170/1830] D_loss=-0.46098 G_loss=0.31848, g_recon_loss=70529024.69982 training time per step: 1.54865/s\n",
      "[175/1830] D_loss=0.23221 G_loss=0.43352, g_recon_loss=235975851.13702 training time per step: 1.85618/s\n",
      "[180/1830] D_loss=0.19760 G_loss=0.42806, g_recon_loss=103113528.17763 training time per step: 1.66807/s\n",
      "[185/1830] D_loss=-0.05847 G_loss=0.39485, g_recon_loss=35498418.31313 training time per step: 1.54146/s\n",
      "[190/1830] D_loss=0.02616 G_loss=0.58692, g_recon_loss=36306631.37675 training time per step: 1.40581/s\n",
      "[195/1830] D_loss=-0.12764 G_loss=0.53293, g_recon_loss=101166835.25652 training time per step: 1.50836/s\n",
      "[200/1830] D_loss=-0.04506 G_loss=0.39313, g_recon_loss=39540371.92070 training time per step: 1.75112/s\n",
      "[205/1830] D_loss=0.14092 G_loss=0.59614, g_recon_loss=38821974.37732 training time per step: 1.43379/s\n",
      "[210/1830] D_loss=-0.04713 G_loss=0.60110, g_recon_loss=20603650.97138 training time per step: 1.42103/s\n",
      "[215/1830] D_loss=0.07224 G_loss=0.40688, g_recon_loss=54295095.69736 training time per step: 1.45843/s\n",
      "[220/1830] D_loss=-0.42740 G_loss=0.48870, g_recon_loss=68787746.14487 training time per step: 1.55717/s\n",
      "[225/1830] D_loss=-0.00812 G_loss=0.39186, g_recon_loss=5040828.52451 training time per step: 1.77645/s\n",
      "[230/1830] D_loss=0.01038 G_loss=0.58442, g_recon_loss=119582158.53741 training time per step: 1.55520/s\n",
      "[235/1830] D_loss=-0.02016 G_loss=0.39268, g_recon_loss=67409530.99687 training time per step: 1.58602/s\n",
      "[240/1830] D_loss=-0.32277 G_loss=0.36189, g_recon_loss=49892776.97479 training time per step: 1.41169/s\n",
      "[245/1830] D_loss=-0.09301 G_loss=0.39492, g_recon_loss=66272917.97784 training time per step: 1.54897/s\n",
      "[250/1830] D_loss=0.11150 G_loss=0.41039, g_recon_loss=147267989.48975 training time per step: 1.58156/s\n",
      "[255/1830] D_loss=-0.13281 G_loss=0.38736, g_recon_loss=31688088.70996 training time per step: 1.46482/s\n",
      "[260/1830] D_loss=-0.01735 G_loss=0.35556, g_recon_loss=19865791.26048 training time per step: 1.43376/s\n",
      "[265/1830] D_loss=-0.00485 G_loss=0.35809, g_recon_loss=10769615.22223 training time per step: 1.44645/s\n",
      "[270/1830] D_loss=0.01849 G_loss=0.56366, g_recon_loss=24043646.59655 training time per step: 1.40854/s\n",
      "[275/1830] D_loss=-0.12119 G_loss=0.39536, g_recon_loss=41633276.05777 training time per step: 1.50977/s\n",
      "[280/1830] D_loss=-0.15111 G_loss=0.38333, g_recon_loss=38911212.38243 training time per step: 1.43779/s\n",
      "[285/1830] D_loss=0.22273 G_loss=0.63120, g_recon_loss=143555258.91307 training time per step: 1.46034/s\n",
      "[290/1830] D_loss=-0.04631 G_loss=0.38538, g_recon_loss=119927473.32917 training time per step: 1.41835/s\n",
      "[295/1830] D_loss=-0.07650 G_loss=0.38260, g_recon_loss=198382812.45181 training time per step: 1.42649/s\n",
      "[300/1830] D_loss=-0.17939 G_loss=0.37033, g_recon_loss=52786038.79629 training time per step: 1.43254/s\n",
      "[305/1830] D_loss=-0.00950 G_loss=0.43839, g_recon_loss=14437149.16737 training time per step: 1.42393/s\n",
      "[310/1830] D_loss=0.08290 G_loss=0.40856, g_recon_loss=93106868.36712 training time per step: 1.42395/s\n",
      "[315/1830] D_loss=-0.08709 G_loss=0.43905, g_recon_loss=30675373.70132 training time per step: 1.41367/s\n",
      "[320/1830] D_loss=-0.14339 G_loss=0.56280, g_recon_loss=46466134.82200 training time per step: 1.43762/s\n",
      "[325/1830] D_loss=-0.00352 G_loss=0.39724, g_recon_loss=22816411.54449 training time per step: 1.44335/s\n",
      "[330/1830] D_loss=-0.11695 G_loss=0.56240, g_recon_loss=119964264.24980 training time per step: 1.44252/s\n",
      "[335/1830] D_loss=-0.00817 G_loss=0.57400, g_recon_loss=29867241.85756 training time per step: 1.50973/s\n",
      "[340/1830] D_loss=0.02449 G_loss=0.52668, g_recon_loss=27443322.44909 training time per step: 1.54223/s\n",
      "[345/1830] D_loss=0.15281 G_loss=0.60371, g_recon_loss=77942679.49584 training time per step: 1.48649/s\n",
      "[350/1830] D_loss=-0.13699 G_loss=0.39983, g_recon_loss=32464741.74808 training time per step: 1.42671/s\n",
      "[355/1830] D_loss=-0.07758 G_loss=0.38078, g_recon_loss=127057432.92372 training time per step: 1.44509/s\n",
      "[360/1830] D_loss=-0.05229 G_loss=0.38086, g_recon_loss=44114559.44592 training time per step: 1.45656/s\n",
      "[365/1830] D_loss=0.03185 G_loss=0.41152, g_recon_loss=10078294.64184 training time per step: 1.68986/s\n",
      "[370/1830] D_loss=0.07382 G_loss=0.57978, g_recon_loss=27267890.19624 training time per step: 1.45042/s\n",
      "[375/1830] D_loss=0.14682 G_loss=0.54890, g_recon_loss=32523310.97124 training time per step: 1.41061/s\n",
      "[380/1830] D_loss=-0.22401 G_loss=0.36114, g_recon_loss=59973815.08468 training time per step: 1.45331/s\n",
      "[385/1830] D_loss=-0.17152 G_loss=0.54517, g_recon_loss=57317067.09197 training time per step: 1.43592/s\n",
      "[390/1830] D_loss=-0.11221 G_loss=0.43018, g_recon_loss=32554362.70218 training time per step: 1.67152/s\n",
      "[395/1830] D_loss=-0.00375 G_loss=0.47625, g_recon_loss=5615964.48623 training time per step: 1.45176/s\n",
      "[400/1830] D_loss=-0.00442 G_loss=0.62414, g_recon_loss=13324385.84509 training time per step: 1.45392/s\n",
      "[405/1830] D_loss=-0.00758 G_loss=0.39356, g_recon_loss=44619639.33282 training time per step: 1.54863/s\n",
      "[410/1830] D_loss=-0.01014 G_loss=0.33320, g_recon_loss=5867071.46784 training time per step: 1.47931/s\n",
      "[415/1830] D_loss=-0.14180 G_loss=0.54713, g_recon_loss=62877926.69107 training time per step: 1.64759/s\n",
      "[420/1830] D_loss=-0.10397 G_loss=0.39724, g_recon_loss=30062559.00333 training time per step: 1.39388/s\n",
      "[425/1830] D_loss=-0.24326 G_loss=0.35335, g_recon_loss=83079167.26817 training time per step: 1.42802/s\n",
      "[430/1830] D_loss=0.22600 G_loss=0.43183, g_recon_loss=74557957.38692 training time per step: 1.66532/s\n",
      "[435/1830] D_loss=0.00004 G_loss=0.39499, g_recon_loss=19510188.24567 training time per step: 1.44885/s\n",
      "[440/1830] D_loss=-0.01568 G_loss=0.39108, g_recon_loss=17547153.11334 training time per step: 1.59628/s\n",
      "[445/1830] D_loss=-0.17107 G_loss=0.37723, g_recon_loss=60463085.71817 training time per step: 1.42548/s\n",
      "[450/1830] D_loss=-0.02549 G_loss=0.38097, g_recon_loss=7914715.76175 training time per step: 1.50735/s\n",
      "[455/1830] D_loss=-0.22418 G_loss=0.39725, g_recon_loss=39903933.54626 training time per step: 1.44219/s\n",
      "[460/1830] D_loss=0.23059 G_loss=0.43345, g_recon_loss=120943681.16421 training time per step: 1.42225/s\n",
      "[465/1830] D_loss=0.01399 G_loss=0.39866, g_recon_loss=92923096.83552 training time per step: 1.46076/s\n",
      "[470/1830] D_loss=-0.08128 G_loss=0.38271, g_recon_loss=106005208.61592 training time per step: 1.46932/s\n",
      "[475/1830] D_loss=-0.01109 G_loss=0.55866, g_recon_loss=29934866.63080 training time per step: 1.43317/s\n",
      "[480/1830] D_loss=-0.05371 G_loss=0.60078, g_recon_loss=26302560.10434 training time per step: 1.57787/s\n",
      "[485/1830] D_loss=-0.00692 G_loss=0.58757, g_recon_loss=37559948.17188 training time per step: 1.41744/s\n",
      "[490/1830] D_loss=0.14969 G_loss=0.59872, g_recon_loss=49717079.58687 training time per step: 1.53133/s\n",
      "[495/1830] D_loss=-0.00898 G_loss=0.57592, g_recon_loss=13897778.31543 training time per step: 1.51907/s\n",
      "[500/1830] D_loss=-0.00649 G_loss=0.35728, g_recon_loss=25277396.30551 training time per step: 1.43271/s\n",
      "[505/1830] D_loss=0.08663 G_loss=0.60539, g_recon_loss=44940710.46244 training time per step: 1.43627/s\n",
      "[510/1830] D_loss=0.02163 G_loss=0.40062, g_recon_loss=80429462.86170 training time per step: 1.46361/s\n",
      "[515/1830] D_loss=0.02909 G_loss=0.54712, g_recon_loss=7519187.38456 training time per step: 1.44568/s\n",
      "[520/1830] D_loss=0.02494 G_loss=0.60366, g_recon_loss=13288108.78216 training time per step: 1.42437/s\n",
      "[525/1830] D_loss=0.01727 G_loss=0.59020, g_recon_loss=6267588.31727 training time per step: 1.45537/s\n",
      "[530/1830] D_loss=0.32706 G_loss=0.44335, g_recon_loss=61578216.93267 training time per step: 1.50793/s\n",
      "[535/1830] D_loss=-0.05067 G_loss=0.61032, g_recon_loss=14682818.44521 training time per step: 1.42716/s\n",
      "[540/1830] D_loss=0.03428 G_loss=0.58655, g_recon_loss=29353025.96672 training time per step: 1.43083/s\n",
      "[545/1830] D_loss=0.04223 G_loss=0.39772, g_recon_loss=93425470.94858 training time per step: 1.44580/s\n",
      "[550/1830] D_loss=-0.01234 G_loss=0.55872, g_recon_loss=36332654.11233 training time per step: 1.46148/s\n",
      "[555/1830] D_loss=-0.01818 G_loss=0.38977, g_recon_loss=69590293.33306 training time per step: 1.61271/s\n",
      "[560/1830] D_loss=-0.23476 G_loss=0.36078, g_recon_loss=88494005.80723 training time per step: 1.43052/s\n"
     ]
    }
   ],
   "source": [
    "modelCase = WGANModel(mode=\"train\", hparams_dict=hparams_dict,\n",
    "                      input_shape=input_shape, target_shape=target_shape, embed_shape=embed_shape,\n",
    "                      discriminator=discriminator, generator=generator)\n",
    "\n",
    "modelCase.make_data_generator(ds_train, ds_val, ds_test,embed_train,embed_val,embed_test)\n",
    "modelCase.train(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b15e2-55f1-4dec-aa64-7c8425b278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e6d98-1b1b-4bd0-add2-654179383ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391db88-1622-47b9-846f-96ad5cf361b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
