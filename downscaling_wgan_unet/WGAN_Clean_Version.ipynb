{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c93bb-d57c-4edf-950d-2d1e31eb6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import climetlab as cml\n",
    "import os\n",
    "!pip install climetlab climetlab_maelstrom_downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bc12c4-abf4-42fe-931c-281c2dcdae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 15:40:11.408239: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_18923/221254215.py:9: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Input, Concatenate,Conv3D,LeakyReLU, Dense, Conv2D,Activation, BatchNormalization,\n",
    "                                     Conv2DTranspose, Input, MaxPool2D)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import sigmoid, linear\n",
    "from tensorflow.keras.preprocessing.image import NumpyArrayIterator\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "np_config.enable_numpy_behavior()\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from typing import Protocol\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6c339-ebfd-410c-921b-637574c8fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmlds_train = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"training\")\n",
    "cmlds_val = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"validation\")\n",
    "cmlds_test = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"testing\")\n",
    "ds_train.to_netcdf(\"ds_train.nc\")\n",
    "ds_val.to_netcdf(\"ds_val.nc\")\n",
    "ds_test.to_netcdf(\"ds_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a871dd91-d841-4dc2-864d-5831b594285e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmlds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25189/3079630106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmlds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_datelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmlds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_xarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mds_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmlds_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_xarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmlds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_xarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cmlds_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(cmlds_train.all_datelist)\n",
    "ds_train = cmlds_train.to_xarray()\n",
    "ds_val = cmlds_val.to_xarray()\n",
    "ds_test = cmlds_test.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76df2f8a-21b6-42ac-bc23-9002a202cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ds_train = xr.open_dataset(\"ds_train.nc\")\n",
    "ds_val = xr.open_dataset(\"ds_val.nc\")\n",
    "ds_test = xr.open_dataset(\"ds_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e476097-aa61-4fa3-a0d9-77fe3d78c2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efeb81bf-d51f-404d-8df6-0489b92ebf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def reshape_ds(ds):\n",
    "    ds = ds.to_array(dim = \"variables\").squeeze()\n",
    "    ds = np.squeeze(ds.values)\n",
    "    ds = np.transpose(ds, (1, 2, 3, 0))\n",
    "    return ds\n",
    "ds_test = reshape_ds(ds_test)\n",
    "ds_val = reshape_ds(ds_val)\n",
    "ds_train = reshape_ds(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ffe543-0863-4401-9866-28d697ac85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = ds_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc3b212-f8f2-470c-9f7c-754898e2ead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b347b-0e87-4379-9c8a-f68a113c06f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f100a1-cd63-4b5f-b863-e9f8275f4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 96, 128, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210ffdd1-142f-415d-8a18-35dc8776c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [ds_train.shape[1],ds_train.shape[2], 2]\n",
    "target_shape = [ds_train.shape[1],ds_train.shape[2],1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a3be8b-e439-4809-81e2-7e5eff8cfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 128, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd7cfc1-5adb-4127-98f5-b48cc40841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary functions used for parsing the hyerparameters from hparams_dict\n",
    "def reduce_dict(dict_in: dict, dict_ref: dict):\n",
    "    \"\"\"\n",
    "    Reduces input dictionary to keys from reference dictionary. If the input dictionary lacks some keys, these are \n",
    "    copied over from the reference dictionary, i.e. the reference dictionary provides the defaults\n",
    "    :param dict_in: input dictionary\n",
    "    :param dict_ref: reference dictionary\n",
    "    :return: reduced form of input dictionary (with keys complemented from dict_ref if necessary)\n",
    "    \"\"\"\n",
    "    method = reduce_dict.__name__\n",
    "\n",
    "    # sanity checks\n",
    "    assert isinstance(dict_in, dict), \"%{0}: dict_in must be a dictionary, but is of type {1}\"\\\n",
    "                                      .format(method, type(dict_in))\n",
    "    assert isinstance(dict_ref, dict), \"%{0}: dict_ref must be a dictionary, but is of type {1}\"\\\n",
    "                                       .format(method, type(dict_ref)) \n",
    "\n",
    "    dict_merged = {**dict_ref, **dict_in}\n",
    "    dict_reduced = {key: dict_merged[key] for key in dict_ref}\n",
    "\n",
    "    return dict_reduced\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "532f4b61-89cd-4576-861b-a35c903963e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(inputs: tf.Tensor = None, num_filters: int = None, kernel: tuple = (3,3), padding: str=\"same\",\n",
    "              activation: str = \"relu\", kernel_init: str = \"he_normal\", l_batch_normalization: bool=False): \n",
    "\n",
    "    \"\"\"\n",
    "    A convolutional layer with optional batch normalization\n",
    "    :param inputs: the input data with dimensions nx, ny and nc\n",
    "    :param num_filters: number of filters (output channel dimension)\n",
    "    :param kernel: tuple indictating kernel size\n",
    "    :param padding: technique for padding (e.g. \"same\" or \"valid\")\n",
    "    :param activation: activation fuction for neurons (e.g. \"relu\")\n",
    "    :param kernel_init: initialization technique (e.g. \"he_normal\" or \"glorot_uniform\")\n",
    "    \"\"\"\n",
    "    x = Conv2D(num_filters, kernel, padding=padding, kernel_initializer=kernel_init)(inputs)\n",
    "    if l_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def conv_block_n(inputs, num_filters, n=2, kernel=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                     kernel_init=\"he_normal\", l_batch_normalization=False):\n",
    "    \"\"\"\n",
    "    Sequential application of two convolutional layers (using conv_block).\n",
    "    \"\"\"\n",
    "\n",
    "    x = conv_block(inputs, num_filters, kernel, padding, activation,\n",
    "                   kernel_init, l_batch_normalization)\n",
    "    \n",
    "    for i in np.arange(n-1):\n",
    "        x = conv_block(x, num_filters, kernel, padding, activation,\n",
    "                       kernel_init, l_batch_normalization)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def encoder_block(inputs, num_filters, kernel_maxpool: tuple=(2,2), l_large: bool=False):\n",
    "    \"\"\"\n",
    "    One complete encoder-block used in U-net\n",
    "    \"\"\"\n",
    "    if l_large:\n",
    "        x = conv_block_n(inputs, num_filters, n=2)\n",
    "    else:\n",
    "        x = conv_block(inputs, num_filters)\n",
    "\n",
    "    p = MaxPool2D(kernel_maxpool)(x)\n",
    "\n",
    "    return x, p\n",
    "\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters, kernel: tuple=(3,3), strides_up: int=2, padding: str= \"same\",\n",
    "                  activation: str=\"relu\", kernel_init: str=\"he_normal\", l_batch_normalization: bool=False):\n",
    "    \"\"\"\n",
    "    One complete decoder block used in U-net (reverting the encoder)\n",
    "    \"\"\"\n",
    "\n",
    "    x = Conv2DTranspose(num_filters, (strides_up, strides_up), strides=strides_up, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block_n(x, num_filters, 2, kernel, padding, activation, kernel_init, l_batch_normalization)\n",
    "    return x\n",
    "\n",
    "def generator(input_shape, channels_start=56, z_branch=False):\n",
    "    \"\"\"\n",
    "    Function to build up the generator architecture, here we take UNET as generator\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" encoder \"\"\"\n",
    "    s1, e1 = encoder_block(inputs, channels_start, l_large=True)\n",
    "    s2, e2 = encoder_block(e1, channels_start*2, l_large=False)\n",
    "    s3, e3 = encoder_block(e2, channels_start*4, l_large=False)\n",
    "\n",
    "    \"\"\" bridge encoder <-> decoder \"\"\"\n",
    "    b1 = conv_block(e3, channels_start*8)\n",
    "\n",
    "    \"\"\" decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s3, channels_start*4)\n",
    "    d2 = decoder_block(d1, s2, channels_start*2)\n",
    "    d3 = decoder_block(d2, s1, channels_start)\n",
    "\n",
    "    output_temp = Conv2D(1, (1,1), kernel_initializer=\"he_normal\", name=\"output_temp\")(d3)\n",
    "    if z_branch:\n",
    "        output_z = Conv2D(1, (1, 1), kernel_initializer=\"he_normal\", name=\"output_z\")(d3)\n",
    "\n",
    "        model = Model(inputs, [output_temp, output_z], name=\"t2m_downscaling_unet_with_z\")\n",
    "    else:    \n",
    "        model = Model(inputs, output_temp, name=\"t2m_downscaling_unet\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c563d867-2965-4d24-947d-b78f1b784e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(target_shape):\n",
    "    \"\"\"\n",
    "    Discriminator: this discriminator so far perfoms best on the precipitation dataset\n",
    "    \"\"\"\n",
    "\n",
    "    x = Input(target_shape)\n",
    "    conv1 = Conv2D(filters=4, kernel_size=2, strides=(1, 1), padding='same')(x)\n",
    "   \n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv2 = tf.reshape(conv1, [-1,1])\n",
    "    fc2 = LeakyReLU(0.2)(conv2)\n",
    "    out_logit = Dense(1)(fc2)\n",
    "    out = tf.nn.sigmoid(out_logit) \n",
    "    D = Model(x, out)\n",
    "    return D\n",
    "\n",
    "def ciritic(target_shape):\n",
    "    print(\"You are trainaing Wasserstain GAN\")\n",
    "    x = Input(target_shape)\n",
    "    conv1 = Conv3D(filters=4, kernel_size=2, strides=(1, 1,1), padding='same')(x)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv2 = tf.reshape(conv1, [-1,1])\n",
    "    fc2 = LeakyReLU(0.2)(conv2)\n",
    "    out = Dense(1, activiation=\"linear\")(fc2)\n",
    "    D = Model(x, out)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adac712e-4538-405b-bc46-e69cca5509e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This chunk defines the losses\n",
    "from typing import Any, Callable\n",
    "### Vanilla GAN loss        \n",
    "def gan_gen_loss(D_fake):\n",
    "    \"\"\"\n",
    "    Define generator loss\n",
    "\n",
    "    Return:  the loss of generator given inputs\n",
    "    \"\"\"\n",
    "    real_labels = tf.ones_like(D_fake)\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=real_labels))            \n",
    "    return G_loss\n",
    "\n",
    "def gan_disc_loss(D_real, D_fake):\n",
    "    \"\"\"\n",
    "    Return the loss of discriminator (Vanilla GAN)\n",
    "    \"\"\"\n",
    "    real_labels = tf.ones_like(D_real)\n",
    "    gen_labels = tf.zeros_like(D_fake)\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real,\n",
    "                                                                          labels=real_labels))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake,\n",
    "                                                                          labels=gen_labels))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    return D_loss\n",
    "\n",
    "def get_gan_loss(D_real, D_fake):\n",
    "    D_loss = gan_disc_loss(D_real, D_fake)\n",
    "    G_loss = gan_gen_loss(D_fake)\n",
    "    return D_loss, G_loss\n",
    "\n",
    "####WGAN loss\n",
    "def wgan_gen_loss(D_fake):\n",
    "    G_loss = -tf.reduce_mean(D_fake)\n",
    "    return G_loss\n",
    "\n",
    "def wgan_critic_loss(D_real,D_fake):\n",
    "    \"\"\"\n",
    "    Return the loss of critic (WGAN)\n",
    "    \"\"\"\n",
    "    D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\n",
    "    return D_loss\n",
    "\n",
    "def get_wgan_losses(D_real, D_fake):\n",
    "    G_loss =  wgan_gen_loss(D_fake)\n",
    "    D_loss = wgan_critic_loss(D_real,D_fake)\n",
    "    return D_loss, G_loss\n",
    "\n",
    "## Reconstruction loss for generator (UNet)\n",
    "def get_recon_loss(target, gen_images):\n",
    "    \"\"\"\n",
    "    Get reconstruction loss \n",
    "    \"\"\"\n",
    "    recon_loss = tf.reduce_mean(tf.square(target - gen_images))\n",
    "    return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "22e40ccc-1dc6-4a6c-8fa6-d77de3df2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your hparameters in a dictionary\n",
    "hparams_dict = {\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 0.001,\n",
    "    \"max_epochs\": 5,\n",
    "    \"context_frames\": 7,\n",
    "    \"sequence_length\": 15,\n",
    "    \"ngf\": 16,\n",
    "    \"enable_gan\": True, #enable gan\n",
    "    \"enable_wgan\":True, #enable wgan \n",
    "    \"enable_embed\":False,  #enable the conditional information \n",
    "    \"d_steps\": 5, # The original paper recommends training, the discriminator for `x` more steps (typically 5) as compared t # one step of the generator.\n",
    "    \"alpha\":0.00005, #WGAN hparams\n",
    "    \"clip_const\":0.01, # default WGAN hparams\n",
    "    \"m\":64, ##WGAN hparams\n",
    "    \"weight_recon\":0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0fbfa586-03d6-4f13-874f-698658616a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANModel(object):\n",
    "\n",
    "    def __init__(self, mode: str = \"train\", hparams_dict: dict = None, \n",
    "                 target_shape:list=None, input_shape:list=None,\n",
    "                 discriminator: Callable=None, generator:Callable=None): \n",
    "        \"\"\"\n",
    "         This is a class for building convLSTM GAN architecture by using updated hparameters\n",
    "             mode                  : string, either \"train\" or \"val\" \n",
    "             hparams_dict          : dictionary, contains the hyperparameters names and default values\n",
    "             input_shape           : tf.Tensor shape equal to the input shape\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        self.target_shape = target_shape\n",
    "        #obtain the hyperparmeters\n",
    "        self.hparams_dict = hparams_dict\n",
    "        self.hparams = self.parse_hparams()\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        self.learning_rate = self.hparams.lr\n",
    "        self.max_epochs = self.hparams.max_epochs\n",
    "        self.sequence_length = self.hparams.sequence_length\n",
    "        self.context_frames = self.hparams.context_frames\n",
    "        self.loss_fun = self.hparams.loss_fun\n",
    "        self.enable_gan = self.hparams.enable_gan\n",
    "        self.enable_wgan = self.hparams.enable_wgan\n",
    "        self.enable_embed = self.hparams.enable_embed\n",
    "        self.d_steps = self.hparams.d_steps\n",
    "        self.ngf = self.hparams.ngf #latent dim\n",
    "        self.weight_recon = self.hparams.weight_recon\n",
    "        self.clip_const = self.hparams.clip_const\n",
    "        #Class attributes\n",
    "        self.recon_loss = None\n",
    "        self.G_loss = None\n",
    "        self.D_loss = None\n",
    "\n",
    "        \n",
    "\n",
    "    def hparams_check(self):\n",
    "        if not self.enable_gan:\n",
    "            if self.enable_wgan:\n",
    "                raise(\"You must set enable_gan to 'True' in hparams_dict congifuration\")\n",
    "\n",
    "                \n",
    "    def parse_hparams(self): \n",
    "        self.hparams_dict = dotdict(self.hparams_dict)\n",
    "        return self.hparams_dict\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        self.d_optim = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        self.g_optim = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    def get_losses(self, D_real, D_fake, target, gen_image):\n",
    "        \"\"\"\n",
    "        Get the losses based on the adopted model (GAN, WGAN, or UNet)\n",
    "        \"\"\"\n",
    "        # Reconstruction loss\n",
    "        recon_loss = get_recon_loss(target,gen_image)\n",
    "\n",
    "        if self.enable_wgan:\n",
    "            G_loss, D_loss = get_wgan_losses(D_real, D_fake)\n",
    "        else:\n",
    "            #vanilla GAN\n",
    "             G_loss, D_loss = get_gan_losses(D_real, D_fake)\n",
    "\n",
    "        return G_loss, D_loss, recon_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs, target, i, g_per_d):\n",
    "        \"\"\"\n",
    "        Training model per step\n",
    "        inputs: inputs tensorflow\n",
    "        target: target tensor\n",
    "        i: the training step\n",
    "        g_per_d: the original paper recommends training, the discriminator for `x` more steps (typically 5) as compared t # one step of the generator.\n",
    "        \n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        \n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator.\n",
    "        \"\"\"\n",
    "\n",
    "        self.G = self.generator(self.input_shape)\n",
    "        self.D = self.discriminator(self.target_shape)\n",
    "        self.define_optimizers()\n",
    "\n",
    "\n",
    "        #Train discriminator/critic\n",
    "        for step in range(self.d_steps):\n",
    "            if self.enable_gan:\n",
    "                with tf.GradientTape() as d_tape:\n",
    "                    # Generate fake images given input images\n",
    "                    gen_images = self.G(inputs)\n",
    "                    # Get the logits for the real images\n",
    "                    D_real = self.D(target)\n",
    "                    # Get the logits for the fake images\n",
    "                    D_fake = self.D(gen_images)\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    if self.enable_wgan:\n",
    "                        if i == 0 and step == 0 :\n",
    "                            print(\"You are training both generator and discriminator (WGAN)\")\n",
    "                            print(\"You are training {} times more discriminator/critic for one time generator\".format(self.d_steps))   \n",
    "                        D_loss = wgan_critic_loss(D_real,D_fake)\n",
    "                    else:\n",
    "                        if i == 0 and step == 0:\n",
    "                            print(\"You are training both generator and discriminator (GAN)\")\n",
    "                            print(\"You are training {} times more discriminator/critic for one time generator\".format(self.d_steps))   \n",
    "                        D_loss = gan_disc_loss(D_real, D_fake)\n",
    "\n",
    "                    d_gradients = d_tape.gradient(D_loss, self.D.trainable_variables)\n",
    "                    #define the training stratigy (ratio of number iteration of training on discriminator)\n",
    "                    self.d_optim.apply_gradients(zip(d_gradients, self.D.trainable_variables))\n",
    "\n",
    "\n",
    "        # Train generator\n",
    "\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # Generate fake images given input images\n",
    "            gen_images = self.G(inputs)\n",
    "            D_fake = self.D(gen_images)\n",
    "            G_loss = gan_gen_loss(D_fake)\n",
    "            recon_loss = get_recon_loss(target, gen_images)\n",
    "            total_gen_loss = (1-self.weight_recon) * G_loss + self.weight_recon*(recon_loss)\n",
    "            g_gradients = g_tape.gradient(total_gen_loss, self.G.trainable_variables)\n",
    "            self.g_optim.apply_gradients(zip(g_gradients, self.G.trainable_variables))\n",
    "            for w in self.D.trainable_variables:\n",
    "                w.assign(tf.clip_by_value(w, -self.clip_const, self.clip_const))\n",
    "                \n",
    "            \n",
    "        return G_loss, D_loss, recon_loss\n",
    "\n",
    "\n",
    "    def make_data_generator(self, ds_train,ds_val,ds_test):\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((ds_train[...,:2],ds_train[...,2:3]))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((ds_val[...,:2],ds_val[...,2:3]))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((ds_test[...,:2],ds_test[...,2:3]))\n",
    "        train_dataset = train_dataset.shuffle(100).repeat(self.max_epochs).batch(self.batch_size)\n",
    "        val_dataset = val_dataset.batch(self.batch_size)\n",
    "        test_dataset = test_dataset.batch(self.batch_size)\n",
    "        self.train_iterator = iter(train_dataset)\n",
    "        self.val_iterator = iter(val_dataset) \n",
    "        self.test_iterator = iter(test_dataset) \n",
    "        \n",
    "    def minic_train(self,n_samples,log_freq=10):\n",
    "        \n",
    "        iterations_epoch = n_samples // self.batch_size\n",
    "        iteration = self.max_epochs * iterations_epoch\n",
    "\n",
    "        for step in range(iteration):\n",
    "            x,y  = next(self.train_iterator)\n",
    "            train_start_time = time.time()\n",
    "            time.sleep(0.1)\n",
    "            train_step_time = time.time() - train_start_time\n",
    "\n",
    "            if step % log_freq == 0:\n",
    "                template = 'training time per step: {:.5f}/s'\n",
    "                print(template.format(train_step_time))\n",
    "                print(x.shape)\n",
    "\n",
    "\n",
    "    def train(self,  n_samples,log_freq=5):\n",
    "    \n",
    "        iterations_epoch = n_samples // self.batch_size\n",
    "        iteration = self.max_epochs * iterations_epoch\n",
    "\n",
    "        for step in range(iteration):\n",
    "            x,y = next(self.train_iterator)\n",
    "\n",
    "            train_start_time = time.time()\n",
    "\n",
    "            g_loss, d_loss, recon_loss = self.train_step(x, y, step, self.d_steps)\n",
    "            train_step_time = time.time() - train_start_time\n",
    "\n",
    "            if step % log_freq == 0:\n",
    "                template = '[{}/{}] D_loss={:.5f} G_loss={:.5f}, g_recon_loss={:.5f} training time per step: {:.5f}/s'\n",
    "                print(template.format(step, iteration, d_loss, g_loss,recon_loss, train_step_time))\n",
    "\n",
    "                \n",
    "    # def prediction(self):\n",
    "    #     iterations = self.val_samples // self.batch_size\n",
    "    #     (x_val,y_val) = next(self.val_iterator)\n",
    "    #     is_first = True\n",
    "    #     for i in range(iterations):\n",
    "    #         output = modelCase.G(x_val)\n",
    "    #         if is_first:\n",
    "    #             outputs = output\n",
    "    #             is_first = False\n",
    "    #         else:\n",
    "    #             outputs = np.concatenate((outputs,output), axis=0)\n",
    "    #     print(\"Inference is done\")\n",
    "    #     return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "21beac37-707e-45a4-b41e-ec4e049ababd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 128, 1]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "14da4019-178e-4f1d-94d7-f4b4b3595eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 128, 2]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ac1fa-bc54-4b69-a1e8-56d6b2cf5458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are training both generator and discriminator (WGAN)\n",
      "You are training 5 times more discriminator/critic for one time generator\n",
      "[0/1830] D_loss=-0.36460 G_loss=0.49876, g_recon_loss=79816254.64380 training time per step: 1.78149/s\n",
      "[5/1830] D_loss=-0.29866 G_loss=0.54312, g_recon_loss=64123080.65329 training time per step: 1.74846/s\n",
      "[10/1830] D_loss=-0.03419 G_loss=0.38774, g_recon_loss=61527745.91914 training time per step: 1.67509/s\n",
      "[15/1830] D_loss=-0.01310 G_loss=0.39193, g_recon_loss=86331556.26000 training time per step: 1.59867/s\n",
      "[20/1830] D_loss=0.01324 G_loss=0.58556, g_recon_loss=59381456.60832 training time per step: 1.58591/s\n",
      "[25/1830] D_loss=-0.13854 G_loss=0.37832, g_recon_loss=97644509.98135 training time per step: 1.60391/s\n",
      "[30/1830] D_loss=0.03697 G_loss=0.58469, g_recon_loss=59250733.93042 training time per step: 1.64778/s\n",
      "[35/1830] D_loss=0.13463 G_loss=0.40334, g_recon_loss=51105119.81785 training time per step: 1.53365/s\n",
      "[40/1830] D_loss=0.20547 G_loss=0.40955, g_recon_loss=48982428.15286 training time per step: 1.59807/s\n",
      "[45/1830] D_loss=-0.21105 G_loss=0.52955, g_recon_loss=91929442.62576 training time per step: 1.55524/s\n",
      "[50/1830] D_loss=-0.09637 G_loss=0.40558, g_recon_loss=38320117.33201 training time per step: 1.53138/s\n",
      "[55/1830] D_loss=0.06842 G_loss=0.56492, g_recon_loss=23529942.34906 training time per step: 1.55057/s\n",
      "[60/1830] D_loss=0.08683 G_loss=0.61270, g_recon_loss=113772423.11999 training time per step: 1.90716/s\n",
      "[65/1830] D_loss=-0.15025 G_loss=0.39235, g_recon_loss=43172105.51940 training time per step: 1.49152/s\n",
      "[70/1830] D_loss=0.01984 G_loss=0.52999, g_recon_loss=4551822.55633 training time per step: 1.54307/s\n",
      "[75/1830] D_loss=0.04195 G_loss=0.41014, g_recon_loss=123686518.56296 training time per step: 1.60256/s\n",
      "[80/1830] D_loss=-0.00219 G_loss=0.39393, g_recon_loss=114415293.68203 training time per step: 1.45821/s\n",
      "[85/1830] D_loss=-0.32603 G_loss=0.35669, g_recon_loss=65384284.78119 training time per step: 1.59704/s\n",
      "[90/1830] D_loss=-0.00462 G_loss=0.39492, g_recon_loss=5173545.16838 training time per step: 1.52103/s\n",
      "[95/1830] D_loss=0.16364 G_loss=0.60625, g_recon_loss=107718700.69450 training time per step: 1.46758/s\n",
      "[100/1830] D_loss=-0.21229 G_loss=0.58360, g_recon_loss=42099129.26860 training time per step: 1.60472/s\n",
      "[105/1830] D_loss=0.21898 G_loss=0.63004, g_recon_loss=62161725.66740 training time per step: 1.51316/s\n",
      "[110/1830] D_loss=-0.08092 G_loss=0.37964, g_recon_loss=75789532.34176 training time per step: 1.44419/s\n",
      "[115/1830] D_loss=-0.02104 G_loss=0.38102, g_recon_loss=28680742.41218 training time per step: 1.55999/s\n",
      "[120/1830] D_loss=-0.38462 G_loss=0.50355, g_recon_loss=102021751.17431 training time per step: 1.66110/s\n",
      "[125/1830] D_loss=0.03572 G_loss=0.60994, g_recon_loss=10006531.95677 training time per step: 1.59572/s\n",
      "[130/1830] D_loss=0.09521 G_loss=0.57437, g_recon_loss=45950117.12637 training time per step: 1.63766/s\n",
      "[135/1830] D_loss=-0.06241 G_loss=0.40949, g_recon_loss=13746597.81661 training time per step: 1.49589/s\n",
      "[140/1830] D_loss=-0.14059 G_loss=0.53614, g_recon_loss=146717306.78381 training time per step: 1.68736/s\n",
      "[145/1830] D_loss=-0.03652 G_loss=0.38879, g_recon_loss=212256573.62068 training time per step: 1.97303/s\n",
      "[150/1830] D_loss=-0.03595 G_loss=0.39105, g_recon_loss=28448628.08657 training time per step: 1.43430/s\n",
      "[155/1830] D_loss=-0.14042 G_loss=0.38377, g_recon_loss=49733272.46048 training time per step: 1.53029/s\n",
      "[160/1830] D_loss=0.01324 G_loss=0.58944, g_recon_loss=14010474.14413 training time per step: 1.61696/s\n",
      "[165/1830] D_loss=0.03986 G_loss=0.37385, g_recon_loss=24361008.63126 training time per step: 1.54503/s\n",
      "[170/1830] D_loss=0.01205 G_loss=0.58457, g_recon_loss=6064250.67801 training time per step: 1.45963/s\n",
      "[175/1830] D_loss=0.14131 G_loss=0.41484, g_recon_loss=117120038.57245 training time per step: 1.66699/s\n",
      "[180/1830] D_loss=0.22787 G_loss=0.61796, g_recon_loss=50757224.24380 training time per step: 1.43851/s\n",
      "[185/1830] D_loss=-0.19309 G_loss=0.36960, g_recon_loss=51303006.79087 training time per step: 1.66635/s\n",
      "[190/1830] D_loss=-0.04945 G_loss=0.44941, g_recon_loss=12276150.88922 training time per step: 1.43096/s\n",
      "[195/1830] D_loss=0.02547 G_loss=0.41120, g_recon_loss=19123059.42932 training time per step: 1.39648/s\n",
      "[200/1830] D_loss=0.00831 G_loss=0.36311, g_recon_loss=28148326.04856 training time per step: 1.65528/s\n",
      "[205/1830] D_loss=-0.01834 G_loss=0.39193, g_recon_loss=169502743.99515 training time per step: 1.50793/s\n",
      "[210/1830] D_loss=-0.00637 G_loss=0.62648, g_recon_loss=21735972.53404 training time per step: 1.74940/s\n",
      "[215/1830] D_loss=0.28260 G_loss=0.64212, g_recon_loss=85943161.56937 training time per step: 1.60509/s\n",
      "[220/1830] D_loss=0.05707 G_loss=0.40152, g_recon_loss=47838411.09815 training time per step: 1.64808/s\n",
      "[225/1830] D_loss=-0.01113 G_loss=0.39174, g_recon_loss=82580103.03704 training time per step: 1.66059/s\n",
      "[230/1830] D_loss=-0.02091 G_loss=0.39631, g_recon_loss=17552761.62817 training time per step: 1.99324/s\n",
      "[235/1830] D_loss=-0.24470 G_loss=0.35335, g_recon_loss=123122606.93510 training time per step: 1.68083/s\n",
      "[240/1830] D_loss=-0.02474 G_loss=0.57378, g_recon_loss=64065372.14403 training time per step: 1.52591/s\n",
      "[245/1830] D_loss=-0.18356 G_loss=0.37365, g_recon_loss=65662397.73592 training time per step: 1.59583/s\n",
      "[250/1830] D_loss=0.00806 G_loss=0.53571, g_recon_loss=16846624.18957 training time per step: 1.53605/s\n",
      "[255/1830] D_loss=0.16823 G_loss=0.41651, g_recon_loss=46841706.17016 training time per step: 1.62123/s\n",
      "[260/1830] D_loss=-0.00611 G_loss=0.63215, g_recon_loss=11601189.61747 training time per step: 1.59651/s\n",
      "[265/1830] D_loss=-0.18356 G_loss=0.55225, g_recon_loss=53327696.43833 training time per step: 1.53299/s\n",
      "[270/1830] D_loss=0.26942 G_loss=0.44033, g_recon_loss=69321372.26862 training time per step: 1.49341/s\n",
      "[275/1830] D_loss=0.19375 G_loss=0.42678, g_recon_loss=97806884.85785 training time per step: 1.60247/s\n",
      "[280/1830] D_loss=0.40240 G_loss=0.45546, g_recon_loss=127438546.64723 training time per step: 1.58620/s\n",
      "[285/1830] D_loss=-0.01679 G_loss=0.59828, g_recon_loss=10131509.08880 training time per step: 1.57036/s\n",
      "[290/1830] D_loss=-0.19274 G_loss=0.36423, g_recon_loss=87974768.96284 training time per step: 1.56582/s\n",
      "[295/1830] D_loss=0.02718 G_loss=0.59352, g_recon_loss=12999791.09295 training time per step: 1.73667/s\n",
      "[300/1830] D_loss=-0.10669 G_loss=0.38286, g_recon_loss=128721523.52643 training time per step: 1.70339/s\n",
      "[305/1830] D_loss=-0.00966 G_loss=0.52937, g_recon_loss=7238434.12626 training time per step: 1.64940/s\n",
      "[310/1830] D_loss=-0.09873 G_loss=0.40000, g_recon_loss=39850876.46242 training time per step: 1.65438/s\n",
      "[315/1830] D_loss=-0.00750 G_loss=0.35173, g_recon_loss=24997967.42256 training time per step: 2.10631/s\n",
      "[320/1830] D_loss=-0.03909 G_loss=0.41669, g_recon_loss=29904051.69052 training time per step: 1.56874/s\n",
      "[325/1830] D_loss=0.24622 G_loss=0.62687, g_recon_loss=153487474.27919 training time per step: 1.69121/s\n",
      "[330/1830] D_loss=0.02969 G_loss=0.61014, g_recon_loss=23783341.71713 training time per step: 1.54614/s\n",
      "[335/1830] D_loss=-0.01129 G_loss=0.39969, g_recon_loss=90176700.23795 training time per step: 1.48452/s\n",
      "[340/1830] D_loss=-0.42676 G_loss=0.48426, g_recon_loss=157713046.91442 training time per step: 1.53668/s\n",
      "[345/1830] D_loss=-0.04602 G_loss=0.34310, g_recon_loss=7807503.38779 training time per step: 1.53193/s\n",
      "[350/1830] D_loss=-0.23671 G_loss=0.35596, g_recon_loss=288729255.91893 training time per step: 1.59629/s\n",
      "[355/1830] D_loss=-0.04648 G_loss=0.38534, g_recon_loss=130758144.36787 training time per step: 1.52766/s\n",
      "[360/1830] D_loss=-0.29840 G_loss=0.51522, g_recon_loss=85637469.83982 training time per step: 1.60290/s\n",
      "[365/1830] D_loss=0.00221 G_loss=0.58227, g_recon_loss=47902669.16541 training time per step: 1.59850/s\n",
      "[370/1830] D_loss=0.13169 G_loss=0.55388, g_recon_loss=31427941.11682 training time per step: 1.57755/s\n",
      "[375/1830] D_loss=-0.17755 G_loss=0.37411, g_recon_loss=104323538.86159 training time per step: 1.63004/s\n",
      "[380/1830] D_loss=-0.21193 G_loss=0.57063, g_recon_loss=43524176.64376 training time per step: 1.56212/s\n",
      "[385/1830] D_loss=0.12775 G_loss=0.37241, g_recon_loss=31290736.06012 training time per step: 1.54560/s\n",
      "[390/1830] D_loss=-0.01515 G_loss=0.35450, g_recon_loss=7948049.71428 training time per step: 1.50002/s\n",
      "[395/1830] D_loss=-0.07519 G_loss=0.37793, g_recon_loss=144168336.05214 training time per step: 1.57964/s\n",
      "[400/1830] D_loss=0.01076 G_loss=0.33853, g_recon_loss=15721030.45990 training time per step: 1.96319/s\n"
     ]
    }
   ],
   "source": [
    "modelCase = WGANModel(mode=\"train\", hparams_dict=hparams_dict,\n",
    "                      input_shape=input_shape,target_shape=target_shape,\n",
    "                      discriminator=discriminator, generator=generator)\n",
    "\n",
    "modelCase.make_data_generator(ds_train, ds_val, ds_test)\n",
    "modelCase.train(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b15e2-55f1-4dec-aa64-7c8425b278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e6d98-1b1b-4bd0-add2-654179383ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b68c6-8792-4877-bf99-e2587bc995d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391db88-1622-47b9-846f-96ad5cf361b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
